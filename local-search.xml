<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Leetcode118：杨辉三角</title>
    <link href="/2020/05/31/leetcode118/"/>
    <url>/2020/05/31/leetcode118/</url>
    
    <content type="html"><![CDATA[<h1 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h1><h4 id="杨辉三角"><a href="#杨辉三角" class="headerlink" title="杨辉三角"></a><a href="https://leetcode-cn.com/problems/pascals-triangle/" target="_blank" rel="noopener">杨辉三角</a></h4><blockquote><p>给定一个非负整数 numRows，生成杨辉三角的前 numRows 行。</p></blockquote><p><strong>示例</strong></p><pre><code class="hljs angelscript">输入: <span class="hljs-number">5</span>输出:[     [<span class="hljs-number">1</span>],    [<span class="hljs-number">1</span>,<span class="hljs-number">1</span>],   [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>],  [<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,<span class="hljs-number">3</span>,<span class="hljs-number">1</span>], [<span class="hljs-number">1</span>,<span class="hljs-number">4</span>,<span class="hljs-number">6</span>,<span class="hljs-number">4</span>,<span class="hljs-number">1</span>]]</code></pre><h1 id="解法一"><a href="#解法一" class="headerlink" title="解法一"></a>解法一</h1>]]></content>
    
    
    <categories>
      
      <category>Leetcode</category>
      
    </categories>
    
    
    <tags>
      
      <tag>esay</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LeetCode1：两数之和</title>
    <link href="/2020/05/28/leetcode01/"/>
    <url>/2020/05/28/leetcode01/</url>
    
    <content type="html"><![CDATA[<h1 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h1><h4 id="两数之和"><a href="#两数之和" class="headerlink" title="两数之和"></a><a href="https://leetcode-cn.com/problems/two-sum/" target="_blank" rel="noopener">两数之和</a></h4><blockquote><p>给定一个整数数组 nums 和一个目标值 target，请你在该数组中找出和为目标值的那 两个 整数，并返回他们的数组下标。你可以假设每种输入只会对应一个答案。但是，数组中同一个元素不能使用两遍。</p></blockquote><p><strong>示例</strong></p><pre><code class="hljs angelscript">给定 nums = [<span class="hljs-number">2</span>, <span class="hljs-number">7</span>, <span class="hljs-number">11</span>, <span class="hljs-number">15</span>], target = <span class="hljs-number">9</span>因为 nums[<span class="hljs-number">0</span>] + nums[<span class="hljs-number">1</span>] = <span class="hljs-number">2</span> + <span class="hljs-number">7</span> = <span class="hljs-number">9</span>所以返回 [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>]</code></pre><h1 id="解法一"><a href="#解法一" class="headerlink" title="解法一"></a>解法一</h1><blockquote><p>思路：暴力遍历，时间复杂度为<code>O(n^2)</code>、空间复杂度为<code>O(1)​</code></p></blockquote><pre><code class="hljs java"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Solution</span></span>&#123;    <span class="hljs-keyword">public</span> <span class="hljs-keyword">int</span>[] twoSum(<span class="hljs-keyword">int</span>[] nums,<span class="hljs-keyword">int</span> target) &#123;        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; nums.length; i++) &#123;            <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> j = i + <span class="hljs-number">1</span>; j &lt; nums.length; j++) &#123;                <span class="hljs-keyword">if</span> (nums[j] == target - nums[i]) &#123;<span class="hljs-comment">//寻找条件</span>                    <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> <span class="hljs-keyword">int</span>[]&#123;i, j&#125;;                &#125;            &#125;        &#125;        <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> IllegalArgumentException(<span class="hljs-string">"no Two sum solution"</span>);    &#125;&#125;</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>easy</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>使用Sqoop导MySQL数据到Hbase报错</title>
    <link href="/2020/04/15/sqoop1/"/>
    <url>/2020/04/15/sqoop1/</url>
    
    <content type="html"><![CDATA[<h1 id="报错日志"><a href="#报错日志" class="headerlink" title="报错日志"></a>报错日志</h1><pre><code class="hljs xml">20/04/14 16:40:45 WARN mapreduce.HBaseImportJob: Could not find HBase table hbase_company20/04/14 16:40:45 WARN mapreduce.HBaseImportJob: This job may fail. Either explicitly create the table,20/04/14 16:40:45 WARN mapreduce.HBaseImportJob: or re-run with --hbase-create-table.20/04/14 16:40:45 INFO zookeeper.ZooKeeper: Session: 0x36db06bc9c32fb9 closed20/04/14 16:40:45 INFO zookeeper.ClientCnxn: EventThread shut down</code></pre><h1 id="失败原因"><a href="#失败原因" class="headerlink" title="失败原因"></a>失败原因</h1><ul><li><p>使用sqoop导mysql数据到HDFS，不用在hive里面建表，在跑mr任务时候自动创建hive表</p></li><li><p>在用sqoop导mysql数据到HDFS，没有在Hbase建表</p></li><li><p>查询官方文档：sqoop1.4.7只支持HBase1.0.1之前的版本的自动创建HBase表的功能</p></li></ul><h1 id="版本信息"><a href="#版本信息" class="headerlink" title="版本信息"></a>版本信息</h1><ul><li><p>sqoop 版本1.4.7 </p></li><li><p>Hbase 版本2.1.1</p></li></ul><h1 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h1><h2 id="1、在hbase里面建表"><a href="#1、在hbase里面建表" class="headerlink" title="1、在hbase里面建表"></a>1、在hbase里面建表</h2><pre><code class="hljs bash">create <span class="hljs-string">'hbase_company'</span>,<span class="hljs-string">'info'</span>scan <span class="hljs-string">'hbase_company'</span></code></pre><h2 id="2、脚本添加参数"><a href="#2、脚本添加参数" class="headerlink" title="2、脚本添加参数"></a>2、脚本添加参数</h2><pre><code class="hljs bash">--hbase-create-table</code></pre><h1 id="附：导数据脚本"><a href="#附：导数据脚本" class="headerlink" title="附：导数据脚本"></a>附：导数据脚本</h1><pre><code class="hljs bash">bin/sqoop import \--connect jdbc:mysql://ip:3306/company \--username root \--password 000000 \--table company \--columns <span class="hljs-string">"id,name,sex"</span> \--column-family <span class="hljs-string">"info"</span> \--hbase-create-table \--hbase-row-key <span class="hljs-string">"id"</span> \--hbase-table <span class="hljs-string">"hbase_company"</span> \--num-mappers 1 \--split-by id</code></pre>]]></content>
    
    
    <categories>
      
      <category>sqoop</category>
      
    </categories>
    
    
    <tags>
      
      <tag>解决问题</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>博客导航</title>
    <link href="/2020/03/28/%E5%8D%9A%E5%AE%A2%E5%AF%BC%E8%88%AA/"/>
    <url>/2020/03/28/%E5%8D%9A%E5%AE%A2%E5%AF%BC%E8%88%AA/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>ClickHouse之深圳MeetUp</title>
    <link href="/2019/10/25/clickhouse1-1/"/>
    <url>/2019/10/25/clickhouse1-1/</url>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>目前公司调研OLAT开源框架，最近ClickHouse火热，看了ClickHouse开源社区的Meetup，于是参加了宣讲，感触蛮多，同时俄罗斯Yandex团队小哥们都很帅、</p><h1 id="纪要"><a href="#纪要" class="headerlink" title="纪要"></a>纪要</h1><h2 id="时间：10月20日下午2-00-6-30"><a href="#时间：10月20日下午2-00-6-30" class="headerlink" title="时间：10月20日下午2:00 -6:30"></a>时间：10月20日下午2:00 -6:30</h2><h2 id="地点：深圳湾软件产业基地4栋B座裙楼UTCP创意加速"><a href="#地点：深圳湾软件产业基地4栋B座裙楼UTCP创意加速" class="headerlink" title="地点：深圳湾软件产业基地4栋B座裙楼UTCP创意加速"></a>地点：深圳湾软件产业基地4栋B座裙楼UTCP创意加速</h2><h2 id="议题"><a href="#议题" class="headerlink" title="议题"></a>议题</h2><ul><li>2:00pm-3:00pm  《Roadmap and overview of ClickHouse》   AlekSei Milovidov</li><li>3:00pm-3:40pm 《ClickHouse编写自定义计算函数》 Sundy Li</li><li>3:40pm-4:20pm  《Clickhouse 在Tencent的应用实践》Tencent  丁晓坤 周东祥</li><li>4:20pm-4:30pm Break</li><li>4:30pm-5:10pm 《ClickHouse MergeTree原理解析》 远光软件 朱凯</li><li>5:10pm-5:50pm 《数仓Clickhouse多维分析应用实践》 华润万家 数据分析高级经理 朱元</li><li>5:50pm-6:30pm   《Recently released features and future plans》 Ivan Blinkov(Temp)</li><li>6:00pm-6:30   Free discussion </li></ul><h1 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h1><ul><li>Yandex团队clickhouse的新特性以及在机器学习方面的研究，未来大有可期。</li><li>腾讯内部已经在大规模应用。</li><li>朱凯老师MergerTree讲解的很细致。</li></ul>]]></content>
    
    
    <categories>
      
      <category>ClickHouse</category>
      
    </categories>
    
    
    <tags>
      
      <tag>meetup</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux(四)：常用基本命令</title>
    <link href="/2018/06/07/linux4/"/>
    <url>/2018/06/07/linux4/</url>
    
    <content type="html"><![CDATA[<h1 id="帮助命令"><a href="#帮助命令" class="headerlink" title="帮助命令"></a>帮助命令</h1><h2 id="1-man-获得帮助信息"><a href="#1-man-获得帮助信息" class="headerlink" title="1 man 获得帮助信息"></a>1 man 获得帮助信息</h2><ul><li><p>基本语法</p><blockquote><p>man [命令或配置文件]        （功能描述：获得帮助信息）</p></blockquote></li><li><p>显示说明</p></li></ul><table><thead><tr><th>信息</th><th>功能</th></tr></thead><tbody><tr><td>NAME</td><td>命令的名称和单行描述</td></tr><tr><td>SYNOPSIS</td><td>怎样使用命令</td></tr><tr><td>DESCRIPTION</td><td>命令功能的深入讨论</td></tr><tr><td>EXAMPLES</td><td>怎样使用命令的例子</td></tr><tr><td>SEE ALSO</td><td>相关主题（通常是手册页）</td></tr></tbody></table><ul><li>案例实操</li></ul><pre><code class="hljs autoit"><span class="hljs-meta"># 查看ls命令的帮助信息 </span>root<span class="hljs-symbol">@hadoop101</span> ~]<span class="hljs-meta"># man ls</span></code></pre><h2 id="2-help-获得shell内置命令的帮助信息"><a href="#2-help-获得shell内置命令的帮助信息" class="headerlink" title="2 help 获得shell内置命令的帮助信息"></a>2 help 获得shell内置命令的帮助信息</h2><ul><li>基本语法</li></ul><blockquote><p>help 命令   （功能描述：获得shell内置命令的帮助信息）</p></blockquote><ul><li>案例实操</li></ul><pre><code class="hljs autoit"><span class="hljs-meta"># 查看cd命令的帮助信息</span>[root<span class="hljs-symbol">@hadoop101</span> ~]<span class="hljs-meta"># help cd</span></code></pre><h2 id="3-常用快捷键"><a href="#3-常用快捷键" class="headerlink" title="3 常用快捷键"></a>3 常用快捷键</h2><table><thead><tr><th>常用快捷键</th><th>功能</th></tr></thead><tbody><tr><td>ctrl + c</td><td>停止进程</td></tr><tr><td>ctrl+l</td><td>清屏；彻底清屏是：reset</td></tr><tr><td>ctrl + q</td><td>退出</td></tr><tr><td>善于用tab键</td><td>提示(更重要的是可以防止敲错)</td></tr><tr><td>上下键</td><td>查找执行过的命令</td></tr><tr><td>ctrl +alt</td><td>linux和Windows之间切换</td></tr></tbody></table><h1 id="文件目录类"><a href="#文件目录类" class="headerlink" title="文件目录类"></a>文件目录类</h1><h2 id="1-pwd-显示当前工作目录的绝对路径"><a href="#1-pwd-显示当前工作目录的绝对路径" class="headerlink" title="1 pwd 显示当前工作目录的绝对路径"></a>1 pwd 显示当前工作目录的绝对路径</h2><blockquote><p>pwd:print working directory 打印工作目录</p></blockquote><ul><li>基本语法</li></ul><blockquote><p>pwd    （功能描述：显示当前工作目录的绝对路径）</p></blockquote><ul><li>案例实操</li></ul><pre><code class="hljs autoit"><span class="hljs-meta"># 显示当前工作目录的绝对路径</span>root<span class="hljs-symbol">@hadoop101</span> ~]<span class="hljs-meta"># pwd</span>/root</code></pre><h2 id="2-ls-列出目录的内容"><a href="#2-ls-列出目录的内容" class="headerlink" title="2 ls 列出目录的内容"></a>2 ls 列出目录的内容</h2><blockquote><p>ls:list 列出目录内容</p></blockquote><ul><li>基本语法</li></ul><blockquote><p>ls [选项] [目录或是文件]</p></blockquote><ul><li>选项说明</li></ul><table><thead><tr><th>选项</th><th>功能</th></tr></thead><tbody><tr><td>-a</td><td>全部的文件，连同隐藏档( 开头为 . 的文件) 一起列出来(常用)</td></tr><tr><td>-l</td><td>长数据串列出，包含文件的属性与权限等等数据；(常用)</td></tr></tbody></table><ul><li>显示说明</li></ul><blockquote><p>每行列出的信息依次是： 文件类型与权限 链接数 文件属主 文件属组 文件大小用byte来表示 建立或最近修改的时间 名字</p></blockquote><ul><li>案例实操</li></ul><pre><code class="hljs angelscript">➜  MyBlog ls -altotal <span class="hljs-number">1112</span>drwxr-xr-x   <span class="hljs-number">14</span> judezeng  staff     <span class="hljs-number">448</span>  <span class="hljs-number">6</span>  <span class="hljs-number">7</span> <span class="hljs-number">17</span>:<span class="hljs-number">03</span> .drwxr-xr-x+  <span class="hljs-number">49</span> judezeng  staff    <span class="hljs-number">1568</span>  <span class="hljs-number">6</span>  <span class="hljs-number">7</span> <span class="hljs-number">17</span>:<span class="hljs-number">51</span> ..-rw-r--r--@   <span class="hljs-number">1</span> judezeng  staff    <span class="hljs-number">6148</span>  <span class="hljs-number">5</span> <span class="hljs-number">16</span> <span class="hljs-number">16</span>:<span class="hljs-number">06</span> .DS_Storedrwxr-xr-x   <span class="hljs-number">21</span> judezeng  staff     <span class="hljs-number">672</span>  <span class="hljs-number">6</span>  <span class="hljs-number">7</span> <span class="hljs-number">17</span>:<span class="hljs-number">04</span> .deploy_git-rw-r--r--    <span class="hljs-number">1</span> judezeng  staff      <span class="hljs-number">65</span>  <span class="hljs-number">5</span> <span class="hljs-number">16</span> <span class="hljs-number">15</span>:<span class="hljs-number">33</span> .gitignore-rw-r--r--@   <span class="hljs-number">1</span> judezeng  staff    <span class="hljs-number">2464</span>  <span class="hljs-number">5</span> <span class="hljs-number">16</span> <span class="hljs-number">15</span>:<span class="hljs-number">41</span> _config.yml</code></pre><h2 id="3-cd-切换目录"><a href="#3-cd-切换目录" class="headerlink" title="3 cd 切换目录"></a>3 cd 切换目录</h2><blockquote><p>cd:Change Directory切换路径</p></blockquote><ul><li>参数说明</li></ul><table><thead><tr><th>参数</th><th>功能</th></tr></thead><tbody><tr><td>cd 绝对路径</td><td>切换路径</td></tr><tr><td>cd相对路径</td><td>切换路径</td></tr><tr><td>cd ~或者cd</td><td>回到自己的家目录</td></tr><tr><td>cd -</td><td>回到上一次所在目录</td></tr><tr><td>cd ..</td><td>回到当前目录的上一级目录</td></tr><tr><td>cd -P</td><td>跳转到实际物理路径，而非快捷方式路径</td></tr></tbody></table><ul><li>案例实操</li></ul><pre><code class="hljs autoit"><span class="hljs-meta"># （1）使用绝对路径切换到root目录</span>[root<span class="hljs-symbol">@hadoop101</span> ~]<span class="hljs-meta"># cd /root/</span><span class="hljs-meta">#（2）使用相对路径切换到“公共的”目录</span>[root<span class="hljs-symbol">@hadoop101</span> ~]<span class="hljs-meta"># cd 公共的/</span><span class="hljs-meta">#（3）表示回到自己的家目录，亦即是 /root 这个目录</span>[root<span class="hljs-symbol">@hadoop101</span> 公共的]<span class="hljs-meta"># cd ~</span><span class="hljs-meta">#（4）cd- 回到上一次所在目录</span>[root<span class="hljs-symbol">@hadoop101</span> ~]<span class="hljs-meta"># cd -</span><span class="hljs-meta">#（5）表示回到当前目录的上一级目录，亦即是 “/root/公共的”的上一级目录的意思；</span>[root<span class="hljs-symbol">@hadoop101</span> 公共的]<span class="hljs-meta"># cd ..</span></code></pre><h2 id="4-mkdir-创建一个新的目录"><a href="#4-mkdir-创建一个新的目录" class="headerlink" title="4 mkdir 创建一个新的目录"></a>4 mkdir 创建一个新的目录</h2><blockquote><p>mkdir:Make directory 建立目录</p></blockquote><ul><li>选项说明</li></ul><table><thead><tr><th>选项</th><th>功能</th></tr></thead><tbody><tr><td>-p</td><td>创建多层目录</td></tr></tbody></table><ul><li>案例实操</li></ul><pre><code class="hljs autoit"><span class="hljs-meta">#（1）创建一个目录</span>[root<span class="hljs-symbol">@hadoop101</span> ~]<span class="hljs-meta"># mkdir xiyou</span>[root<span class="hljs-symbol">@hadoop101</span> ~]<span class="hljs-meta"># mkdir xiyou/mingjie</span><span class="hljs-meta">#（2）创建一个多级目录</span>[root<span class="hljs-symbol">@hadoop101</span> ~]<span class="hljs-meta"># mkdir -p xiyou/dssz/meihouwang</span></code></pre><h2 id="5-rmdir-删除一个空的目录"><a href="#5-rmdir-删除一个空的目录" class="headerlink" title="5 rmdir 删除一个空的目录"></a>5 rmdir 删除一个空的目录</h2><blockquote><p><em>rmdir</em>:Remove directory 移动目录</p></blockquote><ul><li>案例实操</li></ul><pre><code class="hljs autoit"><span class="hljs-meta"># 删除一个空的文件夹</span>[root<span class="hljs-symbol">@hadoop101</span> ~]<span class="hljs-meta"># rmdir xiyou/dssz/meihouwang</span></code></pre><h2 id="6-touch-创建空文件"><a href="#6-touch-创建空文件" class="headerlink" title="6 touch 创建空文件"></a>6 touch 创建空文件</h2><ul><li>案例实操</li></ul><pre><code class="hljs autoit">[root<span class="hljs-symbol">@hadoop101</span> ~]<span class="hljs-meta"># touch xiyou/dssz/sunwukong.txt</span></code></pre><h2 id="7-cp-复制文件或目录"><a href="#7-cp-复制文件或目录" class="headerlink" title="7 cp 复制文件或目录"></a>7 cp 复制文件或目录</h2><ul><li>基本语法</li></ul><blockquote><p>cp [选项] source dest             （功能描述：复制source文件到dest）</p></blockquote><ul><li>选项说明</li></ul><table><thead><tr><th>选项</th><th>功能</th></tr></thead><tbody><tr><td>-r</td><td>递归复制整个文件夹</td></tr></tbody></table><ul><li>参数说明</li></ul><table><thead><tr><th>参数</th><th>功能</th></tr></thead><tbody><tr><td>source</td><td>源文件</td></tr><tr><td>dest</td><td>目标文件</td></tr></tbody></table><ul><li>经验技巧</li></ul><blockquote><p>强制覆盖不提示的方法：\cp</p></blockquote><ul><li>案例实操</li></ul><pre><code class="hljs autoit"><span class="hljs-meta"># （1）复制文件</span>[root<span class="hljs-symbol">@hadoop101</span> ~]<span class="hljs-meta"># cp xiyou/dssz/suwukong.txt xiyou/mingjie/</span><span class="hljs-meta"># （2）递归复制整个文件夹</span>[root<span class="hljs-symbol">@hadoop101</span> ~]<span class="hljs-meta"># cp -r xiyou/dssz/ ./</span></code></pre><h2 id="8-rm-移除文件或目录"><a href="#8-rm-移除文件或目录" class="headerlink" title="8 rm 移除文件或目录"></a>8 rm 移除文件或目录</h2><ul><li>选项说明</li></ul><table><thead><tr><th>选项</th><th>功能</th></tr></thead><tbody><tr><td>-r</td><td>递归删除目录中所有内容</td></tr><tr><td>-f</td><td>强制执行删除操作，而不提示用于进行确认。</td></tr><tr><td>-v</td><td>显示指令的详细执行过程</td></tr></tbody></table><ul><li>案例实操</li></ul><pre><code class="hljs autoit"><span class="hljs-meta">#（1）删除目录中的内容</span>[root<span class="hljs-symbol">@hadoop101</span> ~]<span class="hljs-meta"># rm xiyou/mingjie/sunwukong.txt</span><span class="hljs-meta">#（2）递归删除目录中所有内容</span>[root<span class="hljs-symbol">@hadoop101</span> ~]<span class="hljs-meta"># rm -rf dssz/</span></code></pre><h2 id="9-mv-移动文件与目录或重命名"><a href="#9-mv-移动文件与目录或重命名" class="headerlink" title="9 mv 移动文件与目录或重命名"></a>9 mv 移动文件与目录或重命名</h2><ul><li>基本语法</li></ul><blockquote><p>（1）mv oldNameFile newNameFile  （功能描述：重命名）</p><p>（2）mv /temp/movefile /targetFolder （功能描述：移动文件）</p></blockquote><ul><li>案例实操</li></ul><pre><code class="hljs autoit"><span class="hljs-meta">#（1）重命名</span>[root<span class="hljs-symbol">@hadoop101</span> ~]<span class="hljs-meta"># mv xiyou/dssz/suwukong.txt xiyou/dssz/houge.txt</span><span class="hljs-meta">#（2）移动文件</span>[root<span class="hljs-symbol">@hadoop101</span> ~]<span class="hljs-meta"># mv xiyou/dssz/houge.txt ./</span></code></pre><h2 id="10-cat-查看文件内容"><a href="#10-cat-查看文件内容" class="headerlink" title="10 cat 查看文件内容"></a>10 cat 查看文件内容</h2><blockquote><p>查看文件内容，从第一行开始显示</p></blockquote><ul><li>选项说明</li></ul><table><thead><tr><th>选项</th><th>功能描述</th></tr></thead><tbody><tr><td>-n</td><td>显示所有行的行号，包括空行。</td></tr></tbody></table><ul><li>经验技巧</li></ul><blockquote><p>一般查看比较小的文件，一屏幕能显示全的。</p></blockquote><ul><li>案例实操</li></ul><pre><code class="hljs elixir"><span class="hljs-comment"># 查看文件内容并显示行号</span>[root<span class="hljs-variable">@hadoop101</span> ~]<span class="hljs-variable">$ </span>cat -n houge.txt</code></pre><h2 id="11-more-文件内容分屏查看器"><a href="#11-more-文件内容分屏查看器" class="headerlink" title="11 more 文件内容分屏查看器"></a>11 more 文件内容分屏查看器</h2><blockquote><p>more指令是一个基于VI编辑器的文本过滤器，它以全屏幕的方式按页显示文本文件的内容。more指令中内置了若干快捷键，详见操作说明</p></blockquote><ul><li>操作说明</li></ul><table><thead><tr><th>操作</th><th>功能说明</th></tr></thead><tbody><tr><td>空白键 (space)</td><td>代表向下翻一页；</td></tr><tr><td>Enter</td><td>代表向下翻『一行』；</td></tr><tr><td>q</td><td>代表立刻离开 more ，不再显示该文件内容。</td></tr><tr><td>Ctrl+F</td><td>向下滚动一屏</td></tr><tr><td>Ctrl+B</td><td>返回上一屏</td></tr><tr><td>=</td><td>输出当前行的行号</td></tr><tr><td>:f</td><td>输出文件名和当前行的行号</td></tr></tbody></table><ul><li>案例实操</li></ul><pre><code class="hljs coffeescript"><span class="hljs-comment"># 采用more查看文件</span>[root@hadoop101 ~]<span class="hljs-comment"># more smartd.conf</span></code></pre><h2 id="12-less-分屏显示文件内容"><a href="#12-less-分屏显示文件内容" class="headerlink" title="12 less 分屏显示文件内容"></a>12 less 分屏显示文件内容</h2><blockquote><p>less指令用来分屏查看文件内容，它的功能与more指令类似，但是比more指令更加强大，支持各种显示终端。less指令在显示文件内容时，并不是一次将整个文件加载之后才显示，而是根据显示需要加载内容，对于显示大型文件具有较高的效率。</p></blockquote><ul><li>操作说明</li></ul><table><thead><tr><th>操作</th><th>功能说明</th></tr></thead><tbody><tr><td>空白键</td><td>向下翻动一页；</td></tr><tr><td>[pagedown]</td><td>向下翻动一页</td></tr><tr><td>[pageup]</td><td>向上翻动一页；</td></tr><tr><td>/字串</td><td>向下搜寻『字串』的功能；n：向下查找；N：向上查找；</td></tr><tr><td>?字串</td><td>向上搜寻『字串』的功能；n：向上查找；N：向下查找；</td></tr><tr><td>q</td><td>离开 less 这个程序；</td></tr></tbody></table><h2 id="13-echo"><a href="#13-echo" class="headerlink" title="13 echo"></a>13 echo</h2><blockquote><p>echo输出内容到控制台</p></blockquote><ul><li>基本语法</li></ul><blockquote><p>echo [选项] [输出内容]</p><p>选项: -e： 支持反斜线控制的字符转换</p></blockquote><table><thead><tr><th>控制字符</th><th>作用</th></tr></thead><tbody><tr><td>\</td><td>输出\本身</td></tr><tr><td>\n</td><td>换行符</td></tr><tr><td>\t</td><td>制表符，也就是Tab键</td></tr></tbody></table><ul><li>案例实操</li></ul><pre><code class="hljs elixir">[root<span class="hljs-variable">@hadoop101</span> ~]<span class="hljs-variable">$ </span>echo <span class="hljs-string">"hello\tworld"</span>hello\tworld[root<span class="hljs-variable">@hadoop101</span> ~]<span class="hljs-variable">$ </span>echo -e <span class="hljs-string">"hello\tworld"</span>helloworld</code></pre><h2 id="14-head-显示文件头部内容"><a href="#14-head-显示文件头部内容" class="headerlink" title="14 head 显示文件头部内容"></a>14 head 显示文件头部内容</h2><blockquote><p>head用于显示文件的开头部分内容，默认情况下head指令显示文件的前10行内容。</p></blockquote><ul><li>基本语法</li></ul><blockquote><p>head 文件      （功能描述：查看文件头10行内容）</p><p>head -n 5 文件   （功能描述：查看文件头5行内容，5可以是任意行数）</p></blockquote><ul><li>选项说明</li></ul><table><thead><tr><th>选项</th><th>功能</th></tr></thead><tbody><tr><td>-n&lt;行数&gt;</td><td>指定显示头部内容的行数</td></tr></tbody></table><pre><code class="hljs angelscript"># 查看文件的头<span class="hljs-number">2</span>行[<span class="hljs-symbol">root@</span>hadoop101 ~]# head -n <span class="hljs-number">2</span> smartd.conf</code></pre><h2 id="15-tail-输出文件尾部内容"><a href="#15-tail-输出文件尾部内容" class="headerlink" title="15 tail 输出文件尾部内容"></a>15 tail 输出文件尾部内容</h2><blockquote><p>tail用于输出文件中尾部的内容，默认情况下tail指令显示文件的前10行内容。</p></blockquote><ul><li>基本语法</li></ul><blockquote><p>（1）tail 文件          （功能描述：查看文件头10行内容）</p><p>（2）tail -n 5 文件      （功能描述：查看文件头5行内容，5可以是任意行数）</p><p>（3）tail -f 文件      （功能描述：实时追踪该文档的所有更新）</p></blockquote><table><thead><tr><th>选项</th><th>功能</th></tr></thead><tbody><tr><td>-n&lt;行数&gt;</td><td>输出文件尾部n行内容</td></tr><tr><td>-f</td><td>显示文件最新追加的内容，监视文件变化</td></tr></tbody></table><ul><li>案例实操</li></ul><pre><code class="hljs angelscript"># （<span class="hljs-number">1</span>）查看文件头<span class="hljs-number">1</span>行内容[<span class="hljs-symbol">root@</span>hadoop101 ~]# tail -n <span class="hljs-number">1</span> smartd.conf # （<span class="hljs-number">2</span>）实时追踪该档的所有更新[<span class="hljs-symbol">root@</span>hadoop101 ~]# tail -f houge.txt</code></pre><h2 id="16-gt-输出重定向和-gt-gt-追加"><a href="#16-gt-输出重定向和-gt-gt-追加" class="headerlink" title="16 &gt; 输出重定向和 &gt;&gt; 追加"></a>16 &gt; 输出重定向和 &gt;&gt; 追加</h2><ul><li>基本语法</li></ul><blockquote><p>（1）ls -l &gt;文件      （功能描述：列表的内容写入文件a.txt中（<strong>覆盖写</strong>））</p><p>（2）ls -al &gt;&gt;文件    （功能描述：列表的内容<strong>追加</strong>到文件aa.txt的末尾）</p><p>（3）cat 文件1 &gt; 文件2  （功能描述：将文件1的内容覆盖到文件2）</p><p>（4）echo “内容” &gt;&gt; 文件</p></blockquote><ul><li>案例实操</li></ul><pre><code class="hljs autoit"><span class="hljs-meta"># （1）将ls查看信息写入到文件中</span>[root<span class="hljs-symbol">@hadoop101</span> ~]<span class="hljs-meta"># ls -l&gt;houge.txt</span><span class="hljs-meta"># （2）将ls查看信息追加到文件中</span>[root<span class="hljs-symbol">@hadoop101</span> ~]<span class="hljs-meta"># ls -l&gt;&gt;houge.txt</span><span class="hljs-meta"># （3）采用echo将hello单词追加到文件中</span>[root<span class="hljs-symbol">@hadoop101</span> ~]<span class="hljs-meta"># echo hello&gt;&gt;houge.txt</span></code></pre><h2 id="17-ln-软链接"><a href="#17-ln-软链接" class="headerlink" title="17 ln 软链接"></a>17 ln 软链接</h2><blockquote><p>软链接也成为符号链接，类似于windows里的快捷方式，有自己的数据块，主要存放了链接其他文件的路径。</p></blockquote><ul><li>基本语法</li></ul><blockquote><p>ln -s [原文件或目录] [软链接名]    （功能描述：给原文件创建一个软链接）</p></blockquote><ul><li>经验技巧</li></ul><blockquote><p>删除软链接： rm -rf 软链接名，而不是rm -rf 软链接名/</p></blockquote><blockquote><p>查询：通过ll就可以查看，列表属性第1位是l，尾部会有位置指向。</p></blockquote><ul><li>案例实操</li></ul><pre><code class="hljs elixir"><span class="hljs-comment">#（1）创建软连接</span>[root<span class="hljs-variable">@hadoop101</span> ~]<span class="hljs-comment"># mv houge.txt xiyou/dssz/</span>[root<span class="hljs-variable">@hadoop101</span> ~]<span class="hljs-comment"># ln -s xiyou/dssz/houge.txt ./houzi</span>[root<span class="hljs-variable">@hadoop101</span> ~]<span class="hljs-comment"># ll</span>lrwxrwxrwx. <span class="hljs-number">1</span> root    root      <span class="hljs-number">20 6</span>月  <span class="hljs-number">17 12</span><span class="hljs-symbol">:</span><span class="hljs-number">56</span> houzi -&gt; xiyou/dssz/houge.txt<span class="hljs-comment">#（2）删除软连接</span>[root<span class="hljs-variable">@hadoop101</span> ~]<span class="hljs-comment"># rm -rf houzi</span><span class="hljs-comment">#（3）进入软连接实际物理路径</span>[root<span class="hljs-variable">@hadoop101</span> ~]<span class="hljs-comment"># ln -s xiyou/dssz/ ./dssz</span>[root<span class="hljs-variable">@hadoop101</span> ~]<span class="hljs-comment"># cd -P dssz/</span></code></pre><h2 id="18-history-查看已经执行过历史命令"><a href="#18-history-查看已经执行过历史命令" class="headerlink" title="18 history 查看已经执行过历史命令"></a>18 history 查看已经执行过历史命令</h2><pre><code class="hljs autoit"><span class="hljs-meta"># (1)查看已经执行过的历史命令</span>[root<span class="hljs-symbol">@hadoop101</span> test1]<span class="hljs-meta"># history</span></code></pre><h1 id="时间日期类"><a href="#时间日期类" class="headerlink" title="时间日期类"></a>时间日期类</h1><ul><li>基本语法</li></ul><blockquote><p>date [OPTION]… [+FORMAT]</p></blockquote><ul><li>选项说明</li></ul><table><thead><tr><th>选项</th><th>功能</th></tr></thead><tbody><tr><td>-d&lt;时间字符串&gt;</td><td>显示指定的“时间字符串”表示的时间，而非当前时间</td></tr><tr><td>-s&lt;日期时间&gt;</td><td>设置系统日期时间</td></tr></tbody></table><ul><li>参数说明</li></ul><table><thead><tr><th>参数</th><th>功能</th></tr></thead><tbody><tr><td>&lt;+日期时间格式&gt;</td><td>指定显示时使用的日期时间格式</td></tr></tbody></table><h2 id="1-date-显示当前时间"><a href="#1-date-显示当前时间" class="headerlink" title="1 date 显示当前时间"></a>1 date 显示当前时间</h2><ul><li>基本语法</li></ul><blockquote><p>（1）date                            （功能描述：显示当前时间）</p><p>（2）date +%Y                        （功能描述：显示当前年份）</p><p>（3）date +%m                        （功能描述：显示当前月份）</p><p>（4）date +%d                        （功能描述：显示当前是哪一天）</p><p>（5）date “+%Y-%m-%d %H:%M:%S”       （功能描述：显示年月日时分秒</p></blockquote><ul><li>案例实操</li></ul><pre><code class="hljs angelscript">#（<span class="hljs-number">1</span>）显示当前时间信息[<span class="hljs-symbol">root@</span>hadoop101 ~]# date<span class="hljs-number">2017</span>年 <span class="hljs-number">06</span>月 <span class="hljs-number">19</span>日 星期一 <span class="hljs-number">20</span>:<span class="hljs-number">53</span>:<span class="hljs-number">30</span> CST#（<span class="hljs-number">2</span>）显示当前时间年月日[<span class="hljs-symbol">root@</span>hadoop101 ~]# date +%Y%m%d<span class="hljs-number">20170619</span>#（<span class="hljs-number">3</span>）显示当前时间年月日时分秒[<span class="hljs-symbol">root@</span>hadoop101 ~]# date <span class="hljs-string">"+%Y-%m-%d %H:%M:%S"</span><span class="hljs-number">2017</span><span class="hljs-number">-06</span><span class="hljs-number">-19</span> <span class="hljs-number">20</span>:<span class="hljs-number">54</span>:<span class="hljs-number">58</span></code></pre><h2 id="2-date-显示非当前时间"><a href="#2-date-显示非当前时间" class="headerlink" title="2 date 显示非当前时间"></a>2 date 显示非当前时间</h2><ul><li>基本语法</li></ul><blockquote><p>（1）date -d ‘1 days ago’          （功能描述：显示前一天时间）</p><p>（2）date -d ‘-1 days ago’          （功能描述：显示明天时间）</p></blockquote><ul><li>案例实操</li></ul><pre><code class="hljs angelscript">#（<span class="hljs-number">1</span>）显示前一天[<span class="hljs-symbol">root@</span>hadoop101 ~]# date -d <span class="hljs-string">'1 days ago'</span><span class="hljs-number">2017</span>年 <span class="hljs-number">06</span>月 <span class="hljs-number">18</span>日 星期日 <span class="hljs-number">21</span>:<span class="hljs-number">07</span>:<span class="hljs-number">22</span> CST#（<span class="hljs-number">2</span>）显示明天时间[<span class="hljs-symbol">root@</span>hadoop101 ~]#date -d <span class="hljs-string">'-1 days ago'</span><span class="hljs-number">2017</span>年 <span class="hljs-number">06</span>月 <span class="hljs-number">20</span>日 星期日 <span class="hljs-number">21</span>:<span class="hljs-number">07</span>:<span class="hljs-number">22</span> CST</code></pre><h2 id="3-date-设置系统时间"><a href="#3-date-设置系统时间" class="headerlink" title="3 date 设置系统时间"></a>3 date 设置系统时间</h2><ul><li>基本语法</li></ul><blockquote><p>date -s 字符串时间</p></blockquote><ul><li>案例实操</li></ul><pre><code class="hljs autoit"><span class="hljs-meta">#（1）设置系统当前时间</span>[root<span class="hljs-symbol">@hadoop101</span> ~]<span class="hljs-meta"># date -s <span class="hljs-string">"2017-06-19 20:52:18"</span></span></code></pre><h2 id="4-cal-查看日历"><a href="#4-cal-查看日历" class="headerlink" title="4 cal 查看日历"></a>4 cal 查看日历</h2><ul><li>基本语法</li></ul><blockquote><p>cal [选项]           （功能描述：不加选项，显示本月日历）</p></blockquote><table><thead><tr><th>选项</th><th>功能</th></tr></thead><tbody><tr><td>具体某一年</td><td>显示这一年的日历</td></tr></tbody></table><ul><li>案例实操</li></ul><pre><code class="hljs angelscript">#（<span class="hljs-number">1</span>）查看当前月的日历[<span class="hljs-symbol">root@</span>hadoop101 ~]# cal#（<span class="hljs-number">2</span>）查看<span class="hljs-number">2017</span>年的日历[<span class="hljs-symbol">root@</span>hadoop101 ~]# cal <span class="hljs-number">2017</span></code></pre><h1 id="用户管理命令"><a href="#用户管理命令" class="headerlink" title="用户管理命令"></a>用户管理命令</h1><h2 id="1-useradd-添加新用户"><a href="#1-useradd-添加新用户" class="headerlink" title="1 useradd 添加新用户"></a>1 useradd 添加新用户</h2><ul><li>基本语法</li></ul><blockquote><p>useradd 用户名          （功能描述：添加新用户）</p><p>useradd -g 组名 用户名   （功能描述：添加新用户到某个组）</p></blockquote><pre><code class="hljs autoit"><span class="hljs-meta"># 添加一个用户</span>[root<span class="hljs-symbol">@hadoop101</span> ~]<span class="hljs-meta"># useradd tangseng</span>[root<span class="hljs-symbol">@hadoop101</span> ~]<span class="hljs-meta">#ll /home/</span></code></pre><h2 id="2-passwd-设置用户密码"><a href="#2-passwd-设置用户密码" class="headerlink" title="2 passwd 设置用户密码"></a>2 passwd 设置用户密码</h2><ul><li>基本语法</li></ul><blockquote><p>passwd 用户名   （功能描述：设置用户密码）</p></blockquote><pre><code class="hljs autoit"><span class="hljs-meta"># 设置用户的密码</span>[root<span class="hljs-symbol">@hadoop101</span> ~]<span class="hljs-meta"># passwd tangseng</span></code></pre><h2 id="3-id-查看用户是否存在"><a href="#3-id-查看用户是否存在" class="headerlink" title="3 id 查看用户是否存在"></a>3 id 查看用户是否存在</h2><pre><code class="hljs autoit"><span class="hljs-meta"># 查看用户是否存在</span>[root<span class="hljs-symbol">@hadoop101</span> ~]<span class="hljs-meta">#id tangseng</span></code></pre><h2 id="4-cat-etc-passwd-查看创建了哪些用户"><a href="#4-cat-etc-passwd-查看创建了哪些用户" class="headerlink" title="4 cat /etc/passwd 查看创建了哪些用户"></a>4 cat /etc/passwd 查看创建了哪些用户</h2><pre><code class="hljs autoit">基本语法[root<span class="hljs-symbol">@hadoop101</span> ~]<span class="hljs-meta"># cat  /etc/passwd</span></code></pre><h2 id="5-su-切换用户"><a href="#5-su-切换用户" class="headerlink" title="5 su 切换用户"></a>5 su 切换用户</h2><blockquote><p>su: swith user 切换用户</p></blockquote><ul><li>基本语法</li></ul><blockquote><p>su 用户名称  （功能描述：切换用户，只能获得用户的执行权限，不能获得环境变量）</p></blockquote><blockquote><p>su - 用户名称    （功能描述：切换到用户并获得该用户的环境变量及执行权限）</p></blockquote><ul><li>案例实操</li></ul><pre><code class="hljs awk"><span class="hljs-comment"># 切换用户</span>[root@hadoop101 ~]<span class="hljs-comment">#su tangseng</span>[root@hadoop101 ~]<span class="hljs-comment">#echo $PATH</span><span class="hljs-regexp">/usr/</span>lib64<span class="hljs-regexp">/qt-3.3/</span>bin:<span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/sbin:/u</span>sr<span class="hljs-regexp">/local/</span>bin:<span class="hljs-regexp">/sbin:/</span>bin:<span class="hljs-regexp">/usr/</span>sbin:<span class="hljs-regexp">/usr/</span>bin:<span class="hljs-regexp">/root/</span>bin[root@hadoop101 ~]<span class="hljs-comment">#exit</span>[root@hadoop101 ~]<span class="hljs-comment">#su - tangseng</span>[root@hadoop101 ~]<span class="hljs-comment">#echo $PATH</span><span class="hljs-regexp">/usr/</span>lib64<span class="hljs-regexp">/qt-3.3/</span>bin:<span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/bin:/</span>bin:<span class="hljs-regexp">/usr/</span>bin:<span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/sbin:/u</span>sr<span class="hljs-regexp">/sbin:/</span>sbin:<span class="hljs-regexp">/home/</span>tangseng<span class="hljs-regexp">/bin</span></code></pre><h2 id="6-userdel-删除用户"><a href="#6-userdel-删除用户" class="headerlink" title="6 userdel 删除用户"></a>6 userdel 删除用户</h2><ul><li>基本语法</li></ul><blockquote><p>（1）userdel 用户名     （功能描述：删除用户但保存用户主目录）</p></blockquote><blockquote><p>（2）userdel -r 用户名    （功能描述：用户和用户主目录，都删除）</p></blockquote><table><thead><tr><th>选项</th><th>功能</th></tr></thead><tbody><tr><td>-r</td><td>删除用户的同时，删除与用户相关的所有文件。</td></tr></tbody></table><ul><li>案例实操</li></ul><pre><code class="hljs autoit"><span class="hljs-meta">#（1）删除用户但保存用户主目录</span>[root<span class="hljs-symbol">@hadoop101</span> ~]<span class="hljs-meta">#userdel tangseng</span>[root<span class="hljs-symbol">@hadoop101</span> ~]<span class="hljs-meta">#ll /home/</span><span class="hljs-meta">#（2）删除用户和用户主目录，都删除</span>[root<span class="hljs-symbol">@hadoop101</span> ~]<span class="hljs-meta">#useradd zhubajie</span>[root<span class="hljs-symbol">@hadoop101</span> ~]<span class="hljs-meta">#ll /home/</span>[root<span class="hljs-symbol">@hadoop101</span> ~]<span class="hljs-meta">#userdel -r zhubajie</span>[root<span class="hljs-symbol">@hadoop101</span> ~]<span class="hljs-meta">#ll /home/</span></code></pre><h2 id="7-who-查看登录用户信息"><a href="#7-who-查看登录用户信息" class="headerlink" title="7 who 查看登录用户信息"></a>7 who 查看登录用户信息</h2><ul><li>基本语法</li></ul><blockquote><p>（1）whoami        （功能描述：显示自身用户名称）</p><p>（2）who am i       （功能描述：显示登录用户的用户名）</p></blockquote><ul><li>案例实操</li></ul><pre><code class="hljs autoit"><span class="hljs-meta">#（1）显示自身用户名称</span>[root<span class="hljs-symbol">@hadoop101</span> opt]<span class="hljs-meta"># whoami</span><span class="hljs-meta">#（2）显示登录用户的用户名</span>[root<span class="hljs-symbol">@hadoop101</span> opt]<span class="hljs-meta"># who am i</span></code></pre><h2 id="8-sudo-设置普通用户具有root权限"><a href="#8-sudo-设置普通用户具有root权限" class="headerlink" title="8 sudo 设置普通用户具有root权限"></a>8 sudo 设置普通用户具有root权限</h2><ul><li>添加jude用户，并对其设置密码</li></ul><pre><code class="hljs autoit">[root<span class="hljs-symbol">@hadoop101</span> ~]<span class="hljs-meta">#useradd jude</span>[root<span class="hljs-symbol">@hadoop101</span> ~]<span class="hljs-meta">#passwd jude</span></code></pre><ul><li>修改配置文件</li></ul><pre><code class="hljs autoit">[root<span class="hljs-symbol">@hadoop101</span> ~]<span class="hljs-meta">#vi /etc/sudoers</span></code></pre><blockquote><p>修改 /etc/sudoers 文件，找到下面一行(91行)，在root下面添加一行，如下所示：</p></blockquote><pre><code class="hljs pgsql">## Allow root <span class="hljs-keyword">to</span> run <span class="hljs-keyword">any</span> commands anywhereroot    <span class="hljs-keyword">ALL</span>=(<span class="hljs-keyword">ALL</span>)     <span class="hljs-keyword">ALL</span>jude  <span class="hljs-keyword">ALL</span>=(<span class="hljs-keyword">ALL</span>)     <span class="hljs-keyword">ALL</span></code></pre><blockquote><p>或者配置成采用sudo命令时，不需要输入密码</p></blockquote><pre><code class="hljs pgsql">## Allow root <span class="hljs-keyword">to</span> run <span class="hljs-keyword">any</span> commands anywhereroot      <span class="hljs-keyword">ALL</span>=(<span class="hljs-keyword">ALL</span>)     <span class="hljs-keyword">ALL</span>jude   <span class="hljs-keyword">ALL</span>=(<span class="hljs-keyword">ALL</span>)     NOPASSWD:<span class="hljs-keyword">ALL</span></code></pre><blockquote><p>修改完毕，现在可以用jude帐号登录，然后用命令 sudo ，即可获得root权限进行操作。</p></blockquote><ul><li>案例实操</li></ul><pre><code class="hljs ruby"><span class="hljs-comment">#（1）用普通用户在/opt目录下创建一个文件夹</span>[jude@hadoop101 opt]$ sudo mkdir <span class="hljs-class"><span class="hljs-keyword">module</span></span>[root@hadoop101 opt]<span class="hljs-comment"># chown jude:jude module/</span></code></pre><h2 id="9-usermod-修改用户"><a href="#9-usermod-修改用户" class="headerlink" title="9 usermod 修改用户"></a>9 usermod 修改用户</h2><ul><li>基本语法</li></ul><blockquote><p>usermod -g 用户组 用户名</p></blockquote><table><thead><tr><th>选项</th><th>功能</th></tr></thead><tbody><tr><td>-g</td><td>修改用户的初始登录组，给定的组必须存在。默认组id是1。</td></tr></tbody></table><ul><li>案例实操</li></ul><pre><code class="hljs autoit"><span class="hljs-meta"># 将用户加入到用户组</span>[root<span class="hljs-symbol">@hadoop101</span> opt]<span class="hljs-meta">#usermod -g root zhubajie</span></code></pre><h1 id="用户组管理命令"><a href="#用户组管理命令" class="headerlink" title="用户组管理命令"></a>用户组管理命令</h1><blockquote><p>每个用户都有一个用户组，系统可以对一个用户组中的所有用户进行集中管理。不同Linux 系统对用户组的规定有所不同，</p><p>如Linux下的用户属于与它同名的用户组，这个用户组在创建用户时同时创建。</p><p>用户组的管理涉及用户组的添加、删除和修改。组的增加、删除和修改实际上就是对/etc/group文件的更新。</p></blockquote><h2 id="1-groupadd-新增组"><a href="#1-groupadd-新增组" class="headerlink" title="1 groupadd 新增组"></a>1 groupadd 新增组</h2><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 添加一个xitianqujing组</span>[root@hadoop101 opt]#groupadd xitianqujing</code></pre><h2 id="2-groupdel-删除组"><a href="#2-groupdel-删除组" class="headerlink" title="2 groupdel 删除组"></a>2 groupdel 删除组</h2><pre><code class="hljs autoit"><span class="hljs-meta"># 删除xitianqujing组</span>[root<span class="hljs-symbol">@hadoop101</span> opt]<span class="hljs-meta"># groupdel xitianqujing</span></code></pre><h2 id="3-groupmod-修改组"><a href="#3-groupmod-修改组" class="headerlink" title="3 groupmod 修改组"></a>3 groupmod 修改组</h2><ul><li>基本语法</li></ul><blockquote><p>groupmod -n 新组名 老组名</p></blockquote><ul><li>选项说明</li></ul><table><thead><tr><th>选项</th><th>功能描述</th></tr></thead><tbody><tr><td>-n&lt;新组名&gt;</td><td>指定工作组的新组名</td></tr></tbody></table><ul><li>案例实操</li></ul><pre><code class="hljs autoit">[root<span class="hljs-symbol">@hadoop101</span> ~]<span class="hljs-meta">#groupadd xitianqujing</span>[root<span class="hljs-symbol">@hadoop101</span> ~]<span class="hljs-meta"># groupmod -n xitian xitianqujing</span></code></pre><h2 id="4-cat-etc-group-查看创建了哪些组"><a href="#4-cat-etc-group-查看创建了哪些组" class="headerlink" title="4 cat /etc/group 查看创建了哪些组"></a>4 cat /etc/group 查看创建了哪些组</h2><pre><code class="hljs autoit">[root<span class="hljs-symbol">@hadoop101</span> hadoop01]<span class="hljs-meta"># cat  /etc/group</span></code></pre><h1 id="文件权限类"><a href="#文件权限类" class="headerlink" title="文件权限类"></a>文件权限类</h1><h2 id="1-文件属性"><a href="#1-文件属性" class="headerlink" title="1 文件属性"></a>1 文件属性</h2><blockquote><p>Linux系统是一种典型的多用户系统，不同的用户处于不同的地位，拥有不同的权限。为了保护系统的安全性，Linux系统对不同的用户访问同一文件（包括目录文件）的权限做了不同的规定。在Linux中我们可以使用ll或者ls -l命令来显示一个文件的属性以及文件所属的用户和组。</p></blockquote><ul><li>从左到右的10个字符表示</li></ul><table><thead><tr><th>文件类型</th><th>属主权限</th><th>属组权限</th><th>其他用户权限</th></tr></thead><tbody><tr><td>0</td><td>1   2  3</td><td>4  5   6</td><td>7  8  9</td></tr><tr><td>d</td><td>r   w  x</td><td>r   -   x</td><td>r   -   x</td></tr><tr><td>目录文件</td><td>读 写  执行</td><td>读 写  执行</td><td>读 写  执行</td></tr></tbody></table><blockquote><p>如果没有权限，就会出现减号[ - ]而已。从左至右用0-9这些数字来表示:</p></blockquote><blockquote><p>（1）0首位表示类型,在Linux中第一个字符代表这个文件是目录、文件或链接文件等等</p></blockquote><blockquote><blockquote><p><code>-</code> 代表文件</p></blockquote></blockquote><blockquote><blockquote><p><code>d</code> 代表目录</p></blockquote></blockquote><blockquote><blockquote><p><code>l</code>链接文档(link file)；</p></blockquote></blockquote><blockquote><p>（2）第1-3位确定属主（该文件的所有者）拥有该文件的权限。—User</p></blockquote><blockquote><p>（3）第4-6位确定属组（所有者的同组用户）拥有该文件的权限，—Group</p></blockquote><blockquote><p>（4）第7-9位确定其他用户拥有该文件的权限 —Other</p></blockquote><ul><li><p>rxw作用文件和目录的不同解释</p><ul><li>（1）作用到文件：</li></ul></li></ul><blockquote><p>[ r ]代表可读(read): 可以读取，查看</p></blockquote><blockquote><p>[ w ]代表可写(write): 可以修改，但是不代表可以删除该文件，删除一个文件的前提条件是对该文件所在的目录有写权限，才能删除该文件.</p></blockquote><pre><code>&gt;[ x ]代表可执行(execute):可以被系统执行&gt;&gt;</code></pre><ul><li><p>（2）作用到目录：</p><blockquote><p>[ r ]代表可读(read): 可以读取，ls查看目录内容</p></blockquote><blockquote><p>[ w ]代表可写(write): 可以修改，目录内创建+删除+重命名目录</p></blockquote><blockquote><p>[ x ]代表可执行(execute):可以进入该目录</p></blockquote></li></ul><h2 id="2-chmod-改变权限"><a href="#2-chmod-改变权限" class="headerlink" title="2 chmod 改变权限"></a>2 chmod 改变权限</h2><ul><li>基本语法</li></ul><blockquote><p>u:所有者 g:所有组 o:其他人 a:所有人(u、g、o的总和)</p></blockquote><blockquote><p>第一种方式变更权限</p><p>chmod [{ugoa}{+-=}{rwx}] 文件或目录</p><p>第二种方式变更权限</p><p>chmod [mode=421 ] [文件或目录]</p></blockquote><ul><li>案例实操</li></ul><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash">（1）修改文件使其所属主用户具有执行权限</span>[root@hadoop101 ~]# cp xiyou/dssz/houge.txt ./[root@hadoop101 ~]# chmod u+x houge.txt<span class="hljs-meta">#</span><span class="bash">（2）修改文件使其所属组用户具有执行权限</span>[root@hadoop101 ~]# chmod g+x houge.txt<span class="hljs-meta">#</span><span class="bash">（3）修改文件所属主用户执行权限,并使其他用户具有执行权限</span>[root@hadoop101 ~]# chmod u-x,o+x houge.txt<span class="hljs-meta">#</span><span class="bash">（4）采用数字的方式，设置文件所有者、所属组、其他用户都具有可读可写可执行权限。</span>[root@hadoop101 ~]# chmod 777 houge.txt<span class="hljs-meta">#</span><span class="bash">（5）修改整个文件夹里面的所有文件的所有者、所属组、其他用户都具有可读可写可执行权限。</span>[root@hadoop101 ~]# chmod -R 777 xiyou/</code></pre>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>基本命令</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MySQL面试题(一)</title>
    <link href="/2018/03/10/mysql2/"/>
    <url>/2018/03/10/mysql2/</url>
    
    <content type="html"><![CDATA[<h1 id="MyISAM与InnoDB的区别"><a href="#MyISAM与InnoDB的区别" class="headerlink" title="MyISAM与InnoDB的区别"></a>MyISAM与InnoDB的区别</h1><table><thead><tr><th>对比项</th><th>MyISAM</th><th>InnoDB</th></tr></thead><tbody><tr><td>外键</td><td>不支持</td><td>支持</td></tr><tr><td>事务</td><td>不支持</td><td>支持</td></tr><tr><td>行表锁</td><td>表锁，即使操作一条记录也会锁住整个表，不适合高并发的操作</td><td>行锁,操作时只锁某一行，不对其它行有影响，适合高并发的操作</td></tr><tr><td>缓存</td><td>只缓存索引，不缓存真实数据</td><td>不仅缓存索引还要缓存真实数据，对内存要求较高，而且内存大小对性能有决定性的影响</td></tr></tbody></table><h1 id="索引优化"><a href="#索引优化" class="headerlink" title="索引优化"></a>索引优化</h1><ul><li>数据结构：<code>B+Tree</code></li></ul><blockquote><p>一般来说能够达到range就可以算是优化了 idx name_deptId</p></blockquote><ul><li>口诀（两个法则加6种索引失效的情况）</li></ul><blockquote><p>值匹配我最爱，最左前缀要遵守；</br><br>带头大哥不能死，中间兄弟不能断；</br><br>索引列上少计算，范围之后全失效；</br><br>LIKE百分写最右，覆盖索引不写*；</br><br>不等空值还有OR，索引影响要注意；</br><br>VAR引号不可丢，SQL优化有诀窍。</br></p></blockquote><ul><li><code>b-tree</code>和<code>b+tree</code>的区别<ul><li>B-树的关键字、索引和记录是放在一起的， B+树的非叶子节点中只有关键字和指向下一个节点的索引，记录只放在叶子节点中。</li><li>在B-树中，越靠近根节点的记录查找时间越快，只要找到关键字即可确定记录的存在；而B+树中每个记录的查找时间基本是一样的，都需要从根节点走到叶子节点，而且在叶子节点中还要再比较关键字</li></ul></li></ul><h1 id="MySQL的事务"><a href="#MySQL的事务" class="headerlink" title="MySQL的事务"></a>MySQL的事务</h1><h2 id="事务的基本要素（ACID）"><a href="#事务的基本要素（ACID）" class="headerlink" title="事务的基本要素（ACID）"></a>事务的基本要素（ACID）</h2><ul><li>原子性（Atomicity）：事务开始后所有操作，要么全部做完，要么全部不做，不可能停滞在中间环节。事务执行过程中出错，会回滚到事务开始前的状态，所有的操作就像没有发生一样。也就是说事务是一个不可分割的整体，就像化学中学过的原子，是物质构成的基本单位</li><li>一致性（Consistency）：事务开始前和结束后，数据库的完整性约束没有被破坏 。比如A向B转账，不可能A扣了钱，B却没收到</li><li>隔离性（Isolation）：同一时间，只允许一个事务请求同一数据，不同的事务之间彼此没有任何干扰。比如A正在从一张银行卡中取钱，在A取钱的过程结束前，B不能向这张卡转账。</li><li>持久性（Durability）：事务完成后，事务对数据库的所有更新将被保存到数据库，不能回滚。</li></ul><h2 id="事务的并发问题"><a href="#事务的并发问题" class="headerlink" title="事务的并发问题"></a>事务的并发问题</h2><ul><li><strong>脏读</strong>：事务A读取了事务B更新的数据，然后B回滚操作，那么A读取到的数据是脏数据</li><li><strong>不可重复读</strong>：事务 A 多次读取同一数据，事务 B 在事务A多次读取的过程中，对数据作了更新并提交，导致事务A多次读取同一数据时，结果 不一致</li><li><strong>幻读</strong>：系统管理员A将数据库中所有学生的成绩从具体分数改为ABCDE等级，但是系统管理员B就在这个时候插入了一条具体分数的记录，当系统管理员A改结束后发现还有一条记录没有改过来，就好像发生了幻觉一样，这就叫幻读。<blockquote><p>小结：不可重复读的和幻读很容易混淆，不可重复读侧重于修改，幻读侧重于新增或删除。解决不可重复读的问题只需锁住满足条件的行，解决幻读需要<strong>锁表</strong></p></blockquote></li></ul><h2 id="MySQL事务隔离级别"><a href="#MySQL事务隔离级别" class="headerlink" title="MySQL事务隔离级别"></a>MySQL事务隔离级别</h2><table><thead><tr><th>事务隔离级别</th><th>脏读</th><th>不可重复读</th><th>幻读</th></tr></thead><tbody><tr><td>读未提交（read-uncommitted）</td><td>是</td><td>是</td><td>是</td></tr><tr><td>不可重复读（read-committed）</td><td>否</td><td>是</td><td>是</td></tr><tr><td>可重复读（repeatable-read）</td><td>否</td><td>否</td><td>是</td></tr><tr><td>串行化（serializable）</td><td>否</td><td>否</td><td>否</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>MySQL</category>
      
    </categories>
    
    
    <tags>
      
      <tag>面试题</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数仓工具hive(六)：Hive中常用函数汇总</title>
    <link href="/2017/08/18/hive6/"/>
    <url>/2017/08/18/hive6/</url>
    
    <content type="html"><![CDATA[<h1 id="常用日期函数"><a href="#常用日期函数" class="headerlink" title="常用日期函数"></a>常用日期函数</h1><pre><code class="hljs vbscript">unix_timestamp:返回当前或指定时间的时间戳    from_uni xtime：将时间戳转为日期格式current_date：当前日期current_timestamp：当前的日期加时间to_date：抽取日期部分<span class="hljs-built_in">year</span>：获取年<span class="hljs-built_in">month</span>：获取月<span class="hljs-built_in">day</span>：获取日<span class="hljs-built_in">hour</span>：获取时<span class="hljs-built_in">minute</span>：获取分<span class="hljs-built_in">second</span>：获取秒weekofyear：当前时间是一年中的第几周dayofmonth：当前时间是一个月中的第几天months_between： 两个日期间的月份add_months：日期加减月<span class="hljs-built_in">datediff</span>：两个日期相差的天数  前减后date_add：日期加天数date_sub：日期减天数last_day：日期的当月的最后一天</code></pre><h1 id="常用取整函数"><a href="#常用取整函数" class="headerlink" title="常用取整函数"></a>常用取整函数</h1><pre><code class="hljs gauss"><span class="hljs-built_in">round</span>： 四舍五入<span class="hljs-built_in">ceil</span>：  向上取整<span class="hljs-built_in">select</span> <span class="hljs-built_in">ceil</span>(<span class="hljs-number">10.0102</span>)   <span class="hljs-comment">//11</span><span class="hljs-built_in">floor</span>： 向下取整<span class="hljs-built_in">select</span> <span class="hljs-built_in">floor</span>(<span class="hljs-number">10.99</span>)   <span class="hljs-comment">//10</span></code></pre><h1 id="常用字符串操作函数"><a href="#常用字符串操作函数" class="headerlink" title="常用字符串操作函数"></a>常用字符串操作函数</h1><pre><code class="hljs pgsql">upper： 转大写lower： 转小写length： 长度trim：  前后去空格lpad： 向左补齐，到指定长度rpad：  向右补齐，到指定长度regexp_replace： <span class="hljs-keyword">SELECT</span> regexp_replace(<span class="hljs-string">'100-200'</span>, <span class="hljs-string">'(\\d+)'</span>, <span class="hljs-string">'num'</span>)=<span class="hljs-string">'num-num</span><span class="hljs-string">使用正则表达式匹配目标字符串，匹配成功后替换！</span></code></pre><h1 id="常用集合操作"><a href="#常用集合操作" class="headerlink" title="常用集合操作"></a>常用集合操作</h1><pre><code class="hljs xquery">size： 集合中元素的个数map_keys： 返回<span class="hljs-keyword">map</span>中<span class="hljs-built_in">的key</span>map_values: 返回<span class="hljs-keyword">map</span>中的<span class="hljs-keyword">value</span>array_contains: 判断<span class="hljs-keyword">array</span>中是否包含某个元素sort_array： 将<span class="hljs-keyword">array</span>中的元素排序</code></pre>]]></content>
    
    
    <categories>
      
      <category>Hive</category>
      
    </categories>
    
    
    <tags>
      
      <tag>总结</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hive50道sql必练题</title>
    <link href="/2017/07/22/hive1-2/"/>
    <url>/2017/07/22/hive1-2/</url>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>50道Hive之sql必练题，做完50道题会对HiveSql有一个比较深的认识，且附上答案以及验证结果。</p><h1 id="建表"><a href="#建表" class="headerlink" title="建表"></a>建表</h1><pre><code class="hljs sql"><span class="hljs-comment">-- 学生表</span><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> student(s_id <span class="hljs-keyword">string</span>,s_name <span class="hljs-keyword">string</span>,s_birth <span class="hljs-keyword">string</span>,s_sex <span class="hljs-keyword">string</span>) <span class="hljs-keyword">row</span> <span class="hljs-keyword">format</span> <span class="hljs-keyword">delimited</span> <span class="hljs-keyword">fields</span> <span class="hljs-keyword">terminated</span> <span class="hljs-keyword">by</span> <span class="hljs-string">'\,'</span>;<span class="hljs-comment">-- 课堂表</span><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> course(c_id <span class="hljs-keyword">string</span>,c_name <span class="hljs-keyword">string</span>,t_id <span class="hljs-keyword">string</span>) <span class="hljs-keyword">row</span> <span class="hljs-keyword">format</span> <span class="hljs-keyword">delimited</span> <span class="hljs-keyword">fields</span> <span class="hljs-keyword">terminated</span> <span class="hljs-keyword">by</span> <span class="hljs-string">'\,'</span>;<span class="hljs-comment">-- 教师表</span><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> teacher(t_id <span class="hljs-keyword">string</span>,t_name <span class="hljs-keyword">string</span>) <span class="hljs-keyword">row</span> <span class="hljs-keyword">format</span> <span class="hljs-keyword">delimited</span> <span class="hljs-keyword">fields</span> <span class="hljs-keyword">terminated</span> <span class="hljs-keyword">by</span> <span class="hljs-string">'\,'</span>;<span class="hljs-comment">-- 分数表</span><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> score(s_id <span class="hljs-keyword">string</span>,c_id <span class="hljs-keyword">string</span>,s_score <span class="hljs-built_in">int</span>) <span class="hljs-keyword">row</span> <span class="hljs-keyword">format</span> <span class="hljs-keyword">delimited</span> <span class="hljs-keyword">fields</span> <span class="hljs-keyword">terminated</span> <span class="hljs-keyword">by</span> <span class="hljs-string">'\,'</span>;</code></pre><h1 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h1><pre><code class="hljs bash">-- vim /opt/module/data/student.csv01,赵雷,1990-01-01,男02,钱电,1990-12-21,男03,孙风,1990-05-20,男04,李云,1990-08-06,男05,周梅,1991-12-01,女06,吴兰,1992-03-01,女07,郑竹,1989-07-01,女08,王菊,1990-01-20,女-- vim /opt/module/datas/course.csv01,语文,0202,数学,0103,英语,03-- vim /opt/module/datas/teacher.csv01,张三02,李四03,王五-- vim /opt/module/datas/score.csv01,01,8001,02,9001,03,9902,01,7002,02,6002,03,8003,01,8003,02,8003,03,8004,01,5004,02,3004,03,2005,01,7605,02,8706,01,3106,03,3407,02,8907,03,98</code></pre><h1 id="加载"><a href="#加载" class="headerlink" title="加载"></a>加载</h1><pre><code class="hljs sql"><span class="hljs-keyword">load</span> <span class="hljs-keyword">data</span> <span class="hljs-keyword">local</span> inpath <span class="hljs-string">'/opt/module/datas/student.csv'</span> <span class="hljs-keyword">into</span> <span class="hljs-keyword">table</span> student;<span class="hljs-keyword">load</span> <span class="hljs-keyword">data</span> <span class="hljs-keyword">local</span> inpath <span class="hljs-string">'/opt/module/datas/course.csv'</span> <span class="hljs-keyword">into</span> <span class="hljs-keyword">table</span> course;<span class="hljs-keyword">load</span> <span class="hljs-keyword">data</span> <span class="hljs-keyword">local</span> inpath <span class="hljs-string">'/opt/module/datas/teacher.csv'</span> <span class="hljs-keyword">into</span> <span class="hljs-keyword">table</span> teacher;<span class="hljs-keyword">load</span> <span class="hljs-keyword">data</span> <span class="hljs-keyword">local</span> inpath <span class="hljs-string">'/opt/module/datas/score.csv'</span> <span class="hljs-keyword">into</span> <span class="hljs-keyword">table</span> score;</code></pre>]]></content>
    
    
    <categories>
      
      <category>Hive</category>
      
    </categories>
    
    
    <tags>
      
      <tag>练习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ambari+HDP安装的Hive出现中文乱码解决</title>
    <link href="/2017/07/21/hive1-1/"/>
    <url>/2017/07/21/hive1-1/</url>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>公司决定使用Ambari+HDP这一套大数据运维加部署框架去替代CDH，遇到一些问题会及时记录下来</p><h3 id="Hive注释comment出现乱码"><a href="#Hive注释comment出现乱码" class="headerlink" title="Hive注释comment出现乱码"></a>Hive注释comment出现乱码</h3><h5 id="Hive建表语句"><a href="#Hive建表语句" class="headerlink" title="Hive建表语句"></a>Hive建表语句</h5><pre><code class="hljs sql"><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span>  test.mytest_tm1(              <span class="hljs-keyword">id</span> <span class="hljs-built_in">int</span> <span class="hljs-keyword">comment</span><span class="hljs-string">'编号'</span>,              <span class="hljs-keyword">name</span> <span class="hljs-keyword">string</span> <span class="hljs-keyword">comment</span> <span class="hljs-string">'名字'</span>              )<span class="hljs-keyword">row</span> <span class="hljs-keyword">format</span> <span class="hljs-keyword">delimited</span> <span class="hljs-keyword">fields</span> <span class="hljs-keyword">terminated</span> <span class="hljs-keyword">by</span> <span class="hljs-string">'\u0001'</span><span class="hljs-keyword">lines</span> <span class="hljs-keyword">terminated</span> <span class="hljs-keyword">by</span> <span class="hljs-string">'\n'</span><span class="hljs-keyword">stored</span> <span class="hljs-keyword">as</span> textfile;</code></pre><h5 id="Hive的元数据存在Mysql中，而Mysql字符集的默认Latin1，则会出现乱码"><a href="#Hive的元数据存在Mysql中，而Mysql字符集的默认Latin1，则会出现乱码" class="headerlink" title="Hive的元数据存在Mysql中，而Mysql字符集的默认Latin1，则会出现乱码"></a>Hive的元数据存在Mysql中，而Mysql字符集的默认Latin1，则会出现乱码</h5><p><img src="https://i.loli.net/2020/05/17/jTSHORqgXmnyAb4.png" srcset="/img/loading.gif" alt=""></p><h3 id="修改Mysql字符集-latin1-改成-utf-8"><a href="#修改Mysql字符集-latin1-改成-utf-8" class="headerlink" title="修改Mysql字符集 latin1 改成 utf-8"></a>修改Mysql字符集 latin1 改成 utf-8</h3><p>在hive库里面修改表、分区、视图</p><h5 id="修改表字段注解和表注解"><a href="#修改表字段注解和表注解" class="headerlink" title="修改表字段注解和表注解"></a>修改表字段注解和表注解</h5><pre><code class="hljs sql"><span class="hljs-keyword">use</span> hive；<span class="hljs-comment"># mysql元数据库</span><span class="hljs-keyword">alter</span> <span class="hljs-keyword">table</span> COLUMNS_V2 <span class="hljs-keyword">modify</span> <span class="hljs-keyword">column</span> <span class="hljs-keyword">COMMENT</span> <span class="hljs-built_in">varchar</span>(<span class="hljs-number">256</span>) <span class="hljs-built_in">character</span> <span class="hljs-keyword">set</span> utf8；<span class="hljs-keyword">alter</span> <span class="hljs-keyword">table</span> TABLE_PARAMS <span class="hljs-keyword">modify</span> <span class="hljs-keyword">column</span> PARAM_VALUE <span class="hljs-built_in">varchar</span>(<span class="hljs-number">4000</span>) <span class="hljs-built_in">character</span> <span class="hljs-keyword">set</span> utf8；</code></pre><h5 id="修改分区字段注解"><a href="#修改分区字段注解" class="headerlink" title="修改分区字段注解"></a>修改分区字段注解</h5><pre><code class="hljs sql"><span class="hljs-keyword">alter</span> <span class="hljs-keyword">table</span> PARTITION_PARAMS <span class="hljs-keyword">modify</span> <span class="hljs-keyword">column</span> PARAM_VALUE <span class="hljs-built_in">varchar</span>(<span class="hljs-number">4000</span>) <span class="hljs-built_in">character</span> <span class="hljs-keyword">set</span> utf8 ;<span class="hljs-keyword">alter</span> <span class="hljs-keyword">table</span> PARTITION_KEYS <span class="hljs-keyword">modify</span> <span class="hljs-keyword">column</span> PKEY_COMMENT <span class="hljs-built_in">varchar</span>(<span class="hljs-number">4000</span>) <span class="hljs-built_in">character</span> <span class="hljs-keyword">set</span> utf8;</code></pre><h5 id="修改索引注解"><a href="#修改索引注解" class="headerlink" title="修改索引注解"></a>修改索引注解</h5><pre><code class="hljs sql"><span class="hljs-keyword">alter</span> <span class="hljs-keyword">table</span> INDEX_PARAMS <span class="hljs-keyword">modify</span> <span class="hljs-keyword">column</span> PARAM_VALUE <span class="hljs-built_in">varchar</span>(<span class="hljs-number">4000</span>) <span class="hljs-built_in">character</span> <span class="hljs-keyword">set</span> utf8;</code></pre><h3 id="在ambari的UI页面修改-metastore-的连接-URL"><a href="#在ambari的UI页面修改-metastore-的连接-URL" class="headerlink" title="在ambari的UI页面修改 metastore 的连接 URL"></a>在ambari的UI页面修改 metastore 的连接 URL</h3><p>  注意修改完成后要重启Hive</p><pre><code class="hljs crystal"><span class="hljs-symbol">jdbc:</span><span class="hljs-symbol">mysql:</span>/<span class="hljs-regexp">/ip:3306/database</span>?createDatabaseIfNotExist=<span class="hljs-literal">true</span>&amp;amp;useUnicode=<span class="hljs-literal">true</span>&amp;characterEncoding=UTF-<span class="hljs-number">8</span></code></pre><p><img src="https://i.loli.net/2020/05/17/Zrbyo62eTQPnhRd.png" srcset="/img/loading.gif" alt=""></p><h3 id="验证结果"><a href="#验证结果" class="headerlink" title="验证结果"></a>验证结果</h3><p>  注意：必须是新建hive表，就得表字符集已经不可改变。<br><img src="https://i.loli.net/2020/05/17/NIz4uMsr3KvJOCy.png" srcset="/img/loading.gif" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>Hive</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ambari</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数仓工具hive(四)：Hive文件存储格式以及优缺点</title>
    <link href="/2017/06/04/hive4/"/>
    <url>/2017/06/04/hive4/</url>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>Hive支持的存储数的格式主要有：TEXTFILE 、SEQUENCEFILE、ORC、PARQUET。</p><h1 id="行与列存储的特点"><a href="#行与列存储的特点" class="headerlink" title="行与列存储的特点"></a>行与列存储的特点</h1><h2 id="行存储的特点"><a href="#行存储的特点" class="headerlink" title="行存储的特点"></a>行存储的特点</h2><p>查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快。</p><h2 id="列存储的特点"><a href="#列存储的特点" class="headerlink" title="列存储的特点"></a>列存储的特点</h2><p>因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法。</p><h1 id="Hive文件存储格式以及优缺点"><a href="#Hive文件存储格式以及优缺点" class="headerlink" title="Hive文件存储格式以及优缺点"></a>Hive文件存储格式以及优缺点</h1><h2 id="textfile"><a href="#textfile" class="headerlink" title="textfile"></a>textfile</h2><ul><li>默认的文件格式，行存储。建表时不指定存储格式即为textfile，导入数据时把数据文件拷贝至hdfs不进行处理</li><li>优点：最简单的数据格式，便于和其他工具（Pig, grep, sed, awk）共享数据，便于查看和编辑；加载较快</li><li>缺点：耗费存储空间，I/O性能较低；Hive不进行数据切分合并，不能进行并行操作，查询效率低</li><li>场景：适用于小型查询，查看具体数据内容的测试操作</li></ul><h2 id="sequencefile"><a href="#sequencefile" class="headerlink" title="sequencefile"></a>sequencefile</h2><ul><li>含有键值对的二进制文件，行存储</li><li>优点：可压缩、可分割，优化磁盘利用率和I/O；可并行操作数据，查询效率高</li><li>缺点：存储空间消耗最大；对于Hadoop生态系统之外的工具不适用，需要通过text文件转化加载</li><li>场景：适用于数据量较小、大部分列的查询</li></ul><h2 id="rcfile"><a href="#rcfile" class="headerlink" title="rcfile"></a>rcfile</h2><ul><li>存储模式：按列存储，采用行组模式对数据进行存储(数据按行分块,每块按照列存储)</li><li>存储结构：包括(16字节的HDFS同步块信息以及元数据的头部信息主要包括该行组内的存储的行数、列的字段信息)</li><li>存储空间：采用游程编码</li><li>优点：可压缩，高效的列存取；查询效率较高</li><li>缺点：加载时性能消耗较大，需要通过text文件转化加载；读取全量数据性能低</li><li>场景：多数用于存储需要“长期留存”的数据文件</li></ul><h2 id="orcfile"><a href="#orcfile" class="headerlink" title="orcfile"></a>orcfile</h2><ul><li>存储模式：按列存储，所有列存在一个文件中，</li><li>每个ORC文件首先会被横向切分成多个Stripe，而每个Stripe内部以列存储，所有的列存储在一个文件中，而且每个stripe默认的大小是250MB，相对于RCFile默认的行组大小是4MB，所以比RCFile更高效。Postscripts中存储该表的行数，压缩参数，压缩大小，列等信息；Stripe Footer中包含该stripe的统计结果，包括Max，Min，count等信息；FileFooter中包含该表的统计结果，以及各个Stripe的位置信息；IndexData中保存了该stripe上数据的位置信息，总行数等信息；RowData以stream的形式保存了数据的具体信息</li><li>除了游程编码，引入了字典编码和Bit编码</li><li>场景：适用于Hive中大型的存储、查询</li></ul><h2 id="parquet"><a href="#parquet" class="headerlink" title="parquet"></a>parquet</h2><ul><li>存储模式：按列存储，Parquet文件是以二进制方式存储的，不可以直接读取和修改的，文件是自解析的，文件中包括该文件的数据和元数据</li><li>存储结构：行组(Row Group)：按照行将数据物理上划分为多个单元，每一个行组包含一定的行数，在一个HDFS文件中至少存储一个行组，Parquet读写的时候会将整个行组缓存在内存中，所以如果每一个行组的大小是由内存大的小决定的</li></ul>]]></content>
    
    
    <categories>
      
      <category>Hive</category>
      
    </categories>
    
    
    <tags>
      
      <tag>hive存储策略</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数仓工具Hive(三)：Hive数据类型</title>
    <link href="/2017/06/03/hive3/"/>
    <url>/2017/06/03/hive3/</url>
    
    <content type="html"><![CDATA[<h1 id="基本数据类型"><a href="#基本数据类型" class="headerlink" title="基本数据类型"></a>基本数据类型</h1><table><thead><tr><th>Hive数据类型</th><th>Java数据类型</th><th>长度</th><th>例子</th></tr></thead><tbody><tr><td>TINYINT</td><td>byte</td><td>1byte有符号整数</td><td>20</td></tr><tr><td>SMALINT</td><td>short</td><td>2byte有符号整数</td><td>20</td></tr><tr><td>INT</td><td>int</td><td>4byte有符号整数</td><td>20</td></tr><tr><td>BIGINT</td><td>long</td><td>8byte有符号整数</td><td>20</td></tr><tr><td>BOOLEAN</td><td>boolean</td><td>布尔类型，true或者false</td><td>TRUE  FALSE</td></tr><tr><td>FLOAT</td><td>float</td><td>单精度浮点数</td><td>3.14159</td></tr><tr><td>DOUBLE</td><td>double</td><td>双精度浮点数</td><td>3.14159</td></tr><tr><td>STRING</td><td>string</td><td>字符系列。可以指定字符集。可以使用单引号或者双引号。</td><td>‘now is the time’ “for all good men”</td></tr><tr><td>TIMESTAMP</td><td></td><td>时间类型</td><td></td></tr><tr><td>BINARY</td><td></td><td>字节数组</td><td></td></tr></tbody></table><ul><li>对于Hive的String类型相当于数据库的varchar类型，该类型是一个可变的字符串，不过它不能声明其中最多能存储多少个字符，理论上它可以存储2GB的字符数。</li></ul><h1 id="集合数据类型"><a href="#集合数据类型" class="headerlink" title="集合数据类型"></a>集合数据类型</h1><table><thead><tr><th>数据类型</th><th>描述</th><th>语法示例</th></tr></thead><tbody><tr><td>STRUCT</td><td>和c语言中的struct类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是STRUCT{first STRING, last STRING},那么第1个元素可以通过字段.first来引用。</td><td>struct()</td></tr><tr><td>MAP</td><td>MAP是一组键-值对元组集合，使用数组表示法可以访问数据。例如，如果某个列的数据类型是MAP，其中键-&gt;值对是’first’-&gt;’John’和’last’-&gt;’Doe’，那么可以通过字段名[‘last’]获取最后一个元素</td><td>map()</td></tr><tr><td>ARRAY</td><td>数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为[‘John’, ‘Doe’]，那么第2个元素可以通过数组名[1]进行引用。</td><td>Array()</td></tr></tbody></table><ul><li>Hive有三种复杂数据类型ARRAY、MAP和STRUCT。ARRAY和MAP与Java中的Array和Map类似，而STRUCT与C语言中的Struct类似，它封装了一个命名字段集合，复杂数据类型允许任意层次的嵌套。</li></ul><h2 id="案例实操"><a href="#案例实操" class="headerlink" title="案例实操"></a>案例实操</h2><ul><li>假设某表有如下一行，我们用JSON格式来表示其数据结构。在Hive下访问的格式为<pre><code class="hljs json">&#123;    <span class="hljs-attr">"name"</span>: <span class="hljs-string">"songsong"</span>,    <span class="hljs-attr">"friends"</span>: [<span class="hljs-string">"bingbing"</span> , <span class="hljs-string">"lili"</span>] ,       <span class="hljs-comment">//列表Array, </span>    <span class="hljs-attr">"children"</span>: &#123;                      <span class="hljs-comment">//键值Map,</span>        <span class="hljs-attr">"xiao song"</span>: <span class="hljs-number">18</span> ,        <span class="hljs-attr">"xiaoxiao song"</span>: <span class="hljs-number">19</span>    &#125;,    <span class="hljs-attr">"address"</span>: &#123;                      <span class="hljs-comment">//结构Struct,</span>        <span class="hljs-attr">"street"</span>: <span class="hljs-string">"hui long guan"</span> ,        <span class="hljs-attr">"city"</span>: <span class="hljs-string">"beijing"</span>     &#125;&#125;</code></pre></li><li>基于上述数据结构，我们在Hive里创建对应的表，并导入数据。<br>创建本地测试文件test.txt<pre><code class="hljs groovy">songsong,bingbing_lili,xiao <span class="hljs-string">song:</span><span class="hljs-number">18</span>_xiaoxiao <span class="hljs-string">song:</span><span class="hljs-number">19</span>,hui <span class="hljs-keyword">long</span> guan_beijingyangyang,caicai_susu,xiao <span class="hljs-string">yang:</span><span class="hljs-number">18</span>_xiaoxiao <span class="hljs-string">yang:</span><span class="hljs-number">19</span>,chao yang_beijing</code></pre></li></ul><p><strong>注意：MAP，STRUCT和ARRAY里的元素间关系都可以用同一个字符表示，这里用”_”。</strong></p><ul><li>Hive上创建测试表test<pre><code class="hljs sql"><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> <span class="hljs-keyword">test</span>(<span class="hljs-keyword">name</span> <span class="hljs-keyword">string</span>,friends <span class="hljs-built_in">array</span>&lt;<span class="hljs-keyword">string</span>&gt;,children <span class="hljs-keyword">map</span>&lt;<span class="hljs-keyword">string</span>, <span class="hljs-built_in">int</span>&gt;,address <span class="hljs-keyword">struct</span>&lt;street:<span class="hljs-keyword">string</span>, city:<span class="hljs-keyword">string</span>&gt;)<span class="hljs-keyword">row</span> <span class="hljs-keyword">format</span> <span class="hljs-keyword">delimited</span> <span class="hljs-keyword">fields</span> <span class="hljs-keyword">terminated</span> <span class="hljs-keyword">by</span> <span class="hljs-string">','</span>collection items <span class="hljs-keyword">terminated</span> <span class="hljs-keyword">by</span> <span class="hljs-string">'_'</span><span class="hljs-keyword">map</span> <span class="hljs-keyword">keys</span> <span class="hljs-keyword">terminated</span> <span class="hljs-keyword">by</span> <span class="hljs-string">':'</span><span class="hljs-keyword">lines</span> <span class="hljs-keyword">terminated</span> <span class="hljs-keyword">by</span> <span class="hljs-string">'\n'</span>;</code></pre></li><li>建表语句拆解</li></ul><table><thead><tr><th>字段</th><th>解释</th></tr></thead><tbody><tr><td>row format delimited fields terminated by ‘,’</td><td>– 列分隔符</td></tr><tr><td>collection items terminated by ‘_’</td><td>–MAP STRUCT 和 ARRAY 的分隔符(数据分割符号)</td></tr><tr><td>map keys terminated by ‘:’</td><td>– MAP中的key与value的分隔符</td></tr><tr><td>lines terminated by ‘\n’;</td><td>– 行分隔符</td></tr></tbody></table><ul><li>导入文本数据到测试表<pre><code class="hljs pgsql">hive (<span class="hljs-keyword">default</span>)&gt; <span class="hljs-keyword">load</span> data <span class="hljs-keyword">local</span> inpath ‘/opt/module/datas/test.txt’<span class="hljs-keyword">into</span> <span class="hljs-keyword">table</span> test</code></pre></li><li>访问三种集合列里的数据，以下分别是ARRAY，MAP，STRUCT的访问方式<pre><code class="hljs pgsql">hive (<span class="hljs-keyword">default</span>)&gt; <span class="hljs-keyword">select</span> friends[<span class="hljs-number">1</span>],children[<span class="hljs-string">'xiao song'</span>],address.city <span class="hljs-keyword">from</span> test<span class="hljs-keyword">where</span> <span class="hljs-type">name</span>="songsong";OK_c0     _c1     citylili    <span class="hljs-number">18</span>      beijing<span class="hljs-type">Time</span> taken: <span class="hljs-number">0.076</span> seconds, Fetched: <span class="hljs-number">1</span> <span class="hljs-keyword">row</span>(s)</code></pre></li></ul><hr><h1 id="类型转化"><a href="#类型转化" class="headerlink" title="类型转化"></a>类型转化</h1><p>Hive的原子数据类型是可以进行隐式转换的，类似于Java的类型转换，例如某表达式使用INT类型，TINYINT会自动转换为INT类型，但是Hive不会进行反向转化，例如，某表达式使用TINYINT类型，INT不会自动转换为TINYINT类型，它会返回错误，除非使用CAST操作.</p><h2 id="隐式类型转换规则如下"><a href="#隐式类型转换规则如下" class="headerlink" title="隐式类型转换规则如下"></a>隐式类型转换规则如下</h2><ul><li>任何整数类型都可以隐式地转换为一个范围更广的类型，如TINYINT可以转换成INT，INT可以转换成BIGINT。</li><li>所有整数类型、FLOAT和STRING类型都可以隐式地转换成DOUBLE</li><li>TINYINT、SMALLINT、INT都可以转换为FLOAT</li><li>BOOLEAN类型不可以转换为任何其它的类型</li></ul><h2 id="可以使用CAST操作显示进行数据类型转换"><a href="#可以使用CAST操作显示进行数据类型转换" class="headerlink" title="可以使用CAST操作显示进行数据类型转换"></a>可以使用CAST操作显示进行数据类型转换</h2><p>例如CAST(‘1’ AS INT)将把字符串’1’ 转换成整数1；如果强制类型转换失败，如执行CAST(‘X’ AS INT)，表达式返回空值 NULL。</p>]]></content>
    
    
    <categories>
      
      <category>Hive</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据类型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数仓工具hive(二)：Hive安装部署</title>
    <link href="/2017/05/29/hive2/"/>
    <url>/2017/05/29/hive2/</url>
    
    <content type="html"><![CDATA[<h1 id="Hive安装地址"><a href="#Hive安装地址" class="headerlink" title="Hive安装地址"></a>Hive安装地址</h1><ul><li><a href="http://hive.apache.org/" target="_blank" rel="noopener">Hive官网地址</a></li><li><a href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted" target="_blank" rel="noopener">文档查看地址</a></li><li><a href="http://archive.apache.org/dist/hive/" target="_blank" rel="noopener">下载地址</a></li></ul><h1 id="Hive安装部署"><a href="#Hive安装部署" class="headerlink" title="Hive安装部署"></a>Hive安装部署</h1><h2 id="Hive安装及配置"><a href="#Hive安装及配置" class="headerlink" title="Hive安装及配置"></a>Hive安装及配置</h2><ul><li>把apache-hive-1.2.1-bin.tar.gz上传到linux的/opt/software目录下</li><li>解压apache-hive-1.2.1-bin.tar.gz到/opt/module/目录下面<pre><code class="hljs ruby">[root@hadoop102 software]$ tar -zxvf apache-hive-<span class="hljs-number">1.2</span>.<span class="hljs-number">1</span>-bin.tar.gz -C /opt/<span class="hljs-class"><span class="hljs-keyword">module</span>/</span></code></pre></li><li>修改apache-hive-1.2.1-bin.tar.gz的名称为hive<pre><code class="hljs angelscript">[<span class="hljs-symbol">root@</span>hadoop102 module]$ mv apache-hive<span class="hljs-number">-1.2</span><span class="hljs-number">.1</span>-bin/ hive</code></pre></li><li>修改/opt/module/hive/conf目录下的hive-env.sh.template名称为hive-env.sh<pre><code class="hljs mel">[root@hadoop102 conf]$ mv hive-<span class="hljs-keyword">env</span>.sh.template hive-<span class="hljs-keyword">env</span>.sh</code></pre></li><li>配置hive-env.sh文件<pre><code class="hljs bash"><span class="hljs-comment"># 配置HADOOP_HOME路径E</span><span class="hljs-built_in">export</span> HADOOP_HOME=/opt/module/hadoop-2.7.2<span class="hljs-comment"># 配置HIVE_CONF_DIR路径</span><span class="hljs-built_in">export</span> HIVE_CONF_DIR=/opt/module/hive/conf</code></pre></li></ul><h2 id="Hadoop集群配置"><a href="#Hadoop集群配置" class="headerlink" title="Hadoop集群配置"></a>Hadoop集群配置</h2><ul><li>必须启动hdfs和yarn<pre><code class="hljs bash">[root@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh[root@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh</code></pre></li><li>在HDFS上创建/tmp和/user/hive/warehouse两个目录并修改他们的同组权限可写<pre><code class="hljs angelscript">[<span class="hljs-symbol">root@</span>hadoop102 hadoop<span class="hljs-number">-2.7</span><span class="hljs-number">.2</span>]$ bin/hadoop fs -mkdir /tmp[<span class="hljs-symbol">root@</span>hadoop102 hadoop<span class="hljs-number">-2.7</span><span class="hljs-number">.2</span>]$ bin/hadoop fs -mkdir -p /user/hive/warehouse[<span class="hljs-symbol">root@</span>hadoop102 hadoop<span class="hljs-number">-2.7</span><span class="hljs-number">.2</span>]$ bin/hadoop fs -chmod g+w /tmp[<span class="hljs-symbol">root@</span>hadoop102 hadoop<span class="hljs-number">-2.7</span><span class="hljs-number">.2</span>]$ bin/hadoop fs -chmod g+w /user/hive/warehouse</code></pre></li></ul><h2 id="Hive基本操作"><a href="#Hive基本操作" class="headerlink" title="Hive基本操作"></a>Hive基本操作</h2><ul><li>启动hive<pre><code class="hljs elixir">[root<span class="hljs-variable">@hadoop102</span> hive]<span class="hljs-variable">$ </span>bin/hive</code></pre></li><li>查看数据库<pre><code class="hljs abnf">hive&gt; show databases<span class="hljs-comment">;</span></code></pre></li><li>打开默认数据库<pre><code class="hljs actionscript">hive&gt; <span class="hljs-keyword">use</span> <span class="hljs-keyword">default</span>;</code></pre></li><li>显示default数据库中的表<pre><code class="hljs pgsql">hive&gt; <span class="hljs-keyword">show</span> <span class="hljs-keyword">tables</span>;</code></pre></li><li>创建一张表<pre><code class="hljs pgsql">hive&gt; <span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> student(id <span class="hljs-type">int</span>, <span class="hljs-type">name</span> string);</code></pre></li><li>查看表的结构<pre><code class="hljs pgsql">hive&gt; <span class="hljs-keyword">desc</span> student;hive&gt; <span class="hljs-keyword">show</span> <span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> student;</code></pre></li><li>向表中插入数据<pre><code class="hljs n1ql">hive&gt; <span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> student <span class="hljs-keyword">values</span>(<span class="hljs-number">1000</span>,<span class="hljs-string">"ss"</span>);</code></pre></li><li>查询表中数据<pre><code class="hljs n1ql">hive&gt; <span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> student;</code></pre></li><li>退出hive<pre><code class="hljs abnf">hive&gt; quit<span class="hljs-comment">;</span></code></pre></li></ul><hr><h1 id="将本地文件导入Hive案例"><a href="#将本地文件导入Hive案例" class="headerlink" title="将本地文件导入Hive案例"></a>将本地文件导入Hive案例</h1><ul><li>需求 </li></ul><p>将本地/opt/module/datas/student.txt这个目录下的数据导入到hive的student(id int, name string)表中。</p><ul><li><p>数据准备</p><ul><li>在/opt/module/目录下创建datas<pre><code class="hljs elixir">[root<span class="hljs-variable">@hadoop102</span> <span class="hljs-keyword">module</span>]<span class="hljs-variable">$ </span>mkdir datas</code></pre></li><li>在/opt/module/datas/目录下创建student.txt文件并添加数据<pre><code class="hljs angelscript">[<span class="hljs-symbol">root@</span>hadoop102 datas]$ touch student.txt[<span class="hljs-symbol">root@</span>hadoop102 datas]$ vi student.txt<span class="hljs-number">1001</span>zhangshan<span class="hljs-number">1002</span>lishi<span class="hljs-number">1003</span>zhaoliu</code></pre></li></ul></li><li><p>Hive实际操作</p><ul><li>创建student表, 并声明文件分隔符’\t’<pre><code class="hljs pgsql">hive&gt; <span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> student(id <span class="hljs-type">int</span>, <span class="hljs-type">name</span> string) <span class="hljs-keyword">ROW</span> <span class="hljs-keyword">FORMAT</span> DELIMITED FIELDS TERMINATED<span class="hljs-keyword">BY</span> <span class="hljs-string">'\t'</span>;</code></pre></li><li>加载/opt/module/datas/student.txt 文件到student数据库表中<pre><code class="hljs sql"><span class="hljs-keyword">load</span> <span class="hljs-keyword">data</span> <span class="hljs-keyword">local</span> inpath <span class="hljs-string">'/opt/module/datas/student.txt'</span> <span class="hljs-keyword">into</span> <span class="hljs-keyword">table</span> student;</code></pre></li><li>Hive查询结果<pre><code class="hljs yaml"><span class="hljs-string">hive&gt;</span> <span class="hljs-string">select</span> <span class="hljs-string">*</span> <span class="hljs-string">from</span> <span class="hljs-string">student;</span><span class="hljs-string">OK</span><span class="hljs-number">1001</span><span class="hljs-string">zhangshan</span><span class="hljs-number">1002</span><span class="hljs-string">lishi</span><span class="hljs-number">1003</span><span class="hljs-string">zhaoliu</span><span class="hljs-attr">Time taken:</span> <span class="hljs-number">0.266</span> <span class="hljs-string">seconds,</span> <span class="hljs-attr">Fetched:</span> <span class="hljs-number">3</span> <span class="hljs-string">row(s)</span></code></pre></li></ul></li><li><p>遇到的问题</p><ul><li>再打开一个客户端窗口启动hive，会产生java.sql.SQLException异常</li><li>原因是，Metastore默认存储在自带的derby数据库中，推荐使用MySQL存储Metastore;<pre><code class="hljs css"><span class="hljs-selector-tag">Exception</span> <span class="hljs-selector-tag">in</span> <span class="hljs-selector-tag">thread</span> "<span class="hljs-selector-tag">main</span>" <span class="hljs-selector-tag">java</span><span class="hljs-selector-class">.lang</span><span class="hljs-selector-class">.RuntimeException</span>: <span class="hljs-selector-tag">java</span><span class="hljs-selector-class">.lang</span><span class="hljs-selector-class">.RuntimeException</span>: <span class="hljs-selector-tag">Unable</span> <span class="hljs-selector-tag">to</span> <span class="hljs-selector-tag">instantiate</span> <span class="hljs-selector-tag">org</span><span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.hadoop</span><span class="hljs-selector-class">.hive</span><span class="hljs-selector-class">.ql</span><span class="hljs-selector-class">.metadata</span><span class="hljs-selector-class">.SessionHiveMetaStoreClient</span>        <span class="hljs-selector-tag">at</span> <span class="hljs-selector-tag">org</span><span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.hadoop</span><span class="hljs-selector-class">.hive</span><span class="hljs-selector-class">.ql</span><span class="hljs-selector-class">.session</span><span class="hljs-selector-class">.SessionState</span><span class="hljs-selector-class">.start</span>(<span class="hljs-selector-tag">SessionState</span><span class="hljs-selector-class">.java</span><span class="hljs-selector-pseudo">:522)</span>        <span class="hljs-selector-tag">at</span> <span class="hljs-selector-tag">org</span><span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.hadoop</span><span class="hljs-selector-class">.hive</span><span class="hljs-selector-class">.cli</span><span class="hljs-selector-class">.CliDriver</span><span class="hljs-selector-class">.run</span>(<span class="hljs-selector-tag">CliDriver</span><span class="hljs-selector-class">.java</span><span class="hljs-selector-pseudo">:677)</span>        <span class="hljs-selector-tag">at</span> <span class="hljs-selector-tag">org</span><span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.hadoop</span><span class="hljs-selector-class">.hive</span><span class="hljs-selector-class">.cli</span><span class="hljs-selector-class">.CliDriver</span><span class="hljs-selector-class">.main</span>(<span class="hljs-selector-tag">CliDriver</span><span class="hljs-selector-class">.java</span><span class="hljs-selector-pseudo">:621)</span>        <span class="hljs-selector-tag">at</span> <span class="hljs-selector-tag">sun</span><span class="hljs-selector-class">.reflect</span><span class="hljs-selector-class">.NativeMethodAccessorImpl</span><span class="hljs-selector-class">.invoke0</span>(<span class="hljs-selector-tag">Native</span> <span class="hljs-selector-tag">Method</span>)        <span class="hljs-selector-tag">at</span> <span class="hljs-selector-tag">sun</span><span class="hljs-selector-class">.reflect</span><span class="hljs-selector-class">.NativeMethodAccessorImpl</span><span class="hljs-selector-class">.invoke</span>(<span class="hljs-selector-tag">NativeMethodAccessorImpl</span><span class="hljs-selector-class">.java</span><span class="hljs-selector-pseudo">:57)</span>        <span class="hljs-selector-tag">at</span> <span class="hljs-selector-tag">sun</span><span class="hljs-selector-class">.reflect</span><span class="hljs-selector-class">.DelegatingMethodAccessorImpl</span><span class="hljs-selector-class">.invoke</span>(<span class="hljs-selector-tag">DelegatingMethodAccessorImpl</span><span class="hljs-selector-class">.java</span><span class="hljs-selector-pseudo">:43)</span>        <span class="hljs-selector-tag">at</span> <span class="hljs-selector-tag">java</span><span class="hljs-selector-class">.lang</span><span class="hljs-selector-class">.reflect</span><span class="hljs-selector-class">.Method</span><span class="hljs-selector-class">.invoke</span>(<span class="hljs-selector-tag">Method</span><span class="hljs-selector-class">.java</span><span class="hljs-selector-pseudo">:606)</span>        <span class="hljs-selector-tag">at</span> <span class="hljs-selector-tag">org</span><span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.hadoop</span><span class="hljs-selector-class">.util</span><span class="hljs-selector-class">.RunJar</span><span class="hljs-selector-class">.run</span>(<span class="hljs-selector-tag">RunJar</span><span class="hljs-selector-class">.java</span><span class="hljs-selector-pseudo">:221)</span>        <span class="hljs-selector-tag">at</span> <span class="hljs-selector-tag">org</span><span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.hadoop</span><span class="hljs-selector-class">.util</span><span class="hljs-selector-class">.RunJar</span><span class="hljs-selector-class">.main</span>(<span class="hljs-selector-tag">RunJar</span><span class="hljs-selector-class">.java</span><span class="hljs-selector-pseudo">:136)</span><span class="hljs-selector-tag">Caused</span> <span class="hljs-selector-tag">by</span>: <span class="hljs-selector-tag">java</span><span class="hljs-selector-class">.lang</span><span class="hljs-selector-class">.RuntimeException</span>: <span class="hljs-selector-tag">Unable</span> <span class="hljs-selector-tag">to</span> <span class="hljs-selector-tag">instantiate</span> <span class="hljs-selector-tag">org</span><span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.hadoop</span><span class="hljs-selector-class">.hive</span><span class="hljs-selector-class">.ql</span><span class="hljs-selector-class">.metadata</span><span class="hljs-selector-class">.SessionHiveMetaStoreClient</span>        <span class="hljs-selector-tag">at</span> <span class="hljs-selector-tag">org</span><span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.hadoop</span><span class="hljs-selector-class">.hive</span><span class="hljs-selector-class">.metastore</span><span class="hljs-selector-class">.MetaStoreUtils</span><span class="hljs-selector-class">.newInstance</span>(<span class="hljs-selector-tag">MetaStoreUtils</span><span class="hljs-selector-class">.java</span><span class="hljs-selector-pseudo">:1523)</span>        <span class="hljs-selector-tag">at</span> <span class="hljs-selector-tag">org</span><span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.hadoop</span><span class="hljs-selector-class">.hive</span><span class="hljs-selector-class">.metastore</span><span class="hljs-selector-class">.RetryingMetaStoreClient</span>.&lt;<span class="hljs-selector-tag">init</span>&gt;(<span class="hljs-selector-tag">RetryingMetaStoreClient</span><span class="hljs-selector-class">.java</span><span class="hljs-selector-pseudo">:86)</span>        <span class="hljs-selector-tag">at</span> <span class="hljs-selector-tag">org</span><span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.hadoop</span><span class="hljs-selector-class">.hive</span><span class="hljs-selector-class">.metastore</span><span class="hljs-selector-class">.RetryingMetaStoreClient</span><span class="hljs-selector-class">.getProxy</span>(<span class="hljs-selector-tag">RetryingMetaStoreClient</span><span class="hljs-selector-class">.java</span><span class="hljs-selector-pseudo">:132)</span>        <span class="hljs-selector-tag">at</span> <span class="hljs-selector-tag">org</span><span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.hadoop</span><span class="hljs-selector-class">.hive</span><span class="hljs-selector-class">.metastore</span><span class="hljs-selector-class">.RetryingMetaStoreClient</span><span class="hljs-selector-class">.getProxy</span>(<span class="hljs-selector-tag">RetryingMetaStoreClient</span><span class="hljs-selector-class">.java</span><span class="hljs-selector-pseudo">:104)</span>        <span class="hljs-selector-tag">at</span> <span class="hljs-selector-tag">org</span><span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.hadoop</span><span class="hljs-selector-class">.hive</span><span class="hljs-selector-class">.ql</span><span class="hljs-selector-class">.metadata</span><span class="hljs-selector-class">.Hive</span><span class="hljs-selector-class">.createMetaStoreClient</span>(<span class="hljs-selector-tag">Hive</span><span class="hljs-selector-class">.java</span><span class="hljs-selector-pseudo">:3005)</span>        <span class="hljs-selector-tag">at</span> <span class="hljs-selector-tag">org</span><span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.hadoop</span><span class="hljs-selector-class">.hive</span><span class="hljs-selector-class">.ql</span><span class="hljs-selector-class">.metadata</span><span class="hljs-selector-class">.Hive</span><span class="hljs-selector-class">.getMSC</span>(<span class="hljs-selector-tag">Hive</span><span class="hljs-selector-class">.java</span><span class="hljs-selector-pseudo">:3024)</span>        <span class="hljs-selector-tag">at</span> <span class="hljs-selector-tag">org</span><span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.hadoop</span><span class="hljs-selector-class">.hive</span><span class="hljs-selector-class">.ql</span><span class="hljs-selector-class">.session</span><span class="hljs-selector-class">.SessionState</span><span class="hljs-selector-class">.start</span>(<span class="hljs-selector-tag">SessionState</span><span class="hljs-selector-class">.java</span><span class="hljs-selector-pseudo">:503)</span>... 8 <span class="hljs-selector-tag">more</span></code></pre></li></ul></li></ul><hr><h1 id="Hive元数据配置到MySQL"><a href="#Hive元数据配置到MySQL" class="headerlink" title="Hive元数据配置到MySQL"></a>Hive元数据配置到MySQL</h1><h2 id="驱动拷贝"><a href="#驱动拷贝" class="headerlink" title="驱动拷贝"></a>驱动拷贝</h2><ul><li>拷贝/opt/software/mysql-libs/mysql-connector-java-5.1.27目录下的mysql-connector-java-5.1.27-bin.jar到/opt/module/hive/lib/<pre><code class="hljs angelscript">[<span class="hljs-symbol">root@</span>hadoop102 mysql-connector-java<span class="hljs-number">-5.1</span><span class="hljs-number">.27</span>]# cp mysql-connector-java<span class="hljs-number">-5.1</span><span class="hljs-number">.27</span>-bin.jar /opt/module/hive/lib/</code></pre><h2 id="配置Metastore到MySQL"><a href="#配置Metastore到MySQL" class="headerlink" title="配置Metastore到MySQL"></a>配置Metastore到MySQL</h2></li><li>在/opt/module/hive/conf目录下创建一个hive-site.xml<pre><code class="hljs elixir">[root<span class="hljs-variable">@hadoop102</span> conf]<span class="hljs-variable">$ </span>touch hive-site.xml[root<span class="hljs-variable">@hadoop102</span> conf]<span class="hljs-variable">$ </span>vi hive-site.xml</code></pre></li><li>根据官方文档配置参数，拷贝数据到hive-site.xml文件中<pre><code class="hljs xml"><span class="hljs-meta">&lt;?xml version="1.0"?&gt;</span><span class="hljs-meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>jdbc:mysql://hadoop102:3306/metastore?createDatabaseIfNotExist=true<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">description</span>&gt;</span>JDBC connect string for a JDBC metastore<span class="hljs-tag">&lt;/<span class="hljs-name">description</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">description</span>&gt;</span>Driver class name for a JDBC metastore<span class="hljs-tag">&lt;/<span class="hljs-name">description</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>root<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">description</span>&gt;</span>username to use against metastore database<span class="hljs-tag">&lt;/<span class="hljs-name">description</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>000000<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">description</span>&gt;</span>password to use against metastore database<span class="hljs-tag">&lt;/<span class="hljs-name">description</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span></code></pre></li><li>配置完毕后，如果启动hive异常，可以重新启动虚拟机。（重启后，别忘了启动hadoop集群</li></ul><h2 id="多窗口启动Hive测试"><a href="#多窗口启动Hive测试" class="headerlink" title="多窗口启动Hive测试"></a>多窗口启动Hive测试</h2><ul><li>先启动MySQL<pre><code class="hljs elixir">[root<span class="hljs-variable">@hadoop102</span> mysql-libs]<span class="hljs-variable">$ </span>mysql -uroot -p000000</code></pre></li><li>查看有几个数据库<pre><code class="hljs asciidoc">mysql&gt; show databases;<span class="hljs-code">+--------------------+</span>| Database           |<span class="hljs-code">+--------------------+</span>| information<span class="hljs-emphasis">_schema |</span><span class="hljs-emphasis">| mysql             |</span><span class="hljs-emphasis">| performance_</span>schema || test               |<span class="hljs-code">+--------------------+</span></code></pre></li><li>再次打开多个窗口，分别启动hive<pre><code class="hljs elixir">[root<span class="hljs-variable">@hadoop102</span> hive]<span class="hljs-variable">$ </span>bin/hive</code></pre></li><li>启动hive后，回到MySQL窗口查看数据库，显示增加了metastore数据库<pre><code class="hljs asciidoc">mysql&gt; show databases;<span class="hljs-code">+--------------------+</span>| Database           |<span class="hljs-code">+--------------------+</span>| information<span class="hljs-emphasis">_schema |</span><span class="hljs-emphasis">| metastore          |</span><span class="hljs-emphasis">| mysql             |</span><span class="hljs-emphasis">| performance_</span>schema || test               |<span class="hljs-code">+--------------------+</span></code></pre></li></ul><h2 id="HiveJDBC访问"><a href="#HiveJDBC访问" class="headerlink" title="HiveJDBC访问"></a>HiveJDBC访问</h2><ul><li>启动hiveserver2服务<pre><code class="hljs elixir">[root<span class="hljs-variable">@hadoop102</span> hive]<span class="hljs-variable">$ </span>bin/hiveserver2</code></pre></li><li>启动beeline<pre><code class="hljs angelscript">[<span class="hljs-symbol">root@</span>hadoop102 hive]$ bin/beelineBeeline version <span class="hljs-number">1.2</span><span class="hljs-number">.1</span> by Apache Hivebeeline&gt;</code></pre></li><li>连接hiveserver2<pre><code class="hljs asciidoc">beeline&gt; !connect jdbc:hive2://hadoop102:10000（回车）Connecting to jdbc:hive2://hadoop102:10000Enter username for jdbc:hive2://hadoop102:10000: root（回车）Enter password for jdbc:hive2://hadoop102:10000: （直接回车）Connected to: Apache Hive (version 1.2.1)Driver: Hive JDBC (version 1.2.1)Transaction isolation: TRANSACTION<span class="hljs-emphasis">_REPEATABLE_</span>READ0: jdbc:hive2://hadoop102:10000&gt; show databases;<span class="hljs-code">+----------------+</span>--+| database<span class="hljs-emphasis">_name  |</span><span class="hljs-emphasis">+----------------+--+</span><span class="hljs-emphasis">| default        |</span><span class="hljs-emphasis">| hive_</span>db2       |<span class="hljs-code">+----------------+</span>--+</code></pre></li></ul><h2 id="Hive常用交互命令"><a href="#Hive常用交互命令" class="headerlink" title="Hive常用交互命令"></a>Hive常用交互命令</h2><pre><code class="hljs vim">[root@hadoop102 hive]$ bin/hive -<span class="hljs-keyword">help</span>usage: hive -d,--define &lt;key=value&gt;          Variable subsitution <span class="hljs-keyword">to</span> apply <span class="hljs-keyword">to</span> hive                                  commands. <span class="hljs-keyword">e</span>.g. -d A=B <span class="hljs-built_in">or</span> --define A=B    --database <span class="hljs-symbol">&lt;databasename&gt;</span>     Specify the database <span class="hljs-keyword">to</span> use -<span class="hljs-keyword">e</span> <span class="hljs-symbol">&lt;quoted-query-string&gt;</span>         SQL from <span class="hljs-keyword">command</span> <span class="hljs-built_in">line</span> -<span class="hljs-keyword">f</span> <span class="hljs-symbol">&lt;filename&gt;</span>                    SQL from <span class="hljs-keyword">files</span> -H,--<span class="hljs-keyword">help</span>                        <span class="hljs-keyword">Print</span> <span class="hljs-keyword">help</span> information    --hiveconf &lt;property=value&gt;   Use value <span class="hljs-keyword">for</span> given property    --hivevar &lt;key=value&gt;         Variable subsitution <span class="hljs-keyword">to</span> apply <span class="hljs-keyword">to</span> hive                                  commands. <span class="hljs-keyword">e</span>.g. --hivevar A=B -i <span class="hljs-symbol">&lt;filename&gt;</span>                    Initialization SQL <span class="hljs-keyword">file</span> -S,--<span class="hljs-keyword">silent</span>                      Silent <span class="hljs-keyword">mode</span> in interactive <span class="hljs-keyword">shell</span> -v,--<span class="hljs-keyword">verbose</span>                     Verbose <span class="hljs-keyword">mode</span> (<span class="hljs-keyword">echo</span> executed SQL <span class="hljs-keyword">to</span> the console)</code></pre><ul><li>“-e”不进入hive的交互窗口执行sql语句<pre><code class="hljs sql">[root@hadoop102 hive]$ bin/hive -e "<span class="hljs-keyword">select</span> <span class="hljs-keyword">id</span> <span class="hljs-keyword">from</span> student;"</code></pre></li><li>“-f”执行脚本中sql语句<ul><li>在/opt/module/datas目录下创建hivef.sql文件<pre><code class="hljs elixir">[root<span class="hljs-variable">@hadoop102</span> datas]<span class="hljs-variable">$ </span>touch hivef.sql</code></pre></li><li>执行文件中的sql语句<pre><code class="hljs ruby">[root@hadoop102 hive]$ bin/hive -f /opt/<span class="hljs-class"><span class="hljs-keyword">module</span>/<span class="hljs-title">datas</span>/<span class="hljs-title">hivef</span>.<span class="hljs-title">sql</span></span></code></pre></li><li>执行文件中的sql语句并将结果写入文件中<pre><code class="hljs elixir">[root<span class="hljs-variable">@hadoop102</span> hive]<span class="hljs-variable">$ </span>bin/hive -f /opt/<span class="hljs-keyword">module</span>/datas/hivef.sql  &gt; <span class="hljs-regexp">/opt/module</span><span class="hljs-regexp">/datas/hive</span>_result.txt</code></pre></li></ul></li></ul><h2 id="Hive其他命令操作"><a href="#Hive其他命令操作" class="headerlink" title="Hive其他命令操作"></a>Hive其他命令操作</h2><ul><li>退出hive窗口：<pre><code class="hljs awk">hive(default)&gt;<span class="hljs-keyword">exit</span>;hive(default)&gt;quit;</code></pre>在新版的hive中没区别了，在以前的版本是有的：<br>exit:先隐性提交数据，再退出；<br>quit:不提交数据，退出；</li><li>在hive cli命令窗口中如何查看hdfs文件系统<pre><code class="hljs stylus"><span class="hljs-function"><span class="hljs-title">hive</span><span class="hljs-params">(default)</span></span>&gt;dfs -ls /</code></pre></li><li>在hive cli命令窗口中如何查看本地文件系统<pre><code class="hljs stylus"><span class="hljs-function"><span class="hljs-title">hive</span><span class="hljs-params">(default)</span></span>&gt;! ls /opt/module/datas</code></pre></li><li>查看在hive中输入的所有历史命令<pre><code class="hljs elixir">[root<span class="hljs-variable">@hadoop102</span> ~]<span class="hljs-variable">$ </span>cat .hivehistory</code></pre></li></ul><h2 id="参数配置方式"><a href="#参数配置方式" class="headerlink" title="参数配置方式"></a>参数配置方式</h2><ul><li>查看当前所有的配置信息<pre><code class="hljs gams">hive&gt;<span class="hljs-keyword">set</span>;</code></pre></li></ul><h3 id="参数的配置三种方式"><a href="#参数的配置三种方式" class="headerlink" title="参数的配置三种方式"></a>参数的配置三种方式</h3><ul><li>配置文件方式<ul><li>默认配置文件：hive-default.xml,</li><li>用户自定义配置文件：hive-site.xml</li><li>注意：用户自定义配置会覆盖默认配置。<br>另外，Hive也会读入Hadoop的配置，因为Hive是作为Hadoop的客户端启动的，Hive的配置会覆盖Hadoop的配置。配置文件的设定对本机启动的所有Hive进程都有效<h3 id="命令行参数方式"><a href="#命令行参数方式" class="headerlink" title="命令行参数方式"></a>命令行参数方式</h3>启动Hive时，可以在命令行添加-hiveconf param=value来设定参数。<br>例如：<pre><code class="hljs angelscript">[<span class="hljs-symbol">root@</span>hadoop103 hive]$ bin/hive -hiveconf mapred.reduce.tasks=<span class="hljs-number">10</span>;</code></pre>注意：仅对本次hive启动有效</li></ul></li><li>查看参数设置：<pre><code class="hljs swift">hive (<span class="hljs-keyword">default</span>)&gt; <span class="hljs-keyword">set</span> mapred.<span class="hljs-built_in">reduce</span>.tasks;</code></pre></li></ul><h3 id="参数声明方式"><a href="#参数声明方式" class="headerlink" title="参数声明方式"></a>参数声明方式</h3><p>可以在HQL中使用SET关键字设定参数<br>例如：</p><pre><code class="hljs routeros">hive (default)&gt; <span class="hljs-builtin-name">set</span> mapred.reduce.<span class="hljs-attribute">tasks</span>=100;</code></pre><p>注意：仅对本次hive启动有效。</p><ul><li>查看参数设置<pre><code class="hljs swift">hive (<span class="hljs-keyword">default</span>)&gt; <span class="hljs-keyword">set</span> mapred.<span class="hljs-built_in">reduce</span>.tasks;</code></pre>上述三种设定方式的优先级依次递增。即配置文件&lt;命令行参数&lt;参数声明。注意某些系统级的参数，例如log4j相关的设定，必须用前两种方式设定，因为那些参数的读取在会话建立以前已经完成了。</li></ul>]]></content>
    
    
    <categories>
      
      <category>Hive</category>
      
    </categories>
    
    
    <tags>
      
      <tag>部署</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数仓工具Hive(一)：起源</title>
    <link href="/2017/05/28/hive1/"/>
    <url>/2017/05/28/hive1/</url>
    
    <content type="html"><![CDATA[<h1 id="what-is-hive"><a href="#what-is-hive" class="headerlink" title="what is hive"></a>what is hive</h1><ul><li><p>官方文档</p><p>The Apache Hive ™ data warehouse software facilitates reading, writing, and managing large datasets residing in distributed storage using SQL. Structure can be projected onto data already in storage. A command line tool and JDBC driver are provided to connect users to Hive</p></li><li><p>释义</p><p>Apache Hive™数据仓库软件通过使用SQL读、写以及管理在分布式存储中的大型数据集。 可以将结构映射到已经存储的数据上。 提供了命令行工具和JDBC驱动程序将用户连接到Hive</p></li><li><p>起源</p><p>由Facebook开源用于解决海量结构化日志的数据统计，是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类SQL查询功能，而本质上是将HQL转化为MapReduce</p></li><li><p>HQL实现</p><ul><li>Hive处理的数据存储在HDFS</li><li>Hive分析数据底层实现为MapReduce</li><li>执行程序运行在Yarn上</li></ul></li></ul><h1 id="why-is-Hive"><a href="#why-is-Hive" class="headerlink" title="why is Hive"></a>why is Hive</h1><ul><li><p>优点</p><ul><li>操作接口采用类SQL语法，提供快速开发的能力（简单、容易上手）</li><li>避免了去写MapReduce，减少开发人员的学习成本</li><li>Hive的执行延迟比较高，因此Hive常用于数据分析，对实时性要求不高的场合。</li><li>Hive优势在于处理大数据，对于处理小数据没有优势，因为Hive的执行延迟比较高</li><li>Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数</li></ul></li><li><p>缺点</p><ul><li>Hive的HQL表达能力有限，迭代式算法无法表达，数据挖掘方面不擅长</li><li>Hive的效率比较低，Hive自动生成的MapReduce作业，通常情况下不够智能化，Hive调优比较困难，粒度较粗</li></ul></li></ul><h1 id="Hive架构原理"><a href="#Hive架构原理" class="headerlink" title="Hive架构原理"></a>Hive架构原理</h1><p><img src="https://i.loli.net/2020/05/17/KjNfwxvhWnI4C1t.jpg" srcset="/img/loading.gif" alt="Hive架构原理"></p><ul><li><p>用户接口</p><p>CLI（hive shell）、JDBC/ODBC(java访问hive)、WEBUI（浏览器访问hive）</p></li><li><p>元数据:Metastore</p><p>元数据包括：表名、表所属的数据库（默认是default）、表的拥有者、列/分区字段、表的类型（是否是外部表）、表的数据所在目录等</p></li><li><p>Hadoop</p><p>使用HDFS进行存储，使用MapReduce进行计算</p></li><li><p>驱动器Driver</p><ul><li>解析器（SQL Parser）：将SQL字符串转换成抽象语法树AST，这一步一般都用第三方工具库完成，比如antlr；对AST进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误</li><li>编译器（Physical Plan）：将AST编译生成逻辑执行计划</li><li>优化器（Query Optimizer）：对逻辑执行计划进行优化。</li><li>执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于Hive来说，就是MR/Spark</li></ul></li></ul><h1 id="Hive和数据库比较"><a href="#Hive和数据库比较" class="headerlink" title="Hive和数据库比较"></a>Hive和数据库比较</h1><p>  由于 Hive 采用了类似SQL 的查询语言 HQL(Hive Query Language)，因此很容易将 Hive 理解为数据库。其实从结构上来看，Hive 和数据库除了拥有类似的查询语言，再无类似之处。本文将从多个方面来阐述 Hive 和数据库的差异。数据库可以用在 Online 的应用中，但是Hive 是为数据仓库而设计的，清楚这一点，有助于从应用角度理解 Hive 的特性。</p><ul><li><p>查询语言</p><p>  由于SQL被广泛的应用在数据仓库中，因此，专门针对Hive的特性设计了类SQL的查询语言HQL。熟悉SQL开发的开发者可以很方便的使用Hive进行开发</p></li><li><p>Hive 是建立在 Hadoop 之上的，所有 Hive 的数据都是存储在 HDFS 中的。而数据库则可以将数据保存在块设备或者本地文件系统中</p></li><li><p>由于Hive是针对数据仓库应用设计的，而数据仓库的内容是读多写少的。因此，Hive中不建议对数据的改写，所有的数据都是在加载的时候确定好的。而数据库中的数据通常是需要经常进行修改的，因此可以使用 INSERT INTO … VALUES 添加数据，使用 UPDATE … SET修改数据</p></li><li><p>Hive在加载数据的过程中不会对数据进行任何处理，甚至不会对数据进行扫描，因此也没有对数据中的某些Key建立索引。Hive要访问数据中满足条件的特定值时，需要暴力扫描整个数据，因此访问延迟较高。由于 MapReduce 的引入， Hive 可以并行访问数据，因此即使没有索引，对于大数据量的访问，Hive 仍然可以体现出优势。数据库中，通常会针对一个或者几个列建立索引，因此对于少量的特定条件的数据的访问，数据库可以有很高的效率，较低的延迟。由于数据的访问延迟较高，决定了 Hive 不适合在线数据查询</p></li><li><p>Hive中大多数查询的执行是通过 Hadoop 提供的 MapReduce 来实现的。而数据库通常有自己的执行引擎</p></li><li><p>Hive 在查询数据的时候，由于没有索引，需要扫描整个表，因此延迟较高。另外一个导致 Hive 执行延迟高的因素是 MapReduce框架。由于MapReduce 本身具有较高的延迟，因此在利用MapReduce 执行Hive查询时，也会有较高的延迟。相对的，数据库的执行延迟较低。当然，这个低是有条件的，即数据规模较小，当数据规模大到超过数据库的处理能力的时候，Hive的并行计算显然能体现出优势</p></li><li><p>由于Hive是建立在Hadoop之上的，因此Hive的可扩展性是和Hadoop的可扩展性是一致的（世界上最大的Hadoop 集群在 Yahoo!，2009年的规模在4000 台节点左右）。而数据库由于 ACID 语义的严格限制，扩展行非常有限。目前最先进的并行数据库 Oracle 在理论上的扩展能力也只有100台左右</p></li><li><p>由于Hive建立在集群上并可以利用MapReduce进行并行计算，因此可以支持很大规模的数据；对应的，数据库可以支持的数据规模较小</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>Hive</category>
      
    </categories>
    
    
    <tags>
      
      <tag>起源</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大数据生态Hadoop(三)：实现官方自带wordcount案例</title>
    <link href="/2017/05/08/hadoop3/"/>
    <url>/2017/05/08/hadoop3/</url>
    
    <content type="html"><![CDATA[<h1 id="Hadoop官方wordcount示例"><a href="#Hadoop官方wordcount示例" class="headerlink" title="Hadoop官方wordcount示例"></a>Hadoop官方wordcount示例</h1><p>前面的安装准备工作准备好之后，当然要实现下大数据之入门wordcount案例<br>提供版本JDK1.8+Hadoop2.7.2</p><h4 id="在hadoop-2-7-2文件下面创建一个input文件夹"><a href="#在hadoop-2-7-2文件下面创建一个input文件夹" class="headerlink" title="在hadoop-2.7.2文件下面创建一个input文件夹"></a>在hadoop-2.7.2文件下面创建一个input文件夹</h4><pre><code class="hljs bash">[root@hadoop101 hadoop-2.7.2]<span class="hljs-variable">$mkdir</span> input</code></pre><h4 id="在wcinput文件下创建一个wc-input文件"><a href="#在wcinput文件下创建一个wc-input文件" class="headerlink" title="在wcinput文件下创建一个wc.input文件"></a>在wcinput文件下创建一个wc.input文件</h4><pre><code class="hljs bash">[root@hadoop101 hadoop-2.7.2]<span class="hljs-built_in">cd</span> input[root@hadoop101 input]touch wc.input</code></pre><h4 id="编辑wc-input文件"><a href="#编辑wc-input文件" class="headerlink" title="编辑wc.input文件"></a>编辑wc.input文件</h4><pre><code class="hljs bash">[root@hadoop01 input]vim wc.input<span class="hljs-comment"># 文件内容</span>hadoopmapreduceyarnyarn</code></pre><h4 id="wc-input文件加载到hdfs"><a href="#wc-input文件加载到hdfs" class="headerlink" title="wc.input文件加载到hdfs"></a>wc.input文件加载到hdfs</h4><pre><code class="hljs bash">[root@hadoop101 hadoop-2.7.2]hadoop fs -put input/ /tmp/</code></pre><h4 id="运行官方jar包"><a href="#运行官方jar包" class="headerlink" title="运行官方jar包"></a>运行官方jar包</h4><pre><code class="hljs bash">[root@hadoop101 hadoop-2.7.2]hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /tmp/input/  /tmp/output/</code></pre><h4 id="查看wordcount统计词频"><a href="#查看wordcount统计词频" class="headerlink" title="查看wordcount统计词频"></a>查看wordcount统计词频</h4><pre><code class="hljs bash">[root@hadoop101 hadoop-2.7.2]hadoop fs -cat /tmp/output//part-r-00000<span class="hljs-comment"># 统计内容</span>hadoop1mapreduce1yarn2</code></pre>]]></content>
    
    
    <categories>
      
      <category>Hadoop</category>
      
    </categories>
    
    
    <tags>
      
      <tag>实践</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大数据生态Hadoop(二)：hadoop安装部署</title>
    <link href="/2017/05/07/hadoop2/"/>
    <url>/2017/05/07/hadoop2/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>Hadoop</category>
      
    </categories>
    
    
    <tags>
      
      <tag>部署</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大数据生态Hadoop(一)：起源</title>
    <link href="/2017/05/05/hadoop1/"/>
    <url>/2017/05/05/hadoop1/</url>
    
    <content type="html"><![CDATA[<h1 id="What-is-Hadoop"><a href="#What-is-Hadoop" class="headerlink" title="What is Hadoop"></a>What is Hadoop</h1><ul><li>官方文档<br>The Apache™ Hadoop® project develops open-source software for reliable, scalable, distributed computing.</li><li>释义<br>Apache™Hadoop®项目开发用于可靠、可伸缩的分布式计算的开源软件。</li><li>广义<br>广义上来说，Hadoop通常是指一个更广泛的概念——Hadoop生态圈。</li></ul><h1 id="Hadoop起源"><a href="#Hadoop起源" class="headerlink" title="Hadoop起源"></a>Hadoop起源</h1><ul><li>Lucene框架是Doug Cutting开创的开源软件，用Java书写代码，实现与Google类似的全文搜索功能，它提供了全文检索引擎的架构，包括完整的查询引擎和索引引擎。</li><li>2001年年底Lucene成为Apache基金会的一个子项目。</li><li>对于海量数据的场景，Lucene面对与Google同样的困难，存储数据困难，检索速度慢。</li><li>可以说Google是Hadoop的思想之源(Google在大数据方面的三篇论文)<pre><code class="hljs applescript"><span class="hljs-comment"># Google三篇论文</span>GFS <span class="hljs-comment">---&gt;HDFS</span>Map-Reduce <span class="hljs-comment">---&gt;MR</span>BigTable <span class="hljs-comment">---&gt;HBase</span></code></pre></li><li>2003-2004年，Google公开了部分GFS和MapReduce思想的细节，以此为基础Doug Cutting等人用了2年业余时间实现了DFS和MapReduce机制，使Nutch性能飙升。</li><li>2005 年Hadoop 作为 Lucene的子项目 Nutch的一部分正式引入Apache基金会。</li><li>2006 年 3 月份，Map-Reduce和Nutch Distributed File System (NDFS) 分别被纳入到 Hadoop 项目中，Hadoop就此正式诞生，标志着大数据时代来临，而名字来源于Doug Cutting儿子的玩具大象。</li></ul><h1 id="Hadoop三大发行版本"><a href="#Hadoop三大发行版本" class="headerlink" title="Hadoop三大发行版本"></a>Hadoop三大发行版本</h1><ul><li><a href="http://hadoop.apache.org/releases.html" target="_blank" rel="noopener">Apache Hadoop</a><br>版本最原始（最基础）的版本，对于入门学习最好。</li><li><a href="https://www.cloudera.com/downloads/cdh/5-10-0.html" target="_blank" rel="noopener">Cloudera Hadoop</a><br>在大型互联网企业中用的较多。</li><li><a href="https://hortonworks.com/products/data-center/hdp/" target="_blank" rel="noopener">Hortonworks Hadoop</a><br>文档完善、已经被CDH收购</li></ul><h1 id="Hadoop优势"><a href="#Hadoop优势" class="headerlink" title="Hadoop优势"></a>Hadoop优势</h1><ul><li>高可靠性<br>Hadoop底层维护多个数据副本，所以即使Hadoop某个计算元素或存储出现故障，也不会导致数据的丢失。</li><li>高扩展性<br>在集群间分配任务数据，可方便的扩展数以千计的节点。</li><li>高效性<br>在MapReduce的思想下，Hadoop是并行工作的，以加快任务处理速度。</li><li>高容错性<br>能够自动将失败的任务重新分配。</li></ul><h1 id="Hadoop组成"><a href="#Hadoop组成" class="headerlink" title="Hadoop组成"></a>Hadoop组成</h1><ul><li>HDFS<br>Hadoop分布式文件系统(HDFS)是指被设计成适合运行在通用硬件(commodity hardware)上的分布式文件系统（Distributed File System）</li><li>MapRedurce<br>MapReduce是一种编程模型，用于大规模数据集（大于1TB）的并行运算。概念”Map（映射）”和”Reduce（归约）”，是它们的主要思想，都是从函数式编程语言里借来的，还有从矢量编程语言里借来的特性。</li><li>Yarn<br>Apache Hadoop YARN （Yet Another Resource Negotiator，另一种资源协调者）是一种新的 Hadoop 资源管理器，它是一个通用资源管理系统，可为上层应用提供统一的资源管理和调度，它的引入为集群在利用率、资源统一管理和数据共享等方面带来了巨大好处。</li></ul><h1 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h1><ul><li>商品广告推荐</li><li>保险</li><li>物流</li><li>旅游</li><li>…</li></ul><h1 id="Hadoop生态架构图"><a href="#Hadoop生态架构图" class="headerlink" title="Hadoop生态架构图"></a>Hadoop生态架构图</h1><p><img src="http://q7jwslv80.bkt.clouddn.com/hadoop_1_1.png" srcset="/img/loading.gif" alt="hadoop生态架构图"></p>]]></content>
    
    
    <categories>
      
      <category>Hadoop</category>
      
    </categories>
    
    
    <tags>
      
      <tag>起源</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>详解JPS命令</title>
    <link href="/2015/05/17/java1/"/>
    <url>/2015/05/17/java1/</url>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>jps(Java Virtual Machine Process Status Tool)是JDK 1.5提供的一个显示当前所有java进程pid的命令，简单实用，非常适合在linux/unix平台上简单察看当前java进程的一些简单情况。</p><h1 id="unix的ps命令"><a href="#unix的ps命令" class="headerlink" title="unix的ps命令"></a>unix的ps命令</h1><ul><li>用过unix系统里的ps命令，这个命令主要是用来显示当前系统的进程情况，有哪些进程，及其 id。 </li><li>jps也是一样，它的作用是显示当前系统的java进程情况，及其id号。我们可以通过它来查看我们到底启动了几个java进程，因为每一个java程序都会独占一个java虚拟机实例，和他们的进程号，并可通过opt来查看这些进程的详细启动参数。</li></ul><h1 id="JPS使用方法"><a href="#JPS使用方法" class="headerlink" title="JPS使用方法"></a>JPS使用方法</h1><h4 id="使用方法：在当前命令行下打-jps-系统要配置需要JAVA-HOME环境"><a href="#使用方法：在当前命令行下打-jps-系统要配置需要JAVA-HOME环境" class="headerlink" title="使用方法：在当前命令行下打 jps,系统要配置需要JAVA_HOME环境."></a>使用方法：在当前命令行下打 jps,系统要配置需要JAVA_HOME环境.</h4><p>命令：<code>jps</code></p><pre><code class="hljs basic"><span class="hljs-symbol">26177 </span>AmbariServer<span class="hljs-symbol">29041 </span>TimelineReaderServer<span class="hljs-symbol">4082 </span>ServiceMaster<span class="hljs-symbol">32275 </span>Jps</code></pre><h1 id="常用的参数"><a href="#常用的参数" class="headerlink" title="常用的参数"></a>常用的参数</h1><h5 id="q-只显示pid，不显示class名称-jar文件名和传递给main-方法的参数"><a href="#q-只显示pid，不显示class名称-jar文件名和传递给main-方法的参数" class="headerlink" title="-q 只显示pid，不显示class名称,jar文件名和传递给main 方法的参数"></a>-q 只显示pid，不显示class名称,jar文件名和传递给main 方法的参数</h5><p>命令：<code>jpq -q</code></p><pre><code class="hljs angelscript"><span class="hljs-number">26177</span><span class="hljs-number">29041</span><span class="hljs-number">4082</span><span class="hljs-number">32741</span></code></pre><h4 id="m-输出传递给main-方法的参数，在嵌入式jvm上可能是null"><a href="#m-输出传递给main-方法的参数，在嵌入式jvm上可能是null" class="headerlink" title="-m 输出传递给main 方法的参数，在嵌入式jvm上可能是null"></a>-m 输出传递给main 方法的参数，在嵌入式jvm上可能是null</h4><p>命令：<code>jps -m</code></p><pre><code class="hljs basic"><span class="hljs-symbol">6177 </span>AmbariServer<span class="hljs-symbol">29041 </span>TimelineReaderServer<span class="hljs-symbol">32741 </span>RunJar /<span class="hljs-keyword">usr</span>/hdp/<span class="hljs-number">3.1.0.0</span>-<span class="hljs-number">78</span>/hive/lib/hive-metastore-<span class="hljs-number">3.1.0.3.1.0.0</span>-<span class="hljs-number">78.</span>jar org.apache.hadoop.hive.metastore.HiveMetaStore</code></pre><h4 id="l-输出应用程序main-class的完整package名-或者-应用程序的jar文件完整路径名"><a href="#l-输出应用程序main-class的完整package名-或者-应用程序的jar文件完整路径名" class="headerlink" title="-l 输出应用程序main class的完整package名 或者 应用程序的jar文件完整路径名"></a>-l 输出应用程序main class的完整package名 或者 应用程序的jar文件完整路径名</h4><p>命令：<code>jps -l</code></p><pre><code class="hljs basic"><span class="hljs-symbol">26177 </span>org.apache.ambari.server.controller.AmbariServer<span class="hljs-symbol">29041 </span>org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderServer<span class="hljs-symbol">4082 </span>org.apache.hadoop.yarn.service.ServiceMaster</code></pre><h4 id="v-输出传递给JVM的参数"><a href="#v-输出传递给JVM的参数" class="headerlink" title="-v 输出传递给JVM的参数"></a>-v 输出传递给JVM的参数</h4><p>命令：<code>jps -v</code></p><pre><code class="hljs routeros">6177 AmbariServer -XX:<span class="hljs-attribute">NewRatio</span>=3 -XX:+UseConcMarkSweepGC -XX:-UseGCOverheadLimit -XX:<span class="hljs-attribute">CMSInitiatingOccupancyFraction</span>=60 -XX:+CMSClassUnloadingEnabled -Dsun.zip.<span class="hljs-attribute">disableMemoryMapping</span>=<span class="hljs-literal">true</span> -Xms512m -Xmx2048m -XX:<span class="hljs-attribute">MaxPermSize</span>=128m -Djava.security.auth.login.<span class="hljs-attribute">config</span>=/etc/ambari-server/conf/krb5JAASLogin.conf -Djava.security.krb5.<span class="hljs-attribute">conf</span>=/etc/krb5.conf -Djavax.security.auth.<span class="hljs-attribute">useSubjectCredsOnly</span>=<span class="hljs-literal">false</span> -Dcom.sun.jndi.ldap.connect.pool.<span class="hljs-attribute">protocol</span>=plain ssl -Dcom.sun.jndi.ldap.connect.pool.<span class="hljs-attribute">maxsize</span>=20 -Dcom.sun.jndi.ldap.connect.pool.<span class="hljs-attribute">timeout</span>=30000029041 TimelineReaderServer -Dproc_timelinereader -Dhdp.<span class="hljs-attribute">version</span>=3.1.0.0-78 -Djava.net.<span class="hljs-attribute">preferIPv4Stack</span>=<span class="hljs-literal">true</span> -Dhdp.<span class="hljs-attribute">version</span>=3.1.0.0-78 -Dyarn.id.str= -Dyarn.policy.<span class="hljs-attribute">file</span>=hadoop-policy.xml -Djava.io.<span class="hljs-attribute">tmpdir</span>=/var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir -Dyarn.log.<span class="hljs-attribute">dir</span>=/var/log/hadoop-yarn/yarn -Dyarn.log.<span class="hljs-attribute">file</span>=hadoop-yarn-timelinereader-vm10-101-179-203.ksc.com.log -Dyarn.home.<span class="hljs-attribute">dir</span>=/usr/hdp/3.1.0.0-78/hadoop-yarn -Dyarn.root.<span class="hljs-attribute">logger</span>=INFO,console -Djava.library.<span class="hljs-attribute">path</span>=:/usr/hdp/3.1.0.0-78/hadoop/lib/native/Linux-amd64-64:/usr/hdp/3.1.0.0-78/hadoop/lib/native/Linux-amd64-64:/var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir:/usr/hdp/3.1.0.0-78/hadoop/lib/native -Xmx1024m -Dhadoop.log.<span class="hljs-attribute">dir</span>=/var/log/hadoop-yarn/yarn -Dhadoop.log.<span class="hljs-attribute">file</span>=hadoop-yarn-timelinereader-vm10-101-179-203.ksc.com.log -Dhadoop.home.<span class="hljs-attribute">dir</span>=/usr/hdp/3.1.0.0-78/hadoop -Dhadoop.id.<span class="hljs-attribute">str</span>=yarn -Dhadoop.root.<span class="hljs-attribute">logger</span>=INFO,RFA -Dhadoop.policy.<span class="hljs-attribute">file</span>=hadoop-policy.xml -Dhadoop.security.<span class="hljs-attribute">logger</span>=INFO,NullAppender</code></pre><h4 id="附详JPS细文档"><a href="#附详JPS细文档" class="headerlink" title="附详JPS细文档"></a><a href="https://docs.oracle.com/javase/1.5.0/docs/tooldocs/share/jps.html" target="_blank" rel="noopener">附详JPS细文档</a></h4>]]></content>
    
    
    <categories>
      
      <category>Java</category>
      
    </categories>
    
    
    <tags>
      
      <tag>工具</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux(三)：系统常用命令</title>
    <link href="/2015/02/25/linux3/"/>
    <url>/2015/02/25/linux3/</url>
    
    <content type="html"><![CDATA[<h1 id="配置网络ip地址"><a href="#配置网络ip地址" class="headerlink" title="配置网络ip地址"></a>配置网络ip地址</h1><ul><li><code>ifconfig</code> 配置网络接口</li><li><code>ping</code>测试主机之间网络连通性</li><li>修改IP地址:<code>vim /etc/sysconfig/network-scripts/ifcfg-eth0</code><pre><code class="hljs bash">EVICE=eth0                <span class="hljs-comment">#接口名（设备,网卡）</span>HWADDR=00:0C:2x:6x:0x:xx   <span class="hljs-comment">#MAC地址 </span>TYPE=Ethernet               <span class="hljs-comment">#网络类型（通常是Ethemet）</span>UUID=926a57ba-92c6-4231-bacb-f27e5e6a9f44  <span class="hljs-comment">#随机id</span><span class="hljs-comment">#系统启动的时候网络接口是否有效（yes/no）</span>ONBOOT=yes                <span class="hljs-comment"># IP的配置方法[none|static|bootp|dhcp]（引导时不使用协议|静态分配IP|BOOTP协议|DHCP协议）</span>BOOTPROTO=static      <span class="hljs-comment">#IP地址</span>IPADDR=192.168.1.101   <span class="hljs-comment">#网关  </span>GATEWAY=192.168.1.2      <span class="hljs-comment">#域名解析器</span>DNS1=192.168.1.2</code></pre></li></ul><h1 id="配置主机名"><a href="#配置主机名" class="headerlink" title="配置主机名"></a>配置主机名</h1><ul><li>hostname 显示和设置系统的主机名称</li><li>修改主机名称:<code>vi /etc/sysconfig/network</code>,注意：主机名称不要有“_”下划线</li></ul><h1 id="关闭防火墙"><a href="#关闭防火墙" class="headerlink" title="关闭防火墙"></a>关闭防火墙</h1><h2 id="service-后台服务管理"><a href="#service-后台服务管理" class="headerlink" title="service 后台服务管理"></a>service 后台服务管理</h2><ul><li>基本语法<pre><code class="hljs routeros">service  服务名 start（功能描述：开启服务）service  服务名 stop（功能描述：关闭服务）service  服务名 restart（功能描述：重新启动服务）service  服务名 status（功能描述：查看服务状态</code></pre></li><li>经验技巧:查看服务的方法：<code>/etc/init.d/</code>服务名</li><li>案例实操<pre><code class="hljs routeros">service<span class="hljs-built_in"> network </span>statusservice<span class="hljs-built_in"> network </span>stopservice<span class="hljs-built_in"> network </span>startservice<span class="hljs-built_in"> network </span>restartservice --status-all #查看系统中所有的后台服务</code></pre></li></ul><h2 id="chkconfig-设置后台服务的自启配置"><a href="#chkconfig-设置后台服务的自启配置" class="headerlink" title="chkconfig 设置后台服务的自启配置"></a>chkconfig 设置后台服务的自启配置</h2><ul><li><p>基本语法</p><pre><code class="hljs properties"><span class="hljs-attr">chkconfig</span>    <span class="hljs-string">（功能描述：查看所有服务器自启配置）</span><span class="hljs-attr">chkconfig</span>  <span class="hljs-string">服务名 off   （功能描述：关掉指定服务的自动启动）</span><span class="hljs-attr">chkconfig</span>  <span class="hljs-string">服务名 on   （功能描述：开启指定服务的自动启动）</span><span class="hljs-attr">chkconfig</span>  <span class="hljs-string">服务名 --list（功能描述：查看服务开机启动状态）</span></code></pre></li><li><p>案例实操</p><pre><code class="hljs nginx"><span class="hljs-attribute">chkconfig</span> iptables <span class="hljs-literal">off</span> <span class="hljs-comment"># 关闭iptables服务的自动启动</span>chkconfig iptables <span class="hljs-literal">on</span>  <span class="hljs-comment"># 开启iptables服务的自动启动</span></code></pre></li></ul><h2 id="进程运行级别"><a href="#进程运行级别" class="headerlink" title="进程运行级别"></a>进程运行级别</h2><p><img src="https://i.loli.net/2020/05/24/dm7rgZQjGvlitpA.png" srcset="/img/loading.gif" alt="Linux进程运行级别"></p><h2 id="关闭防火墙-1"><a href="#关闭防火墙-1" class="headerlink" title="关闭防火墙"></a>关闭防火墙</h2><ul><li>临时关闭防火墙<pre><code class="hljs routeros">service iptables statusservice iptables stop # 临时关闭防火墙</code></pre></li><li>开机启动时关闭防火墙<pre><code class="hljs cmake">chkconfig iptables --<span class="hljs-keyword">list</span> <span class="hljs-comment">#查看防火墙开机启动状态</span>chkconfig iptables <span class="hljs-keyword">off</span> <span class="hljs-comment"># 设置开机时关闭防火墙</span></code></pre></li></ul><h1 id="关机重启命令"><a href="#关机重启命令" class="headerlink" title="关机重启命令"></a>关机重启命令</h1><blockquote><p>在linux领域内大多用在服务器上，很少遇到关机的操作。毕竟服务器上跑一个服务是永无止境的，除非特殊情况下，不得已才会关机。<br>正确的关机流程为：<code>sync &gt; shutdown &gt; reboot &gt; halt</code></p></blockquote><ul><li>基本语法<pre><code class="hljs mipsasm"><span class="hljs-keyword">sync </span> （功能描述：将数据由内存同步到硬盘中）halt （功能描述：关闭系统，等同于<span class="hljs-keyword">shutdown </span>-h now 和 poweroff）reboot （功能描述：就是重启，等同于 <span class="hljs-keyword">shutdown </span>-r now）<span class="hljs-keyword">shutdown </span>           [选项] 时间</code></pre></li></ul><table><thead><tr><th>选项</th><th>功能</th></tr></thead><tbody><tr><td>-h</td><td>-h=halt关机</td></tr><tr><td>-r</td><td>-r=reboot重启</td></tr></tbody></table><table><thead><tr><th>参数</th><th>功能</th></tr></thead><tbody><tr><td>now</td><td>立刻关机</td></tr><tr><td>时间</td><td>等待多久后关机（时间单位是分钟）。</td></tr></tbody></table><ul><li>经验技巧<blockquote><p>Linux系统中为了提高磁盘的读写效率，对磁盘采取了 “预读迟写”操作方式。当用户保存文件时，Linux核心并不一定立即将保存数据写入物理磁盘中，而是将数据保存在缓冲区中，等缓冲区满时再写入磁盘，这种方式可以极大的提高磁盘写入数据的效率。但是，也带来了安全隐患，如果数据还未写入磁盘时，系统掉电或者其他严重问题出现，则将导致数据丢失。使用sync指令可以立即将缓冲区的数据写入磁盘</p></blockquote></li><li>案例实操<pre><code class="hljs autoit">syncreboothalt<span class="hljs-built_in">shutdown</span> -h <span class="hljs-number">1</span> <span class="hljs-string">'This server will shutdown after 1 mins'</span><span class="hljs-built_in">shutdown</span> -h now <span class="hljs-meta"># 立马关机（等同于 halt）</span><span class="hljs-built_in">shutdown</span> -r now <span class="hljs-meta"># 系统立马重启（等同于 reboot）</span></code></pre></li></ul>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>命令</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux(二)：VI和VIM编辑器</title>
    <link href="/2015/02/23/linux2/"/>
    <url>/2015/02/23/linux2/</url>
    
    <content type="html"><![CDATA[<h1 id="是什么"><a href="#是什么" class="headerlink" title="是什么"></a>是什么</h1><blockquote><p>VI是Unix操作系统和类Unix操作系统中最通用的文本编辑器。<br>VIM编辑器是从VI发展出来的一个性能更强大的文本编辑器。可以主动的以字体颜色辨别语法的正确性，方便程序设计。VIM与VI编辑器完全兼容</p></blockquote><h1 id="一般模式"><a href="#一般模式" class="headerlink" title="一般模式"></a>一般模式</h1><blockquote><p>以vi打开一个档案就直接进入一般模式了（这是默认的模式）。在这个模式中， 你可以使用『上下左右』按键来移动光标，你可以使用『删除字符』或『删除整行』来处理档案内容， 也可以使用『复制、贴上』来处理你的文件数据。</p></blockquote><table><thead><tr><th>语法</th><th>功能描述</th></tr></thead><tbody><tr><td>yy</td><td>复制光标当前一行</td></tr><tr><td>y数字y</td><td>复制一段（从第几行到第几行）</td></tr><tr><td>p</td><td>箭头移动到目的行粘贴</td></tr><tr><td>u</td><td>撤销上一步</td></tr><tr><td>dd</td><td>删除光标当前行</td></tr><tr><td>d数字d</td><td>删除光标（含）后多少行</td></tr><tr><td>x</td><td>删除一个字母，相当于del</td></tr><tr><td>X</td><td>删除一个字母，相当于Backspace</td></tr><tr><td>yw</td><td>复制一个词</td></tr><tr><td>dw</td><td>删除一个词</td></tr><tr><td>shift+^</td><td>移动到行头</td></tr><tr><td>shift+$</td><td>移动到行尾</td></tr><tr><td>1+shift+g</td><td>移动到页头，数字</td></tr><tr><td>shift+g</td><td>移动到页尾</td></tr><tr><td>数字N+shift+g</td><td>移动到目标行</td></tr></tbody></table><h1 id="编辑模式"><a href="#编辑模式" class="headerlink" title="编辑模式"></a>编辑模式</h1><blockquote><p>在一般模式中可以进行删除、复制、粘贴等的动作，但是却无法编辑文件内容的！要等到你按下『i, I, o, O, a, A, r, R』等任何一个字母之后才会进入编辑模式。<br>注意了！通常在Linux中，按下这些按键时，在画面的左下方会出现『INSERT或 REPLACE』的字样，此时才可以进行编辑。而如果要回到一般模式时， 则必须要按下『Esc』这个按键即可退出编辑模式。</p></blockquote><h2 id="进入编辑模式"><a href="#进入编辑模式" class="headerlink" title="进入编辑模式"></a>进入编辑模式</h2><table><thead><tr><th>按键</th><th>功能</th></tr></thead><tbody><tr><td>i</td><td>当前光标前</td></tr><tr><td>a</td><td>当前光标后</td></tr><tr><td>o</td><td>当前光标行的下一行</td></tr><tr><td>I</td><td>光标所在行最前</td></tr><tr><td>A</td><td>光标所在行最后</td></tr><tr><td>O</td><td>当前光标行的上一行</td></tr></tbody></table><h2 id="退出编辑模式"><a href="#退出编辑模式" class="headerlink" title="退出编辑模式"></a>退出编辑模式</h2><blockquote><p>按『Esc』键</p></blockquote><h1 id="指令模式"><a href="#指令模式" class="headerlink" title="指令模式"></a>指令模式</h1><blockquote><p>在一般模式当中，输入『 : / ?』3个中的任何一个按钮，就可以将光标移动到最底下那一行。<br>在这个模式当中， 可以提供你『搜寻资料』的动作，而读取、存盘、大量取代字符、离开 vi 、显示行号等动作是在此模式中达成的！</p></blockquote><h2 id="基本语法"><a href="#基本语法" class="headerlink" title="基本语法"></a>基本语法</h2><table><thead><tr><th>命令</th><th>功能</th></tr></thead><tbody><tr><td>:w</td><td>保存</td></tr><tr><td>:q</td><td>退出</td></tr><tr><td>:!</td><td>强制执行</td></tr><tr><td>/要查找的词</td><td>n 查找下一个，N 往上查找</td></tr><tr><td>? 要查找的词</td><td>n是查找上一个，shift+n是往下查找</td></tr><tr><td>:set nu</td><td>显示行号</td></tr><tr><td>:set nonu</td><td>关闭行号</td></tr></tbody></table><h2 id="案例实操"><a href="#案例实操" class="headerlink" title="案例实操"></a>案例实操</h2><blockquote><p>强制保存退出:wq!    </p></blockquote>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>编辑器</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MySQL基础知识</title>
    <link href="/2015/02/02/mysql1/"/>
    <url>/2015/02/02/mysql1/</url>
    
    <content type="html"><![CDATA[<h1 id="数据库概述"><a href="#数据库概述" class="headerlink" title="数据库概述"></a>数据库概述</h1><blockquote><p>数据库指的就是数据‘仓库’，专门存储数据的。<br>数组，集合，文件….<br>数据库存放的数据便于管理</p></blockquote><h2 id="数据库存储数据的特点"><a href="#数据库存储数据的特点" class="headerlink" title="数据库存储数据的特点"></a>数据库存储数据的特点</h2><ul><li>将数据放到表中，表再放到库中</li><li>一个数据库中可以有多个表，每个表都有一个的名字，用来标识自己。表名具有唯一性。</li><li>表具有一些特性，这些特性定义了数据在表中如何存储，类似java中 “类”的设计。</li><li>表由列组成，我们也称为字段。所有表都是由一个或多个列组成的，每一列类似java 中的”属性”</li><li>表中的数据是按行存储的，每一行类似于java中的“对象”。</li></ul><h2 id="常见的数据库"><a href="#常见的数据库" class="headerlink" title="常见的数据库"></a>常见的数据库</h2><p>Mysql、Oracle、DB2、SQL Server</p><h2 id="MySQL的安装"><a href="#MySQL的安装" class="headerlink" title="MySQL的安装"></a>MySQL的安装</h2><blockquote><p>Mac 下安装<code>brew install mysql</code></p></blockquote><pre><code class="hljs axapta"><span class="hljs-meta">#启动服务</span>mysql.<span class="hljs-keyword">server</span> start<span class="hljs-meta">#停止服务</span>mysql.<span class="hljs-keyword">server</span> stop<span class="hljs-meta">#重启服务</span>mysql.<span class="hljs-keyword">server</span> restart</code></pre><h2 id="sql语法规范"><a href="#sql语法规范" class="headerlink" title="sql语法规范"></a>sql语法规范</h2><ul><li>A.    不区分大小写</li><li>B.    语句结束使用;</li><li>C.    注释可以使用单行或者多行注释<br>单行注释：<br>注释的内容<br>多行注释：/<em>注释的内容</em>/</li><li>D.    语句可以分行编写，但是关键字不能拆开编写</li></ul><h2 id="SQL语句的分类"><a href="#SQL语句的分类" class="headerlink" title="SQL语句的分类"></a>SQL语句的分类</h2><table><thead><tr><th>定义</th><th>分类</th></tr></thead><tbody><tr><td>DML Data Manipulation Language</td><td>数据操作语言</td></tr><tr><td>DDL Data Definition Language</td><td>数据定义语言</td></tr><tr><td>DCL Data Control Language</td><td>数据控制语言</td></tr><tr><td>DQL Data Query Language</td><td>数据查询语言</td></tr></tbody></table><h1 id="DQL语言"><a href="#DQL语言" class="headerlink" title="DQL语言"></a>DQL语言</h1><h2 id="基础查询"><a href="#基础查询" class="headerlink" title="基础查询"></a>基础查询</h2><p>语法：</br><br>select 查询列表 from 表名;</br></p><p><strong>特点：</strong></p><ul><li>1.查询的结果为一个虚拟表，并没有真实存在</li><li>2.查询列表可以是常量、字段、表达式、函数以及上述的组合</li></ul><h2 id="条件查询"><a href="#条件查询" class="headerlink" title="条件查询"></a>条件查询</h2><p>语法：</br><br>select 查询列表 from 表名</br><br>where 条件;</br><br><strong>条件的形式：</strong></br></p><ul><li>关系表达式 </br><br>通过关系运算符组成的表达式：关系表达式<br>sql中的关系运算符：&gt; &lt; &gt;= &lt;= = != &lt;&gt;</li><li>逻辑表达式</br><br>通过逻辑运算符组成的表达式：逻辑表达式<br>sql中的逻辑运算符：and、or 、not   【&amp;&amp; || ！】</li><li>模糊查询</br><br>like、between and、in、is null</li></ul><h2 id="排序查询"><a href="#排序查询" class="headerlink" title="排序查询"></a>排序查询</h2><p>语法：</br><br>select 查询列表</br><br>from 表名</br><br>where 条件</br><br>order by 排序列表 asc|desc;</br></p><p>特点：</br></p><ul><li>asc代表升序，desc代表降序。默认是升序</li><li>查询列表可以是 单个字段、多个字段、函数、表达式、别名或上述的组合</li></ul><h2 id="常见函数-单行函数"><a href="#常见函数-单行函数" class="headerlink" title="常见函数-单行函数"></a>常见函数-单行函数</h2><p>函数：类似于“方法”，都是为了完成特定的功能，将内部实现细节隐藏，对外暴露函数名<br>关心的是：函数名和函数功能</br><br>函数调用的语法：<br>select 函数名(参数列表);</p><h3 id="字符函数"><a href="#字符函数" class="headerlink" title="字符函数"></a>字符函数</h3><blockquote><p>length 获取字节个数</br><br>replace 替换所有</br><br>lower 变小写</br><br>upper 变大写</br><br>substr 截取子串</br><br>lpad 左填充</br><br>rpad 右填充</br><br>trim 去前后空格或指定字符</br><br>instr 获取子串第一次出现的索引</br><br>concat 拼接字符串</br></p></blockquote><h3 id="数学函数"><a href="#数学函数" class="headerlink" title="数学函数"></a>数学函数</h3><blockquote><p>ceil 向上取整</br><br>floor 向下取整</br><br>rand 随机数</br><br>abs 绝对值</br><br>mod 取余</br><br>round 四舍五入</br><br>truncate 截断</br></p></blockquote><h3 id="日期函数"><a href="#日期函数" class="headerlink" title="日期函数"></a>日期函数</h3><blockquote><p>now 获取当前日期+时间</br><br>year/month/day 获取年、月、日</br><br>date_format 格式日期，返回格式好的字符串</br><br>DATE_FORMAT(NOW(),’%Y年%m月%d日 %H小时%i分钟%s秒’)</br><br>str_to_date 将字符串解析成日期</br><br>curtime 获取当前时间，不包含日期</br><br>curdate 获取当前日期，不包含时间</br></p></blockquote><p>注意：mysql默认使用的时区不是东八区<br>查询mysql的时区：<code>show variables like &#39;%time_zone%&#39;;</code><br>可以修改mysql.ini配置mysql默认时区：<br>设置mysql时区<br>default-time-zone=’+08:00’<br>修改完配置文件需要重启mysql服务才会生效</p><h3 id="流程控制函数"><a href="#流程控制函数" class="headerlink" title="流程控制函数"></a>流程控制函数</h3><blockquote><p>if函数</br><br>case结构</br></p></blockquote><ul><li>示例1：查询员工的姓名和旧工资、显示的新工资<br>如果部门编号&gt;100,则工资显示为3倍<br>如果部门编号&gt;80,则工资显示为2倍<br>如果部门编号&gt;50,则工资显示为原工资<br>否则，显示为1/2倍<pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span> last_name,salary 旧工资,<span class="hljs-keyword">CASE</span><span class="hljs-keyword">WHEN</span> department_id&gt;<span class="hljs-number">100</span> <span class="hljs-keyword">THEN</span> salary*<span class="hljs-number">3</span><span class="hljs-keyword">WHEN</span> department_id&gt;<span class="hljs-number">80</span> <span class="hljs-keyword">THEN</span> salary*<span class="hljs-number">2</span><span class="hljs-keyword">WHEN</span> department_id&gt;<span class="hljs-number">50</span> <span class="hljs-keyword">THEN</span> salary<span class="hljs-keyword">ELSE</span> salary/<span class="hljs-number">2</span><span class="hljs-keyword">END</span> 新工资<span class="hljs-keyword">FROM</span> employees;</code></pre></li><li>示例2：案例：显示姓名、工种编号、旧工资、新工资，如果工种为IT_PROG，则工资显示成5倍<pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span> job_id,last_name,salary,<span class="hljs-keyword">CASE</span> job_id<span class="hljs-keyword">WHEN</span> <span class="hljs-string">'IT_PROG'</span> <span class="hljs-keyword">THEN</span> salary*<span class="hljs-number">5</span><span class="hljs-keyword">WHEN</span> <span class="hljs-string">'ST_CLERK'</span> <span class="hljs-keyword">THEN</span> salary*<span class="hljs-number">3</span><span class="hljs-keyword">WHEN</span> <span class="hljs-string">'AD_VP'</span> <span class="hljs-keyword">THEN</span> salary<span class="hljs-keyword">ELSE</span>  salary/<span class="hljs-number">2</span><span class="hljs-keyword">END</span> 新工资<span class="hljs-keyword">FROM</span> employees;</code></pre></li></ul><h2 id="分组函数"><a href="#分组函数" class="headerlink" title="分组函数"></a>分组函数</h2><p>调用语法：select 函数名(实参列表);</br></p><p><strong>分组函数和单行函数的区别：</strong></p><ul><li>单行函数：将一个数据进行处理，返回一个值</li><li>分组函数：将虚拟表看做一个组，处理一组数据，返回一个值</li></ul><p><strong>常见的分组函数：</strong><br>字段|含义<br>-|-<br>sum(字段)|求该字段的所有值的和</br><br>avg(字段) |求该字段的平均值</br><br>max(字段) |求最大值</br><br>min(字段) |求最小值</br><br>count(字段) |计算该字段中非空的值的个数</br></p><p><strong>特点</strong></p><ul><li>分组函数可以搭配筛选条件使用</li><li>分组函数的参数可以为字段或表达式</li><li>分组函数都忽略null值</li><li>分组函数支持的类型<br>  max、min、count支持任意类型<br>  sum、avg仅仅支持数值型</li><li>count的使用<br>count(*)或count(常量值) ：统计结果集中的行数<br>count(distinct 字段)：实现去重后的统计</li><li>和分组函数一同查询的字段不能是任意字段，可以是group by后面的字段！</li></ul><h2 id="分组查询"><a href="#分组查询" class="headerlink" title="分组查询"></a>分组查询</h2><p><strong>语法：</strong></br></p><p>select 分组函数,分组的字段 – ⑤</br><br>from 表名 –   ①</br><br>where 分组前的筛选条件– ②</br><br>group by 分组的字段,…  – ③</br><br>having 分组后的筛选条件 – ④</br><br>order by 排序列表 –⑥</br></p><p><strong>特点：</strong></p><p>筛选可以分为分组前筛选和分组后筛选<br>筛选|针对的数据    |    使用的关键字    |    位置<br>-|-|-|-<br>分组前筛选|    原始表|where|group by前面<br>    分组后筛选|分组后的结果集        |having            |group by后面</p><blockquote><p>分组函数做条件放在having后面！</p></blockquote><h2 id="连接查询"><a href="#连接查询" class="headerlink" title="连接查询"></a>连接查询</h2><p>查询的字段或条件来自于多张表，这个时候则需要使用多表连接查询，简称连接查询</br><br>笛卡尔乘积的现象：</br><br>    表1行数m行，表2行数n行，结果为m*n行<br>如何产生的？<br>    没有添加连接条件<br>如何解决？<br>    添加连接条件<br>如何添加连接条件？</br></p><p><strong>sql92语法：</strong></p><pre><code>内连接：    等值连接    非等值连接    自连接外连接：【支持的不太好】mysql中不支持！    左外连接    右外连接    全外连接</code></pre><p><strong>sql99语法：</strong></br></p><pre><code>内连接：    等值连接    非等值连接    自连接外连接：    左外连接    右外连接    全外连接【mysql不支持】</code></pre><h2 id="子查询"><a href="#子查询" class="headerlink" title="子查询"></a>子查询</h2><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><p>一个查询语句里面又嵌套了另一个完整的查询语句，被嵌套在里面的查询语句，称为 子查询或内查询<br>外面的查询语句，称为主查询或外查询</p><h3 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h3><p>按位置不同：</p><blockquote><p>select后面</br><br>from后面</br><br>where或having后面    ★</br><br>    按查询结果的行数不同：</br><br>        单行子查询</br><br>        多行子查询</br><br>exists后面</br></p></blockquote><h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><p>放在后面的子查询的特点：</p><ul><li>子查询放在小括号内</li><li>子查询优先于主查询执行，主查询一般会用到子查询的结果</li><li>一般放在条件的右侧</li><li>子查询一般会搭配着操作符使用<br>  单行子查询，搭配单行操作符使用：&gt; &lt; &gt;= &lt;= = &lt;&gt;<br>  多行子查询，搭配多行操作符使用：in、not in、any、all</li></ul><h2 id="分页查询"><a href="#分页查询" class="headerlink" title="分页查询"></a>分页查询</h2><h3 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h3><p>select 查询列表<br>from 表<br>inner join 表 on 连接条件</p><p>where 筛选条件<br>group by 分组<br>having 分组后筛选</p><p>order by 排序】</p><p>limit 起始条目索引,条目数;</p><h3 id="特点："><a href="#特点：" class="headerlink" title="特点："></a>特点：</h3><ul><li>limit子句放在最后</li><li>起始条目索引可以省略，默认从第一条开始 ，索引为0</li></ul><h1 id="DDL语言"><a href="#DDL语言" class="headerlink" title="DDL语言"></a>DDL语言</h1><h2 id="库和表的管理"><a href="#库和表的管理" class="headerlink" title="库和表的管理"></a>库和表的管理</h2><ul><li><p>创建数据库语法：<br>create database【 if not exists】 库名;</br></p></li><li><p>删除数据库语法：<br>drop database 【if exists】 库名;</p></li><li><p>创建表语法：<br>create table 【if not exists】 表名(<br>  字段名    字段类型    【字段约束】,<br>  字段名    字段类型    【字段约束】,<br>  ….<br>);</p></li><li><p>删除表语法：<br>drop table 【if exists】表名;</p></li><li><p>修改表语法：alter table 表名 add/drop/modify/change column 列名 字段类型；</p></li><li><p>修改表名：ALTER TABLE 旧表名 RENAME TO 新表名;</p><h2 id="常用mysql数据类型："><a href="#常用mysql数据类型：" class="headerlink" title="常用mysql数据类型："></a>常用mysql数据类型：</h2><blockquote><p>选择数据类型原则：类型越简单越好，能保存数值的类型的所占空间越小越好</p></blockquote></li><li><p>整型        TINYINT/SMALLINT/MEDIUMINT/INT/BIGINT</p></li><li><p>浮点型        FLOAT(n,m)/DOUBLE(n,m)/DECIMAL(n,m)</p><pre><code>n:代表整数部位+小数部位的最大长度m:代表小数部位的最大长度FLOAT(5,3) :-99.999~99.999</code></pre></li><li><p>字符型        CHAR(n)/VARCHAR(n)/TEXT<br>TEXT:用于保存较长的文本，比如备注文字、协议文字</p></li></ul><table><thead><tr><th>字段</th><th>n的意思</th><th>特点</th><th>性能</th></tr></thead><tbody><tr><td>CHAR(n)</td><td>能保存的字符的最大个数,可选，默认是1</td><td>固定长度的字符</td><td>较高</td></tr><tr><td>VARCHAR(n)</td><td>能保存的字符的最大个数，必写</td><td>可变长度的字符    较低</td><td></td></tr><tr><td>- 日期型        DATETIME/DATE/TIME/TIMESTAMP时间戳</td><td></td><td></td><td></td></tr><tr><td>TATE:只有日期，TIME：只有时间</td><td></td><td></td><td></td></tr></tbody></table><table><thead><tr><th>示例</th><th>所占字节</th><th>保存日期范围</th><th>受时区和服务器版本的影响</th></tr></thead><tbody><tr><td>DATETIME：日期+时间</td><td>8个字节</td><td>1900-1-1~9999-12-31</td><td>不受</td></tr><tr><td>TIMESTAMP:日期+时间</td><td>4个字节</td><td>1970-1-1~2038-12-31</td><td>受</td></tr><tr><td>## 常见约束</td><td></td><td></td><td></td></tr><tr><td>- 说明：用于额外限定表中的字段值，为了保证数据的完整性（准确性和可靠性）</td><td></td><td></td><td></td></tr><tr><td>- 常见的六大约束：</td><td></td><td></td><td></td></tr><tr><td>- NOT NULL非空：该字段的值为必填项</td><td></td><td></td><td></td></tr><tr><td>- UNIQUE 唯一：该字段的值不能重复，可以为空，一个表中可以有多个</td><td></td><td></td><td></td></tr><tr><td>- PRIMARY KEY 主键：该字段的值不能重复，不能为空，一个表中只能有一个，一般来讲，一个表最好有一个主键</td><td></td><td></td><td></td></tr><tr><td>- DEFAULT 默认：该字段的值如果不手动插入，也有默认值</td><td></td><td></td><td></td></tr><tr><td>- CHECK 检查【mysql不支持】：用于检测该字段的值是否满足某个条件</td><td></td><td></td><td></td></tr><tr><td>- FOREIGN KEY 外键：用于限定两个表的关系，从表的某列值要来自于主表的某列值</td><td></td><td></td><td></td></tr></tbody></table><h2 id="分类-1"><a href="#分类-1" class="headerlink" title="分类"></a>分类</h2><blockquote><p>表级约束：只有外键、主键、唯一</br><br>列级约束：只有非空、默认、主键、唯一</p></blockquote><h1 id="DDL语言-1"><a href="#DDL语言-1" class="headerlink" title="DDL语言"></a>DDL语言</h1><h2 id="insert-插入"><a href="#insert-插入" class="headerlink" title="insert 插入"></a>insert 插入</h2><ul><li>插入单行：<br>  insert into 表名(列名1，列名2,…) values(值1,值2,…);</li><li>批量插入多行：<br>  insert into 表名(列名1，列名2,…) values(值1,值2,…),(值1,值2,…),(值1,值2,…);<h2 id="特点：-1"><a href="#特点：-1" class="headerlink" title="特点："></a>特点：</h2></li><li>①整型的值，不用引号<br>字符或日期型的值，使用引号</li><li>②插入的值必须满足对应的字段类型和约束</li><li>③字段可以调换顺序，不一定是表中字段的存储顺序，但要注意字段和值必须一一对应！</li><li>④字段和值的个数、顺序、类型必须一致<br>如果可以为null或default，则需要使用null或default填充</li><li>⑤如果要插入的为该表的所有列，则直接省略列名，默认所有列！</li></ul><h2 id="update-更新"><a href="#update-更新" class="headerlink" title="update 更新"></a>update 更新</h2><p>语法：<br>update  表名 set 字段名 = 新值,字段名 = 新值 【where 条件】;</p><h2 id="delete-删除"><a href="#delete-删除" class="headerlink" title="delete 删除"></a>delete 删除</h2><p>语法：delete from 表名 where 条件</p><h2 id="truncate-删除"><a href="#truncate-删除" class="headerlink" title="truncate 删除"></a>truncate 删除</h2><p>语法：<br>truncate table 表名;</p><h2 id="delete与truncate的区别（面试题）"><a href="#delete与truncate的区别（面试题）" class="headerlink" title="delete与truncate的区别（面试题）"></a>delete与truncate的区别（面试题）</h2><ul><li>delete可以加WHERE条件<br>TRUNCATE不可以加WHERE条件</li><li>truncate效率更高</li><li>delete 可以返回受影响的行数<br>TRUNCATE没有返回受影响 的行数</li><li>如果使用DELETE删除带自增长列的表，删除后再插入，则自增长列从断点处开始插入<br>如果使用TRUNCATE删除带自增长列的表，删除后再插入，则自增长列从1开始</li><li>delete语句可以回滚,<br>TRUNCATE 语句不能回滚</li></ul>]]></content>
    
    
    <categories>
      
      <category>MySQL</category>
      
    </categories>
    
    
    <tags>
      
      <tag>基础</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linux(一)：概述</title>
    <link href="/2015/01/03/linux1/"/>
    <url>/2015/01/03/linux1/</url>
    
    <content type="html"><![CDATA[<h1 id="Linux起源"><a href="#Linux起源" class="headerlink" title="Linux起源"></a>Linux起源</h1><p>Linux 内核最初只是由芬兰人林纳斯·托瓦兹（Linus Torvalds）在赫尔辛基大学上学时出于个人爱好而编写的。</br><br>Linux 是一套免费使用和自由传播的类 Unix 操作系统，是一个基于 POSIX 和 UNIX 的多用户、多任务、支持多线程和多 CPU 的操作系统。</br><br>Linux 能运行主要的 UNIX 工具软件、应用程序和网络协议。它支持 32 位和 64 位硬件。Linux 继承了 Unix 以网络为核心的设计思想，是一个性能稳定的多用户网络操作系统。</br></p><h1 id="Linux-的发行版"><a href="#Linux-的发行版" class="headerlink" title="Linux 的发行版"></a>Linux 的发行版</h1><p>目前市面上较知名的发行版有：Ubuntu、RedHat、CentOS、Debian、Fedora、SuSE、OpenSUSE、Arch Linux、SolusOS 等。</p><h1 id="Linux-vs-Windows"><a href="#Linux-vs-Windows" class="headerlink" title="Linux vs Windows"></a>Linux vs Windows</h1><table><thead><tr><th>比较</th><th>Windows</th><th>Linux</th></tr></thead><tbody><tr><td>界面</td><td>界面统一，外壳程序固定所有 Windows 程序菜单几乎一致，快捷键也几乎相同</td><td>图形界面风格依发布版不同而不同，可能互不兼容。GNU/Linux 的终端机是从 UNIX 传承下来，基本命令和操作方法也几乎一致。</td></tr><tr><td>驱动程序</td><td>驱动程序丰富，版本更新频繁。默认安装程序里面一般包含有该版本发布时流行的硬件驱动程序，之后所出的新硬件驱动依赖于硬件厂商提供。对于一些老硬件，如果没有了原配的驱动有时很难支持。另外，有时硬件厂商未提供所需版本的 Windows 下的驱动，也会比较头痛。</td><td>由志愿者开发，由 Linux 核心开发小组发布，很多硬件厂商基于版权考虑并未提供驱动程序，尽管多数无需手动安装，但是涉及安装则相对复杂，使得新用户面对驱动程序问题（是否存在和安装方法）会一筹莫展。但是在开源开发模式下，许多老硬件尽管在Windows下很难支持的也容易找到驱动。HP、Intel、AMD 等硬件厂商逐步不同程度支持开源驱动，问题正在得到缓解。</td></tr><tr><td>使用</td><td>使用比较简单，容易入门。图形化界面对没有计算机背景知识的用户使用十分有利。</td><td>图形界面使用简单，容易入门。文字界面，需要学习才能掌握。</td></tr><tr><td>学习</td><td>系统构造复杂、变化频繁，且知识、技能淘汰快，深入学习困难。</td><td>系统构造简单、稳定，且知识、技能传承性好，深入学习相对容易。</td></tr><tr><td>软件</td><td>每一种特定功能可能都需要商业软件的支持，需要购买相应的授权。</td><td>大部分软件都可以自由获取，同样功能的软件选择较少。</td></tr></tbody></table><hr><h1 id="Linux文件与目录结构"><a href="#Linux文件与目录结构" class="headerlink" title="Linux文件与目录结构"></a>Linux文件与目录结构</h1><hr><p>登录系统后，在当前命令窗口下输入命令：</p><pre><code class="hljs bash">[root@hadoop01 /]<span class="hljs-variable">$ls</span> /bin  boot  data   dev  etc  home  lib  lib64  media  metastore_db  mnt  opt  proc  root  run  sbin  srv  swapfile  sys  tmp  usr  var</code></pre><p>以下是对这些目录的解释：</p><blockquote><p>/bin：</br><br>bin是Binary的缩写, 这个目录存放着最经常使用的命令。</p></blockquote><blockquote><p>/boot：</br><br>这里存放的是启动Linux时使用的一些核心文件，包括一些连接文件以及镜像文件。</p></blockquote><blockquote><p>/dev ：</br><br>dev是Device(设备)的缩写, 该目录下存放的是Linux的外部设备，在Linux中访问设备的方式和访问文件的方式是相同的。</p></blockquote><blockquote><p>/etc：<br/><br>这个目录用来存放所有的系统管理所需要的配置文件和子目录。</p></blockquote><blockquote><p>/home：<br/><br>用户的主目录，在Linux中，每个用户都有一个自己的目录，一般该目录名是以用户的账号命名的。</p></blockquote><blockquote><p>/lib：<br/><br>这个目录里存放着系统最基本的动态连接共享库，其作用类似于Windows里的DLL文件。几乎所有的应用程序都需要用到这些共享库。</p></blockquote><blockquote><p>/lost+found：<br/><br>这个目录一般情况下是空的，当系统非法关机后，这里就存放了一些文件。</p></blockquote><blockquote><p>/media：<br/><br>linux 系统会自动识别一些设备，例如U盘、光驱等等，当识别后，linux会把识别的设备挂载到这个目录下。</p></blockquote><blockquote><p>/mnt：<br/><br>系统提供该目录是为了让用户临时挂载别的文件系统的，我们可以将光驱挂载在/mnt/上，然后进入该目录就可以查看光驱里的内容了。</p></blockquote><blockquote><p>/opt：<br/><br> 这是给主机额外安装软件所摆放的目录。比如你安装一个ORACLE数据库则就可以放到这个目录下。默认是空的。</p></blockquote><blockquote><p>/proc：<br/><br>这个目录是一个虚拟的目录，它是系统内存的映射，我们可以通过直接访问这个目录来获取系统信息。<br>这个目录的内容不在硬盘上而是在内存里，我们也可以直接修改里面的某些文件，比如可以通过下面的命令来屏蔽主机的ping命令，使别人无法ping你的机器：</p></blockquote><blockquote><p>/root：<br/><br>该目录为系统管理员，也称作超级权限者的用户主目录。</p></blockquote><blockquote><p>/sbin：<br/><br>s就是Super User的意思，这里存放的是系统管理员使用的系统管理程序。</p></blockquote><blockquote><p>/selinux：<br/><br> 这个目录是Redhat/CentOS所特有的目录，Selinux是一个安全机制，类似于windows的防火墙，但是这套机制比较复杂，这个目录就是存放selinux相关的文件的。</p></blockquote><blockquote><p>/srv：<br/><br> 该目录存放一些服务启动之后需要提取的数据。</p></blockquote><blockquote><p>/sys：<br/><br> 这是linux2.6内核的一个很大的变化。该目录下安装了2.6内核中新出现的一个文件系统 sysfs 。<br>sysfs文件系统集成了下面3种文件系统的信息：针对进程信息的proc文件系统、针对设备的devfs文件系统以及针对伪终端的devpts文件系统。<br>该文件系统是内核设备树的一个直观反映。<br>当一个内核对象被创建的时候，对应的文件和目录也在内核对象子系统中被创建。</p></blockquote><blockquote><p>/tmp：</br><br>这个目录是用来存放一些临时文件的。</p></blockquote><blockquote><p>/usr：<br/><br> 这是一个非常重要的目录，用户的很多应用程序和文件都放在这个目录下，类似于windows下的program files目录。</p></blockquote><blockquote><p>/usr/bin：<br/><br>系统用户使用的应用程序。</p></blockquote><blockquote><p>/usr/sbin：<br/><br>超级用户使用的比较高级的管理程序和系统守护程序。</p></blockquote><blockquote><p>/usr/src：<br/><br>内核源代码默认的放置目录。</p></blockquote><blockquote><p>/var：<br/><br>这个目录中存放着在不断扩充着的东西，我们习惯将那些经常被修改的目录放在这个目录下。包括各种日志文件。</p></blockquote><blockquote><p>/run：<br/><br>是一个临时文件系统，存储系统启动以来的信息。当系统重启时，这个目录下的文件应该被删掉或清除。如果你的系统上有 /var/run 目录，应该让它指向 run。</p></blockquote><p>在 Linux 系统中，有几个目录是比较重要的，平时需要注意不要误删除或者随意更改内部文件。  /etc： 上边也提到了，这个是系统中的配置文件，如果你更改了该目录下的某个文件可能会导致系统不能启动。  /bin, /sbin, /usr/bin, /usr/sbin: 这是系统预设的执行文件的放置目录，比如 ls 就是在/bin/ls 目录下的。  值得提出的是，/bin, /usr/bin 是给系统用户使用的指令（除root外的通用户），而/sbin, /usr/sbin 则是给root使用的指令。  /var： 这是一个非常重要的目录，系统上跑了很多程序，那么每个程序都会有相应的日志产生，而这些日志就被记录到这个目录下，具体在/var/log 目录下，另外mail的预设放置也是在这里。</p>]]></content>
    
    
    <categories>
      
      <category>Linux</category>
      
    </categories>
    
    
    <tags>
      
      <tag>起源</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
