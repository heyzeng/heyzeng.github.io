<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>2020年5月日记</title>
    <link href="/2020/05/01/202005/"/>
    <url>/2020/05/01/202005/</url>
    
    <content type="html"><![CDATA[<h1 id="5月1日"><a href="#5月1日" class="headerlink" title="5月1日"></a>5月1日</h1><p>五一放假第一天，和朋友自驾去松山湖玩了一圈，吃的很开心、玩的开心。</p><h1 id="5月9号"><a href="#5月9号" class="headerlink" title="5月9号"></a>5月9号</h1><p>今天是爸爸生日，和爸爸电话问候了下，从小到大爸爸都是嘴上严厉，心里很柔和的人。我很爱他，都说父爱如山，可以男人与男人的对话还是显得那么生硬。以后要改改自己说话的样子。</p><h1 id="5月10号"><a href="#5月10号" class="headerlink" title="5月10号"></a>5月10号</h1><p>今天是母亲节，给她老人家发了红包，她也不收下。总是很开心的样子。</p><h1 id="5月16日"><a href="#5月16日" class="headerlink" title="5月16日"></a>5月16日</h1><p>朋友入职腾讯、恭喜她！！！</p><h1 id="5月17号"><a href="#5月17号" class="headerlink" title="5月17号"></a>5月17号</h1><p>保持有计划的生活节奏。</p><h1 id="5月18号"><a href="#5月18号" class="headerlink" title="5月18号"></a>5月18号</h1>]]></content>
    
    
    <categories>
      
      <category>日记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>生活点滴</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>使用Sqoop导MySQL数据到Hbase报错</title>
    <link href="/2020/04/15/sqoop1/"/>
    <url>/2020/04/15/sqoop1/</url>
    
    <content type="html"><![CDATA[<h1 id="报错日志"><a href="#报错日志" class="headerlink" title="报错日志"></a>报错日志</h1><pre><code class="hljs xml">20/04/14 16:40:45 WARN mapreduce.HBaseImportJob: Could not find HBase table hbase_company20/04/14 16:40:45 WARN mapreduce.HBaseImportJob: This job may fail. Either explicitly create the table,20/04/14 16:40:45 WARN mapreduce.HBaseImportJob: or re-run with --hbase-create-table.20/04/14 16:40:45 INFO zookeeper.ZooKeeper: Session: 0x36db06bc9c32fb9 closed20/04/14 16:40:45 INFO zookeeper.ClientCnxn: EventThread shut down</code></pre><h1 id="失败原因"><a href="#失败原因" class="headerlink" title="失败原因"></a>失败原因</h1><ul><li><p>使用sqoop导mysql数据到HDFS，不用在hive里面建表，在跑mr任务时候自动创建hive表</p></li><li><p>在用sqoop导mysql数据到HDFS，没有在Hbase建表</p></li><li><p>查询官方文档：sqoop1.4.7只支持HBase1.0.1之前的版本的自动创建HBase表的功能</p></li></ul><h1 id="版本信息"><a href="#版本信息" class="headerlink" title="版本信息"></a>版本信息</h1><ul><li><p>sqoop 版本1.4.7 </p></li><li><p>Hbase 版本2.1.1</p></li></ul><h1 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h1><h2 id="1、在hbase里面建表"><a href="#1、在hbase里面建表" class="headerlink" title="1、在hbase里面建表"></a>1、在hbase里面建表</h2><pre><code class="hljs bash">create <span class="hljs-string">'hbase_company'</span>,<span class="hljs-string">'info'</span>scan <span class="hljs-string">'hbase_company'</span></code></pre><h2 id="2、脚本添加参数"><a href="#2、脚本添加参数" class="headerlink" title="2、脚本添加参数"></a>2、脚本添加参数</h2><pre><code class="hljs bash">--hbase-create-table</code></pre><h1 id="附：导数据脚本"><a href="#附：导数据脚本" class="headerlink" title="附：导数据脚本"></a>附：导数据脚本</h1><pre><code class="hljs bash">bin/sqoop import \--connect jdbc:mysql://ip:3306/company \--username root \--password 000000 \--table company \--columns <span class="hljs-string">"id,name,sex"</span> \--column-family <span class="hljs-string">"info"</span> \--hbase-create-table \--hbase-row-key <span class="hljs-string">"id"</span> \--hbase-table <span class="hljs-string">"hbase_company"</span> \--num-mappers 1 \--split-by id</code></pre>]]></content>
    
    
    <categories>
      
      <category>sqoop</category>
      
    </categories>
    
    
    <tags>
      
      <tag>解决问题</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>博客导航</title>
    <link href="/2020/03/28/%E5%8D%9A%E5%AE%A2%E5%AF%BC%E8%88%AA/"/>
    <url>/2020/03/28/%E5%8D%9A%E5%AE%A2%E5%AF%BC%E8%88%AA/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>ClickHouse之深圳MeetUp</title>
    <link href="/2019/10/25/clickhouse1-1/"/>
    <url>/2019/10/25/clickhouse1-1/</url>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>俄罗斯小哥很帅</p><h1 id="纪要"><a href="#纪要" class="headerlink" title="纪要"></a>纪要</h1><h1 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h1><ul><li>Yandex团队clickhouse的新特性以及在机器学习方面的研究</li><li>腾讯</li><li>朱凯老师MergerTree</li></ul><h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1>]]></content>
    
    
    <categories>
      
      <category>ClickHouse</category>
      
    </categories>
    
    
    <tags>
      
      <tag>meetup</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数仓工具hive(六)：Hive中常用函数汇总</title>
    <link href="/2017/08/18/hive6/"/>
    <url>/2017/08/18/hive6/</url>
    
    <content type="html"><![CDATA[<h1 id="常用日期函数"><a href="#常用日期函数" class="headerlink" title="常用日期函数"></a>常用日期函数</h1><pre><code class="hljs vbscript">unix_timestamp:返回当前或指定时间的时间戳    from_uni xtime：将时间戳转为日期格式current_date：当前日期current_timestamp：当前的日期加时间to_date：抽取日期部分<span class="hljs-built_in">year</span>：获取年<span class="hljs-built_in">month</span>：获取月<span class="hljs-built_in">day</span>：获取日<span class="hljs-built_in">hour</span>：获取时<span class="hljs-built_in">minute</span>：获取分<span class="hljs-built_in">second</span>：获取秒weekofyear：当前时间是一年中的第几周dayofmonth：当前时间是一个月中的第几天months_between： 两个日期间的月份add_months：日期加减月<span class="hljs-built_in">datediff</span>：两个日期相差的天数  前减后date_add：日期加天数date_sub：日期减天数last_day：日期的当月的最后一天</code></pre><h1 id="常用取整函数"><a href="#常用取整函数" class="headerlink" title="常用取整函数"></a>常用取整函数</h1><pre><code class="hljs gauss"><span class="hljs-built_in">round</span>： 四舍五入<span class="hljs-built_in">ceil</span>：  向上取整<span class="hljs-built_in">select</span> <span class="hljs-built_in">ceil</span>(<span class="hljs-number">10.0102</span>)   <span class="hljs-comment">//11</span><span class="hljs-built_in">floor</span>： 向下取整<span class="hljs-built_in">select</span> <span class="hljs-built_in">floor</span>(<span class="hljs-number">10.99</span>)   <span class="hljs-comment">//10</span></code></pre><h1 id="常用字符串操作函数"><a href="#常用字符串操作函数" class="headerlink" title="常用字符串操作函数"></a>常用字符串操作函数</h1><pre><code class="hljs pgsql">upper： 转大写lower： 转小写length： 长度trim：  前后去空格lpad： 向左补齐，到指定长度rpad：  向右补齐，到指定长度regexp_replace： <span class="hljs-keyword">SELECT</span> regexp_replace(<span class="hljs-string">'100-200'</span>, <span class="hljs-string">'(\\d+)'</span>, <span class="hljs-string">'num'</span>)=<span class="hljs-string">'num-num</span><span class="hljs-string">使用正则表达式匹配目标字符串，匹配成功后替换！</span></code></pre><h1 id="常用集合操作"><a href="#常用集合操作" class="headerlink" title="常用集合操作"></a>常用集合操作</h1><pre><code class="hljs xquery">size： 集合中元素的个数map_keys： 返回<span class="hljs-keyword">map</span>中<span class="hljs-built_in">的key</span>map_values: 返回<span class="hljs-keyword">map</span>中的<span class="hljs-keyword">value</span>array_contains: 判断<span class="hljs-keyword">array</span>中是否包含某个元素sort_array： 将<span class="hljs-keyword">array</span>中的元素排序</code></pre>]]></content>
    
    
    <categories>
      
      <category>Hive</category>
      
    </categories>
    
    
    <tags>
      
      <tag>总结</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hive50道sql必练题</title>
    <link href="/2017/07/22/hive1-2/"/>
    <url>/2017/07/22/hive1-2/</url>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>50道Hive之sql必练题，做完50道题会对HiveSql有一个比较深的认识，且附上答案以及验证结果。</p><h1 id="建表"><a href="#建表" class="headerlink" title="建表"></a>建表</h1><pre><code class="hljs sql"><span class="hljs-comment">-- 学生表</span><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> student(s_id <span class="hljs-keyword">string</span>,s_name <span class="hljs-keyword">string</span>,s_birth <span class="hljs-keyword">string</span>,s_sex <span class="hljs-keyword">string</span>) <span class="hljs-keyword">row</span> <span class="hljs-keyword">format</span> <span class="hljs-keyword">delimited</span> <span class="hljs-keyword">fields</span> <span class="hljs-keyword">terminated</span> <span class="hljs-keyword">by</span> <span class="hljs-string">'\,'</span>;<span class="hljs-comment">-- 课堂表</span><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> course(c_id <span class="hljs-keyword">string</span>,c_name <span class="hljs-keyword">string</span>,t_id <span class="hljs-keyword">string</span>) <span class="hljs-keyword">row</span> <span class="hljs-keyword">format</span> <span class="hljs-keyword">delimited</span> <span class="hljs-keyword">fields</span> <span class="hljs-keyword">terminated</span> <span class="hljs-keyword">by</span> <span class="hljs-string">'\,'</span>;<span class="hljs-comment">-- 教师表</span><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> teacher(t_id <span class="hljs-keyword">string</span>,t_name <span class="hljs-keyword">string</span>) <span class="hljs-keyword">row</span> <span class="hljs-keyword">format</span> <span class="hljs-keyword">delimited</span> <span class="hljs-keyword">fields</span> <span class="hljs-keyword">terminated</span> <span class="hljs-keyword">by</span> <span class="hljs-string">'\,'</span>;<span class="hljs-comment">-- 分数表</span><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> score(s_id <span class="hljs-keyword">string</span>,c_id <span class="hljs-keyword">string</span>,s_score <span class="hljs-built_in">int</span>) <span class="hljs-keyword">row</span> <span class="hljs-keyword">format</span> <span class="hljs-keyword">delimited</span> <span class="hljs-keyword">fields</span> <span class="hljs-keyword">terminated</span> <span class="hljs-keyword">by</span> <span class="hljs-string">'\,'</span>;</code></pre><h1 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h1><pre><code class="hljs bash">-- vim /opt/module/data/student.csv01,赵雷,1990-01-01,男02,钱电,1990-12-21,男03,孙风,1990-05-20,男04,李云,1990-08-06,男05,周梅,1991-12-01,女06,吴兰,1992-03-01,女07,郑竹,1989-07-01,女08,王菊,1990-01-20,女-- vim /opt/module/datas/course.csv01,语文,0202,数学,0103,英语,03-- vim /opt/module/datas/teacher.csv01,张三02,李四03,王五-- vim /opt/module/datas/score.csv01,01,8001,02,9001,03,9902,01,7002,02,6002,03,8003,01,8003,02,8003,03,8004,01,5004,02,3004,03,2005,01,7605,02,8706,01,3106,03,3407,02,8907,03,98</code></pre><h1 id="加载"><a href="#加载" class="headerlink" title="加载"></a>加载</h1><pre><code class="hljs sql"><span class="hljs-keyword">load</span> <span class="hljs-keyword">data</span> <span class="hljs-keyword">local</span> inpath <span class="hljs-string">'/opt/module/datas/student.csv'</span> <span class="hljs-keyword">into</span> <span class="hljs-keyword">table</span> student;<span class="hljs-keyword">load</span> <span class="hljs-keyword">data</span> <span class="hljs-keyword">local</span> inpath <span class="hljs-string">'/opt/module/datas/course.csv'</span> <span class="hljs-keyword">into</span> <span class="hljs-keyword">table</span> course;<span class="hljs-keyword">load</span> <span class="hljs-keyword">data</span> <span class="hljs-keyword">local</span> inpath <span class="hljs-string">'/opt/module/datas/teacher.csv'</span> <span class="hljs-keyword">into</span> <span class="hljs-keyword">table</span> teacher;<span class="hljs-keyword">load</span> <span class="hljs-keyword">data</span> <span class="hljs-keyword">local</span> inpath <span class="hljs-string">'/opt/module/datas/score.csv'</span> <span class="hljs-keyword">into</span> <span class="hljs-keyword">table</span> score;</code></pre>]]></content>
    
    
    <categories>
      
      <category>Hive</category>
      
    </categories>
    
    
    <tags>
      
      <tag>练习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ambari+HDP安装的Hive出现中文乱码解决</title>
    <link href="/2017/07/21/hive1-1/"/>
    <url>/2017/07/21/hive1-1/</url>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>公司决定使用Ambari+HDP这一套大数据运维加部署框架去替代CDH，遇到一些问题会及时记录下来</p><h3 id="Hive注释comment出现乱码"><a href="#Hive注释comment出现乱码" class="headerlink" title="Hive注释comment出现乱码"></a>Hive注释comment出现乱码</h3><h5 id="Hive建表语句"><a href="#Hive建表语句" class="headerlink" title="Hive建表语句"></a>Hive建表语句</h5><pre><code class="hljs sql"><span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span>  test.mytest_tm1(              <span class="hljs-keyword">id</span> <span class="hljs-built_in">int</span> <span class="hljs-keyword">comment</span><span class="hljs-string">'编号'</span>,              <span class="hljs-keyword">name</span> <span class="hljs-keyword">string</span> <span class="hljs-keyword">comment</span> <span class="hljs-string">'名字'</span>              )<span class="hljs-keyword">row</span> <span class="hljs-keyword">format</span> <span class="hljs-keyword">delimited</span> <span class="hljs-keyword">fields</span> <span class="hljs-keyword">terminated</span> <span class="hljs-keyword">by</span> <span class="hljs-string">'\u0001'</span><span class="hljs-keyword">lines</span> <span class="hljs-keyword">terminated</span> <span class="hljs-keyword">by</span> <span class="hljs-string">'\n'</span><span class="hljs-keyword">stored</span> <span class="hljs-keyword">as</span> textfile;</code></pre><h5 id="Hive的元数据存在Mysql中，而Mysql字符集的默认Latin1，则会出现乱码"><a href="#Hive的元数据存在Mysql中，而Mysql字符集的默认Latin1，则会出现乱码" class="headerlink" title="Hive的元数据存在Mysql中，而Mysql字符集的默认Latin1，则会出现乱码"></a>Hive的元数据存在Mysql中，而Mysql字符集的默认Latin1，则会出现乱码</h5><p><img src="https://i.loli.net/2020/05/17/jTSHORqgXmnyAb4.png" srcset="/img/loading.gif" alt=""></p><h3 id="修改Mysql字符集-latin1-改成-utf-8"><a href="#修改Mysql字符集-latin1-改成-utf-8" class="headerlink" title="修改Mysql字符集 latin1 改成 utf-8"></a>修改Mysql字符集 latin1 改成 utf-8</h3><p>在hive库里面修改表、分区、视图</p><h5 id="修改表字段注解和表注解"><a href="#修改表字段注解和表注解" class="headerlink" title="修改表字段注解和表注解"></a>修改表字段注解和表注解</h5><pre><code class="hljs sql"><span class="hljs-keyword">use</span> hive；<span class="hljs-comment"># mysql元数据库</span><span class="hljs-keyword">alter</span> <span class="hljs-keyword">table</span> COLUMNS_V2 <span class="hljs-keyword">modify</span> <span class="hljs-keyword">column</span> <span class="hljs-keyword">COMMENT</span> <span class="hljs-built_in">varchar</span>(<span class="hljs-number">256</span>) <span class="hljs-built_in">character</span> <span class="hljs-keyword">set</span> utf8；<span class="hljs-keyword">alter</span> <span class="hljs-keyword">table</span> TABLE_PARAMS <span class="hljs-keyword">modify</span> <span class="hljs-keyword">column</span> PARAM_VALUE <span class="hljs-built_in">varchar</span>(<span class="hljs-number">4000</span>) <span class="hljs-built_in">character</span> <span class="hljs-keyword">set</span> utf8；</code></pre><h5 id="修改分区字段注解"><a href="#修改分区字段注解" class="headerlink" title="修改分区字段注解"></a>修改分区字段注解</h5><pre><code class="hljs sql"><span class="hljs-keyword">alter</span> <span class="hljs-keyword">table</span> PARTITION_PARAMS <span class="hljs-keyword">modify</span> <span class="hljs-keyword">column</span> PARAM_VALUE <span class="hljs-built_in">varchar</span>(<span class="hljs-number">4000</span>) <span class="hljs-built_in">character</span> <span class="hljs-keyword">set</span> utf8 ;<span class="hljs-keyword">alter</span> <span class="hljs-keyword">table</span> PARTITION_KEYS <span class="hljs-keyword">modify</span> <span class="hljs-keyword">column</span> PKEY_COMMENT <span class="hljs-built_in">varchar</span>(<span class="hljs-number">4000</span>) <span class="hljs-built_in">character</span> <span class="hljs-keyword">set</span> utf8;</code></pre><h5 id="修改索引注解"><a href="#修改索引注解" class="headerlink" title="修改索引注解"></a>修改索引注解</h5><pre><code class="hljs sql"><span class="hljs-keyword">alter</span> <span class="hljs-keyword">table</span> INDEX_PARAMS <span class="hljs-keyword">modify</span> <span class="hljs-keyword">column</span> PARAM_VALUE <span class="hljs-built_in">varchar</span>(<span class="hljs-number">4000</span>) <span class="hljs-built_in">character</span> <span class="hljs-keyword">set</span> utf8;</code></pre><h3 id="在ambari的UI页面修改-metastore-的连接-URL"><a href="#在ambari的UI页面修改-metastore-的连接-URL" class="headerlink" title="在ambari的UI页面修改 metastore 的连接 URL"></a>在ambari的UI页面修改 metastore 的连接 URL</h3><p>  注意修改完成后要重启Hive</p><pre><code class="hljs crystal"><span class="hljs-symbol">jdbc:</span><span class="hljs-symbol">mysql:</span>/<span class="hljs-regexp">/ip:3306/database</span>?createDatabaseIfNotExist=<span class="hljs-literal">true</span>&amp;amp;useUnicode=<span class="hljs-literal">true</span>&amp;characterEncoding=UTF-<span class="hljs-number">8</span></code></pre><p><img src="https://i.loli.net/2020/05/17/Zrbyo62eTQPnhRd.png" srcset="/img/loading.gif" alt=""></p><h3 id="验证结果"><a href="#验证结果" class="headerlink" title="验证结果"></a>验证结果</h3><p>  注意：必须是新建hive表，就得表字符集已经不可改变。<br><img src="https://i.loli.net/2020/05/17/NIz4uMsr3KvJOCy.png" srcset="/img/loading.gif" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>Hive</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ambari</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数仓工具hive(四)：Hive文件存储格式以及优缺点</title>
    <link href="/2017/06/04/hive4/"/>
    <url>/2017/06/04/hive4/</url>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>Hive支持的存储数的格式主要有：TEXTFILE 、SEQUENCEFILE、ORC、PARQUET。</p><h1 id="行与列存储的特点"><a href="#行与列存储的特点" class="headerlink" title="行与列存储的特点"></a>行与列存储的特点</h1><h2 id="行存储的特点"><a href="#行存储的特点" class="headerlink" title="行存储的特点"></a>行存储的特点</h2><p>查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快。</p><h2 id="列存储的特点"><a href="#列存储的特点" class="headerlink" title="列存储的特点"></a>列存储的特点</h2><p>因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法。</p><h1 id="Hive文件存储格式以及优缺点"><a href="#Hive文件存储格式以及优缺点" class="headerlink" title="Hive文件存储格式以及优缺点"></a>Hive文件存储格式以及优缺点</h1><h2 id="textfile"><a href="#textfile" class="headerlink" title="textfile"></a>textfile</h2><ul><li>默认的文件格式，行存储。建表时不指定存储格式即为textfile，导入数据时把数据文件拷贝至hdfs不进行处理</li><li>优点：最简单的数据格式，便于和其他工具（Pig, grep, sed, awk）共享数据，便于查看和编辑；加载较快</li><li>缺点：耗费存储空间，I/O性能较低；Hive不进行数据切分合并，不能进行并行操作，查询效率低</li><li>场景：适用于小型查询，查看具体数据内容的测试操作</li></ul><h2 id="sequencefile"><a href="#sequencefile" class="headerlink" title="sequencefile"></a>sequencefile</h2><ul><li>含有键值对的二进制文件，行存储</li><li>优点：可压缩、可分割，优化磁盘利用率和I/O；可并行操作数据，查询效率高</li><li>缺点：存储空间消耗最大；对于Hadoop生态系统之外的工具不适用，需要通过text文件转化加载</li><li>场景：适用于数据量较小、大部分列的查询</li></ul><h2 id="rcfile"><a href="#rcfile" class="headerlink" title="rcfile"></a>rcfile</h2><ul><li>存储模式：按列存储，采用行组模式对数据进行存储(数据按行分块,每块按照列存储)</li><li>存储结构：包括(16字节的HDFS同步块信息以及元数据的头部信息主要包括该行组内的存储的行数、列的字段信息)</li><li>存储空间：采用游程编码</li><li>优点：可压缩，高效的列存取；查询效率较高</li><li>缺点：加载时性能消耗较大，需要通过text文件转化加载；读取全量数据性能低</li><li>场景：多数用于存储需要“长期留存”的数据文件</li></ul><h2 id="orcfile"><a href="#orcfile" class="headerlink" title="orcfile"></a>orcfile</h2><ul><li>存储模式：按列存储，所有列存在一个文件中，</li><li>每个ORC文件首先会被横向切分成多个Stripe，而每个Stripe内部以列存储，所有的列存储在一个文件中，而且每个stripe默认的大小是250MB，相对于RCFile默认的行组大小是4MB，所以比RCFile更高效。Postscripts中存储该表的行数，压缩参数，压缩大小，列等信息；Stripe Footer中包含该stripe的统计结果，包括Max，Min，count等信息；FileFooter中包含该表的统计结果，以及各个Stripe的位置信息；IndexData中保存了该stripe上数据的位置信息，总行数等信息；RowData以stream的形式保存了数据的具体信息</li><li>除了游程编码，引入了字典编码和Bit编码</li><li>场景：适用于Hive中大型的存储、查询</li></ul><h2 id="parquet"><a href="#parquet" class="headerlink" title="parquet"></a>parquet</h2><ul><li>存储模式：按列存储，Parquet文件是以二进制方式存储的，不可以直接读取和修改的，文件是自解析的，文件中包括该文件的数据和元数据</li><li>存储结构：行组(Row Group)：按照行将数据物理上划分为多个单元，每一个行组包含一定的行数，在一个HDFS文件中至少存储一个行组，Parquet读写的时候会将整个行组缓存在内存中，所以如果每一个行组的大小是由内存大的小决定的</li></ul>]]></content>
    
    
    <categories>
      
      <category>Hive</category>
      
    </categories>
    
    
    <tags>
      
      <tag>hive存储策略</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数仓工具hive(二)：Hive安装部署</title>
    <link href="/2017/05/29/hive2/"/>
    <url>/2017/05/29/hive2/</url>
    
    <content type="html"><![CDATA[<h1 id="Hive安装地址"><a href="#Hive安装地址" class="headerlink" title="Hive安装地址"></a>Hive安装地址</h1><ul><li><a href="http://hive.apache.org/" target="_blank" rel="noopener">Hive官网地址</a></li><li><a href="https://cwiki.apache.org/confluence/display/Hive/GettingStarted" target="_blank" rel="noopener">文档查看地址</a></li><li><a href="http://archive.apache.org/dist/hive/" target="_blank" rel="noopener">下载地址</a></li></ul><h1 id="Hive安装部署"><a href="#Hive安装部署" class="headerlink" title="Hive安装部署"></a>Hive安装部署</h1><h2 id="Hive安装及配置"><a href="#Hive安装及配置" class="headerlink" title="Hive安装及配置"></a>Hive安装及配置</h2><ul><li>把apache-hive-1.2.1-bin.tar.gz上传到linux的/opt/software目录下</li><li>解压apache-hive-1.2.1-bin.tar.gz到/opt/module/目录下面<pre><code class="hljs ruby">[root@hadoop102 software]$ tar -zxvf apache-hive-<span class="hljs-number">1.2</span>.<span class="hljs-number">1</span>-bin.tar.gz -C /opt/<span class="hljs-class"><span class="hljs-keyword">module</span>/</span></code></pre></li><li>修改apache-hive-1.2.1-bin.tar.gz的名称为hive<pre><code class="hljs angelscript">[<span class="hljs-symbol">root@</span>hadoop102 module]$ mv apache-hive<span class="hljs-number">-1.2</span><span class="hljs-number">.1</span>-bin/ hive</code></pre></li><li>修改/opt/module/hive/conf目录下的hive-env.sh.template名称为hive-env.sh<pre><code class="hljs mel">[root@hadoop102 conf]$ mv hive-<span class="hljs-keyword">env</span>.sh.template hive-<span class="hljs-keyword">env</span>.sh</code></pre></li><li>配置hive-env.sh文件<pre><code class="hljs bash"><span class="hljs-comment"># 配置HADOOP_HOME路径E</span><span class="hljs-built_in">export</span> HADOOP_HOME=/opt/module/hadoop-2.7.2<span class="hljs-comment"># 配置HIVE_CONF_DIR路径</span><span class="hljs-built_in">export</span> HIVE_CONF_DIR=/opt/module/hive/conf</code></pre><h2 id="Hadoop集群配置"><a href="#Hadoop集群配置" class="headerlink" title="Hadoop集群配置"></a>Hadoop集群配置</h2></li><li>必须启动hdfs和yarn<pre><code class="hljs bash">[root@hadoop102 hadoop-2.7.2]$ sbin/start-dfs.sh[root@hadoop103 hadoop-2.7.2]$ sbin/start-yarn.sh</code></pre></li><li>在HDFS上创建/tmp和/user/hive/warehouse两个目录并修改他们的同组权限可写<pre><code class="hljs angelscript">[<span class="hljs-symbol">root@</span>hadoop102 hadoop<span class="hljs-number">-2.7</span><span class="hljs-number">.2</span>]$ bin/hadoop fs -mkdir /tmp[<span class="hljs-symbol">root@</span>hadoop102 hadoop<span class="hljs-number">-2.7</span><span class="hljs-number">.2</span>]$ bin/hadoop fs -mkdir -p /user/hive/warehouse[<span class="hljs-symbol">root@</span>hadoop102 hadoop<span class="hljs-number">-2.7</span><span class="hljs-number">.2</span>]$ bin/hadoop fs -chmod g+w /tmp[<span class="hljs-symbol">root@</span>hadoop102 hadoop<span class="hljs-number">-2.7</span><span class="hljs-number">.2</span>]$ bin/hadoop fs -chmod g+w /user/hive/warehouse</code></pre><h2 id="Hive基本操作"><a href="#Hive基本操作" class="headerlink" title="Hive基本操作"></a>Hive基本操作</h2></li><li>启动hive<pre><code class="hljs elixir">[root<span class="hljs-variable">@hadoop102</span> hive]<span class="hljs-variable">$ </span>bin/hive</code></pre></li><li>查看数据库<pre><code class="hljs abnf">hive&gt; show databases<span class="hljs-comment">;</span></code></pre></li><li>打开默认数据库<pre><code class="hljs actionscript">hive&gt; <span class="hljs-keyword">use</span> <span class="hljs-keyword">default</span>;</code></pre></li><li>显示default数据库中的表<pre><code class="hljs pgsql">hive&gt; <span class="hljs-keyword">show</span> <span class="hljs-keyword">tables</span>;</code></pre></li><li>创建一张表<pre><code class="hljs pgsql">hive&gt; <span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> student(id <span class="hljs-type">int</span>, <span class="hljs-type">name</span> string);</code></pre></li><li>查看表的结构<pre><code class="hljs pgsql">hive&gt; <span class="hljs-keyword">desc</span> student;hive&gt; <span class="hljs-keyword">show</span> <span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> student;</code></pre></li><li>向表中插入数据<pre><code class="hljs n1ql">hive&gt; <span class="hljs-keyword">insert</span> <span class="hljs-keyword">into</span> student <span class="hljs-keyword">values</span>(<span class="hljs-number">1000</span>,<span class="hljs-string">"ss"</span>);</code></pre></li><li>查询表中数据<pre><code class="hljs n1ql">hive&gt; <span class="hljs-keyword">select</span> * <span class="hljs-keyword">from</span> student;</code></pre></li><li>退出hive<pre><code class="hljs abnf">hive&gt; quit<span class="hljs-comment">;</span></code></pre><h1 id="将本地文件导入Hive案例"><a href="#将本地文件导入Hive案例" class="headerlink" title="将本地文件导入Hive案例"></a>将本地文件导入Hive案例</h1></li><li>需求 </li></ul><p>将本地/opt/module/datas/student.txt这个目录下的数据导入到hive的student(id int, name string)表中。</p><ul><li><p>数据准备</p><ul><li>在/opt/module/目录下创建datas<pre><code class="hljs elixir">[root<span class="hljs-variable">@hadoop102</span> <span class="hljs-keyword">module</span>]<span class="hljs-variable">$ </span>mkdir datas</code></pre></li><li>在/opt/module/datas/目录下创建student.txt文件并添加数据<pre><code class="hljs angelscript">[<span class="hljs-symbol">root@</span>hadoop102 datas]$ touch student.txt[<span class="hljs-symbol">root@</span>hadoop102 datas]$ vi student.txt<span class="hljs-number">1001</span>zhangshan<span class="hljs-number">1002</span>lishi<span class="hljs-number">1003</span>zhaoliu</code></pre></li></ul></li><li><p>Hive实际操作</p><ul><li>创建student表, 并声明文件分隔符’\t’<pre><code class="hljs pgsql">hive&gt; <span class="hljs-keyword">create</span> <span class="hljs-keyword">table</span> student(id <span class="hljs-type">int</span>, <span class="hljs-type">name</span> string) <span class="hljs-keyword">ROW</span> <span class="hljs-keyword">FORMAT</span> DELIMITED FIELDS TERMINATED<span class="hljs-keyword">BY</span> <span class="hljs-string">'\t'</span>;</code></pre></li><li>加载/opt/module/datas/student.txt 文件到student数据库表中<pre><code class="hljs sql"><span class="hljs-keyword">load</span> <span class="hljs-keyword">data</span> <span class="hljs-keyword">local</span> inpath <span class="hljs-string">'/opt/module/datas/student.txt'</span> <span class="hljs-keyword">into</span> <span class="hljs-keyword">table</span> student;</code></pre></li><li>Hive查询结果<pre><code class="hljs yaml"><span class="hljs-string">hive&gt;</span> <span class="hljs-string">select</span> <span class="hljs-string">*</span> <span class="hljs-string">from</span> <span class="hljs-string">student;</span><span class="hljs-string">OK</span><span class="hljs-number">1001</span><span class="hljs-string">zhangshan</span><span class="hljs-number">1002</span><span class="hljs-string">lishi</span><span class="hljs-number">1003</span><span class="hljs-string">zhaoliu</span><span class="hljs-attr">Time taken:</span> <span class="hljs-number">0.266</span> <span class="hljs-string">seconds,</span> <span class="hljs-attr">Fetched:</span> <span class="hljs-number">3</span> <span class="hljs-string">row(s)</span></code></pre></li></ul></li><li><p>遇到的问题</p><ul><li>再打开一个客户端窗口启动hive，会产生java.sql.SQLException异常</li><li>原因是，Metastore默认存储在自带的derby数据库中，推荐使用MySQL存储Metastore;<pre><code class="hljs css"><span class="hljs-selector-tag">Exception</span> <span class="hljs-selector-tag">in</span> <span class="hljs-selector-tag">thread</span> "<span class="hljs-selector-tag">main</span>" <span class="hljs-selector-tag">java</span><span class="hljs-selector-class">.lang</span><span class="hljs-selector-class">.RuntimeException</span>: <span class="hljs-selector-tag">java</span><span class="hljs-selector-class">.lang</span><span class="hljs-selector-class">.RuntimeException</span>: <span class="hljs-selector-tag">Unable</span> <span class="hljs-selector-tag">to</span> <span class="hljs-selector-tag">instantiate</span> <span class="hljs-selector-tag">org</span><span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.hadoop</span><span class="hljs-selector-class">.hive</span><span class="hljs-selector-class">.ql</span><span class="hljs-selector-class">.metadata</span><span class="hljs-selector-class">.SessionHiveMetaStoreClient</span>        <span class="hljs-selector-tag">at</span> <span class="hljs-selector-tag">org</span><span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.hadoop</span><span class="hljs-selector-class">.hive</span><span class="hljs-selector-class">.ql</span><span class="hljs-selector-class">.session</span><span class="hljs-selector-class">.SessionState</span><span class="hljs-selector-class">.start</span>(<span class="hljs-selector-tag">SessionState</span><span class="hljs-selector-class">.java</span><span class="hljs-selector-pseudo">:522)</span>        <span class="hljs-selector-tag">at</span> <span class="hljs-selector-tag">org</span><span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.hadoop</span><span class="hljs-selector-class">.hive</span><span class="hljs-selector-class">.cli</span><span class="hljs-selector-class">.CliDriver</span><span class="hljs-selector-class">.run</span>(<span class="hljs-selector-tag">CliDriver</span><span class="hljs-selector-class">.java</span><span class="hljs-selector-pseudo">:677)</span>        <span class="hljs-selector-tag">at</span> <span class="hljs-selector-tag">org</span><span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.hadoop</span><span class="hljs-selector-class">.hive</span><span class="hljs-selector-class">.cli</span><span class="hljs-selector-class">.CliDriver</span><span class="hljs-selector-class">.main</span>(<span class="hljs-selector-tag">CliDriver</span><span class="hljs-selector-class">.java</span><span class="hljs-selector-pseudo">:621)</span>        <span class="hljs-selector-tag">at</span> <span class="hljs-selector-tag">sun</span><span class="hljs-selector-class">.reflect</span><span class="hljs-selector-class">.NativeMethodAccessorImpl</span><span class="hljs-selector-class">.invoke0</span>(<span class="hljs-selector-tag">Native</span> <span class="hljs-selector-tag">Method</span>)        <span class="hljs-selector-tag">at</span> <span class="hljs-selector-tag">sun</span><span class="hljs-selector-class">.reflect</span><span class="hljs-selector-class">.NativeMethodAccessorImpl</span><span class="hljs-selector-class">.invoke</span>(<span class="hljs-selector-tag">NativeMethodAccessorImpl</span><span class="hljs-selector-class">.java</span><span class="hljs-selector-pseudo">:57)</span>        <span class="hljs-selector-tag">at</span> <span class="hljs-selector-tag">sun</span><span class="hljs-selector-class">.reflect</span><span class="hljs-selector-class">.DelegatingMethodAccessorImpl</span><span class="hljs-selector-class">.invoke</span>(<span class="hljs-selector-tag">DelegatingMethodAccessorImpl</span><span class="hljs-selector-class">.java</span><span class="hljs-selector-pseudo">:43)</span>        <span class="hljs-selector-tag">at</span> <span class="hljs-selector-tag">java</span><span class="hljs-selector-class">.lang</span><span class="hljs-selector-class">.reflect</span><span class="hljs-selector-class">.Method</span><span class="hljs-selector-class">.invoke</span>(<span class="hljs-selector-tag">Method</span><span class="hljs-selector-class">.java</span><span class="hljs-selector-pseudo">:606)</span>        <span class="hljs-selector-tag">at</span> <span class="hljs-selector-tag">org</span><span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.hadoop</span><span class="hljs-selector-class">.util</span><span class="hljs-selector-class">.RunJar</span><span class="hljs-selector-class">.run</span>(<span class="hljs-selector-tag">RunJar</span><span class="hljs-selector-class">.java</span><span class="hljs-selector-pseudo">:221)</span>        <span class="hljs-selector-tag">at</span> <span class="hljs-selector-tag">org</span><span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.hadoop</span><span class="hljs-selector-class">.util</span><span class="hljs-selector-class">.RunJar</span><span class="hljs-selector-class">.main</span>(<span class="hljs-selector-tag">RunJar</span><span class="hljs-selector-class">.java</span><span class="hljs-selector-pseudo">:136)</span><span class="hljs-selector-tag">Caused</span> <span class="hljs-selector-tag">by</span>: <span class="hljs-selector-tag">java</span><span class="hljs-selector-class">.lang</span><span class="hljs-selector-class">.RuntimeException</span>: <span class="hljs-selector-tag">Unable</span> <span class="hljs-selector-tag">to</span> <span class="hljs-selector-tag">instantiate</span> <span class="hljs-selector-tag">org</span><span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.hadoop</span><span class="hljs-selector-class">.hive</span><span class="hljs-selector-class">.ql</span><span class="hljs-selector-class">.metadata</span><span class="hljs-selector-class">.SessionHiveMetaStoreClient</span>        <span class="hljs-selector-tag">at</span> <span class="hljs-selector-tag">org</span><span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.hadoop</span><span class="hljs-selector-class">.hive</span><span class="hljs-selector-class">.metastore</span><span class="hljs-selector-class">.MetaStoreUtils</span><span class="hljs-selector-class">.newInstance</span>(<span class="hljs-selector-tag">MetaStoreUtils</span><span class="hljs-selector-class">.java</span><span class="hljs-selector-pseudo">:1523)</span>        <span class="hljs-selector-tag">at</span> <span class="hljs-selector-tag">org</span><span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.hadoop</span><span class="hljs-selector-class">.hive</span><span class="hljs-selector-class">.metastore</span><span class="hljs-selector-class">.RetryingMetaStoreClient</span>.&lt;<span class="hljs-selector-tag">init</span>&gt;(<span class="hljs-selector-tag">RetryingMetaStoreClient</span><span class="hljs-selector-class">.java</span><span class="hljs-selector-pseudo">:86)</span>        <span class="hljs-selector-tag">at</span> <span class="hljs-selector-tag">org</span><span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.hadoop</span><span class="hljs-selector-class">.hive</span><span class="hljs-selector-class">.metastore</span><span class="hljs-selector-class">.RetryingMetaStoreClient</span><span class="hljs-selector-class">.getProxy</span>(<span class="hljs-selector-tag">RetryingMetaStoreClient</span><span class="hljs-selector-class">.java</span><span class="hljs-selector-pseudo">:132)</span>        <span class="hljs-selector-tag">at</span> <span class="hljs-selector-tag">org</span><span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.hadoop</span><span class="hljs-selector-class">.hive</span><span class="hljs-selector-class">.metastore</span><span class="hljs-selector-class">.RetryingMetaStoreClient</span><span class="hljs-selector-class">.getProxy</span>(<span class="hljs-selector-tag">RetryingMetaStoreClient</span><span class="hljs-selector-class">.java</span><span class="hljs-selector-pseudo">:104)</span>        <span class="hljs-selector-tag">at</span> <span class="hljs-selector-tag">org</span><span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.hadoop</span><span class="hljs-selector-class">.hive</span><span class="hljs-selector-class">.ql</span><span class="hljs-selector-class">.metadata</span><span class="hljs-selector-class">.Hive</span><span class="hljs-selector-class">.createMetaStoreClient</span>(<span class="hljs-selector-tag">Hive</span><span class="hljs-selector-class">.java</span><span class="hljs-selector-pseudo">:3005)</span>        <span class="hljs-selector-tag">at</span> <span class="hljs-selector-tag">org</span><span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.hadoop</span><span class="hljs-selector-class">.hive</span><span class="hljs-selector-class">.ql</span><span class="hljs-selector-class">.metadata</span><span class="hljs-selector-class">.Hive</span><span class="hljs-selector-class">.getMSC</span>(<span class="hljs-selector-tag">Hive</span><span class="hljs-selector-class">.java</span><span class="hljs-selector-pseudo">:3024)</span>        <span class="hljs-selector-tag">at</span> <span class="hljs-selector-tag">org</span><span class="hljs-selector-class">.apache</span><span class="hljs-selector-class">.hadoop</span><span class="hljs-selector-class">.hive</span><span class="hljs-selector-class">.ql</span><span class="hljs-selector-class">.session</span><span class="hljs-selector-class">.SessionState</span><span class="hljs-selector-class">.start</span>(<span class="hljs-selector-tag">SessionState</span><span class="hljs-selector-class">.java</span><span class="hljs-selector-pseudo">:503)</span>... 8 <span class="hljs-selector-tag">more</span></code></pre><h1 id="Hive元数据配置到MySQL"><a href="#Hive元数据配置到MySQL" class="headerlink" title="Hive元数据配置到MySQL"></a>Hive元数据配置到MySQL</h1><h2 id="驱动拷贝"><a href="#驱动拷贝" class="headerlink" title="驱动拷贝"></a>驱动拷贝</h2></li></ul></li><li><p>拷贝/opt/software/mysql-libs/mysql-connector-java-5.1.27目录下的mysql-connector-java-5.1.27-bin.jar到/opt/module/hive/lib/</p><pre><code class="hljs angelscript">[<span class="hljs-symbol">root@</span>hadoop102 mysql-connector-java<span class="hljs-number">-5.1</span><span class="hljs-number">.27</span>]# cp mysql-connector-java<span class="hljs-number">-5.1</span><span class="hljs-number">.27</span>-bin.jar /opt/module/hive/lib/</code></pre><h2 id="配置Metastore到MySQL"><a href="#配置Metastore到MySQL" class="headerlink" title="配置Metastore到MySQL"></a>配置Metastore到MySQL</h2></li><li><p>在/opt/module/hive/conf目录下创建一个hive-site.xml</p><pre><code class="hljs elixir">[root<span class="hljs-variable">@hadoop102</span> conf]<span class="hljs-variable">$ </span>touch hive-site.xml[root<span class="hljs-variable">@hadoop102</span> conf]<span class="hljs-variable">$ </span>vi hive-site.xml</code></pre></li><li><p>根据官方文档配置参数，拷贝数据到hive-site.xml文件中</p><pre><code class="hljs xml"><span class="hljs-meta">&lt;?xml version="1.0"?&gt;</span><span class="hljs-meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>jdbc:mysql://hadoop102:3306/metastore?createDatabaseIfNotExist=true<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">description</span>&gt;</span>JDBC connect string for a JDBC metastore<span class="hljs-tag">&lt;/<span class="hljs-name">description</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">description</span>&gt;</span>Driver class name for a JDBC metastore<span class="hljs-tag">&lt;/<span class="hljs-name">description</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>root<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">description</span>&gt;</span>username to use against metastore database<span class="hljs-tag">&lt;/<span class="hljs-name">description</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>000000<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span>  <span class="hljs-tag">&lt;<span class="hljs-name">description</span>&gt;</span>password to use against metastore database<span class="hljs-tag">&lt;/<span class="hljs-name">description</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span></code></pre></li><li><p>配置完毕后，如果启动hive异常，可以重新启动虚拟机。（重启后，别忘了启动hadoop集群</p></li></ul><h2 id="多窗口启动Hive测试"><a href="#多窗口启动Hive测试" class="headerlink" title="多窗口启动Hive测试"></a>多窗口启动Hive测试</h2><ul><li>先启动MySQL<pre><code class="hljs elixir">[root<span class="hljs-variable">@hadoop102</span> mysql-libs]<span class="hljs-variable">$ </span>mysql -uroot -p000000</code></pre></li><li>查看有几个数据库<pre><code class="hljs asciidoc">mysql&gt; show databases;<span class="hljs-code">+--------------------+</span>| Database           |<span class="hljs-code">+--------------------+</span>| information<span class="hljs-emphasis">_schema |</span><span class="hljs-emphasis">| mysql             |</span><span class="hljs-emphasis">| performance_</span>schema || test               |<span class="hljs-code">+--------------------+</span></code></pre></li><li>再次打开多个窗口，分别启动hive<pre><code class="hljs elixir">[root<span class="hljs-variable">@hadoop102</span> hive]<span class="hljs-variable">$ </span>bin/hive</code></pre></li><li>启动hive后，回到MySQL窗口查看数据库，显示增加了metastore数据库<pre><code class="hljs asciidoc">mysql&gt; show databases;<span class="hljs-code">+--------------------+</span>| Database           |<span class="hljs-code">+--------------------+</span>| information<span class="hljs-emphasis">_schema |</span><span class="hljs-emphasis">| metastore          |</span><span class="hljs-emphasis">| mysql             |</span><span class="hljs-emphasis">| performance_</span>schema || test               |<span class="hljs-code">+--------------------+</span></code></pre><h2 id="HiveJDBC访问"><a href="#HiveJDBC访问" class="headerlink" title="HiveJDBC访问"></a>HiveJDBC访问</h2></li><li>启动hiveserver2服务<pre><code class="hljs elixir">[root<span class="hljs-variable">@hadoop102</span> hive]<span class="hljs-variable">$ </span>bin/hiveserver2</code></pre></li><li>启动beeline<pre><code class="hljs angelscript">[<span class="hljs-symbol">root@</span>hadoop102 hive]$ bin/beelineBeeline version <span class="hljs-number">1.2</span><span class="hljs-number">.1</span> by Apache Hivebeeline&gt;</code></pre></li><li>连接hiveserver2<pre><code class="hljs asciidoc">beeline&gt; !connect jdbc:hive2://hadoop102:10000（回车）Connecting to jdbc:hive2://hadoop102:10000Enter username for jdbc:hive2://hadoop102:10000: root（回车）Enter password for jdbc:hive2://hadoop102:10000: （直接回车）Connected to: Apache Hive (version 1.2.1)Driver: Hive JDBC (version 1.2.1)Transaction isolation: TRANSACTION<span class="hljs-emphasis">_REPEATABLE_</span>READ0: jdbc:hive2://hadoop102:10000&gt; show databases;<span class="hljs-code">+----------------+</span>--+| database<span class="hljs-emphasis">_name  |</span><span class="hljs-emphasis">+----------------+--+</span><span class="hljs-emphasis">| default        |</span><span class="hljs-emphasis">| hive_</span>db2       |<span class="hljs-code">+----------------+</span>--+</code></pre><h2 id="Hive常用交互命令"><a href="#Hive常用交互命令" class="headerlink" title="Hive常用交互命令"></a>Hive常用交互命令</h2><pre><code class="hljs vim">[root@hadoop102 hive]$ bin/hive -<span class="hljs-keyword">help</span>usage: hive -d,--define &lt;key=value&gt;          Variable subsitution <span class="hljs-keyword">to</span> apply <span class="hljs-keyword">to</span> hive                                  commands. <span class="hljs-keyword">e</span>.g. -d A=B <span class="hljs-built_in">or</span> --define A=B    --database <span class="hljs-symbol">&lt;databasename&gt;</span>     Specify the database <span class="hljs-keyword">to</span> use -<span class="hljs-keyword">e</span> <span class="hljs-symbol">&lt;quoted-query-string&gt;</span>         SQL from <span class="hljs-keyword">command</span> <span class="hljs-built_in">line</span> -<span class="hljs-keyword">f</span> <span class="hljs-symbol">&lt;filename&gt;</span>                    SQL from <span class="hljs-keyword">files</span> -H,--<span class="hljs-keyword">help</span>                        <span class="hljs-keyword">Print</span> <span class="hljs-keyword">help</span> information    --hiveconf &lt;property=value&gt;   Use value <span class="hljs-keyword">for</span> given property    --hivevar &lt;key=value&gt;         Variable subsitution <span class="hljs-keyword">to</span> apply <span class="hljs-keyword">to</span> hive                                  commands. <span class="hljs-keyword">e</span>.g. --hivevar A=B -i <span class="hljs-symbol">&lt;filename&gt;</span>                    Initialization SQL <span class="hljs-keyword">file</span> -S,--<span class="hljs-keyword">silent</span>                      Silent <span class="hljs-keyword">mode</span> in interactive <span class="hljs-keyword">shell</span> -v,--<span class="hljs-keyword">verbose</span>                     Verbose <span class="hljs-keyword">mode</span> (<span class="hljs-keyword">echo</span> executed SQL <span class="hljs-keyword">to</span> the console)</code></pre></li><li>“-e”不进入hive的交互窗口执行sql语句<pre><code class="hljs sql">[root@hadoop102 hive]$ bin/hive -e "<span class="hljs-keyword">select</span> <span class="hljs-keyword">id</span> <span class="hljs-keyword">from</span> student;"</code></pre></li><li>“-f”执行脚本中sql语句<ul><li>在/opt/module/datas目录下创建hivef.sql文件<pre><code class="hljs elixir">[root<span class="hljs-variable">@hadoop102</span> datas]<span class="hljs-variable">$ </span>touch hivef.sql</code></pre></li><li>执行文件中的sql语句<pre><code class="hljs ruby">[root@hadoop102 hive]$ bin/hive -f /opt/<span class="hljs-class"><span class="hljs-keyword">module</span>/<span class="hljs-title">datas</span>/<span class="hljs-title">hivef</span>.<span class="hljs-title">sql</span></span></code></pre></li><li>执行文件中的sql语句并将结果写入文件中<pre><code class="hljs elixir">[root<span class="hljs-variable">@hadoop102</span> hive]<span class="hljs-variable">$ </span>bin/hive -f /opt/<span class="hljs-keyword">module</span>/datas/hivef.sql  &gt; <span class="hljs-regexp">/opt/module</span><span class="hljs-regexp">/datas/hive</span>_result.txt</code></pre><h2 id="Hive其他命令操作"><a href="#Hive其他命令操作" class="headerlink" title="Hive其他命令操作"></a>Hive其他命令操作</h2></li></ul></li><li>退出hive窗口：<pre><code class="hljs awk">hive(default)&gt;<span class="hljs-keyword">exit</span>;hive(default)&gt;quit;</code></pre>在新版的hive中没区别了，在以前的版本是有的：<br>exit:先隐性提交数据，再退出；<br>quit:不提交数据，退出；</li><li>在hive cli命令窗口中如何查看hdfs文件系统<pre><code class="hljs stylus"><span class="hljs-function"><span class="hljs-title">hive</span><span class="hljs-params">(default)</span></span>&gt;dfs -ls /</code></pre></li><li>在hive cli命令窗口中如何查看本地文件系统<pre><code class="hljs stylus"><span class="hljs-function"><span class="hljs-title">hive</span><span class="hljs-params">(default)</span></span>&gt;! ls /opt/module/datas</code></pre></li><li>查看在hive中输入的所有历史命令<pre><code class="hljs elixir">[root<span class="hljs-variable">@hadoop102</span> ~]<span class="hljs-variable">$ </span>cat .hivehistory</code></pre><h2 id="参数配置方式"><a href="#参数配置方式" class="headerlink" title="参数配置方式"></a>参数配置方式</h2></li><li>查看当前所有的配置信息<pre><code class="hljs gams">hive&gt;<span class="hljs-keyword">set</span>;</code></pre><h3 id="参数的配置三种方式"><a href="#参数的配置三种方式" class="headerlink" title="参数的配置三种方式"></a>参数的配置三种方式</h3></li><li>配置文件方式<ul><li>默认配置文件：hive-default.xml,</li><li>用户自定义配置文件：hive-site.xml</li><li>注意：用户自定义配置会覆盖默认配置。<br>另外，Hive也会读入Hadoop的配置，因为Hive是作为Hadoop的客户端启动的，Hive的配置会覆盖Hadoop的配置。配置文件的设定对本机启动的所有Hive进程都有效<h3 id="命令行参数方式"><a href="#命令行参数方式" class="headerlink" title="命令行参数方式"></a>命令行参数方式</h3>启动Hive时，可以在命令行添加-hiveconf param=value来设定参数。<br>例如：<pre><code class="hljs angelscript">[<span class="hljs-symbol">root@</span>hadoop103 hive]$ bin/hive -hiveconf mapred.reduce.tasks=<span class="hljs-number">10</span>;</code></pre>注意：仅对本次hive启动有效</li></ul></li><li>查看参数设置：<pre><code class="hljs swift">hive (<span class="hljs-keyword">default</span>)&gt; <span class="hljs-keyword">set</span> mapred.<span class="hljs-built_in">reduce</span>.tasks;</code></pre><h3 id="参数声明方式"><a href="#参数声明方式" class="headerlink" title="参数声明方式"></a>参数声明方式</h3>可以在HQL中使用SET关键字设定参数<br>例如：<pre><code class="hljs routeros">hive (default)&gt; <span class="hljs-builtin-name">set</span> mapred.reduce.<span class="hljs-attribute">tasks</span>=100;</code></pre>注意：仅对本次hive启动有效。</li><li>查看参数设置<pre><code class="hljs swift">hive (<span class="hljs-keyword">default</span>)&gt; <span class="hljs-keyword">set</span> mapred.<span class="hljs-built_in">reduce</span>.tasks;</code></pre>上述三种设定方式的优先级依次递增。即配置文件&lt;命令行参数&lt;参数声明。注意某些系统级的参数，例如log4j相关的设定，必须用前两种方式设定，因为那些参数的读取在会话建立以前已经完成了。</li></ul>]]></content>
    
    
    <categories>
      
      <category>Hive</category>
      
    </categories>
    
    
    <tags>
      
      <tag>部署</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数仓工具Hive(一)：起源</title>
    <link href="/2017/05/28/hive1/"/>
    <url>/2017/05/28/hive1/</url>
    
    <content type="html"><![CDATA[<h1 id="what-is-hive"><a href="#what-is-hive" class="headerlink" title="what is hive"></a>what is hive</h1><ul><li><p>官方文档</p><p>The Apache Hive ™ data warehouse software facilitates reading, writing, and managing large datasets residing in distributed storage using SQL. Structure can be projected onto data already in storage. A command line tool and JDBC driver are provided to connect users to Hive</p></li><li><p>释义</p><p>Apache Hive™数据仓库软件通过使用SQL读、写以及管理在分布式存储中的大型数据集。 可以将结构映射到已经存储的数据上。 提供了命令行工具和JDBC驱动程序将用户连接到Hive</p></li><li><p>起源</p><p>由Facebook开源用于解决海量结构化日志的数据统计，是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类SQL查询功能，而本质上是将HQL转化为MapReduce</p></li><li><p>HQL实现</p><ul><li>Hive处理的数据存储在HDFS</li><li>Hive分析数据底层实现为MapReduce</li><li>执行程序运行在Yarn上</li></ul></li></ul><h1 id="why-is-Hive"><a href="#why-is-Hive" class="headerlink" title="why is Hive"></a>why is Hive</h1><ul><li><p>优点</p><ul><li>操作接口采用类SQL语法，提供快速开发的能力（简单、容易上手）</li><li>避免了去写MapReduce，减少开发人员的学习成本</li><li>Hive的执行延迟比较高，因此Hive常用于数据分析，对实时性要求不高的场合。</li><li>Hive优势在于处理大数据，对于处理小数据没有优势，因为Hive的执行延迟比较高</li><li>Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数</li></ul></li><li><p>缺点</p><ul><li>Hive的HQL表达能力有限，迭代式算法无法表达，数据挖掘方面不擅长</li><li>Hive的效率比较低，Hive自动生成的MapReduce作业，通常情况下不够智能化，Hive调优比较困难，粒度较粗</li></ul></li></ul><h1 id="Hive架构原理"><a href="#Hive架构原理" class="headerlink" title="Hive架构原理"></a>Hive架构原理</h1><p><img src="https://i.loli.net/2020/05/17/KjNfwxvhWnI4C1t.jpg" srcset="/img/loading.gif" alt="Hive架构原理"></p><ul><li><p>用户接口</p><p>CLI（hive shell）、JDBC/ODBC(java访问hive)、WEBUI（浏览器访问hive）</p></li><li><p>元数据:Metastore</p><p>元数据包括：表名、表所属的数据库（默认是default）、表的拥有者、列/分区字段、表的类型（是否是外部表）、表的数据所在目录等</p></li><li><p>Hadoop</p><p>使用HDFS进行存储，使用MapReduce进行计算</p></li><li><p>驱动器Driver</p><ul><li>解析器（SQL Parser）：将SQL字符串转换成抽象语法树AST，这一步一般都用第三方工具库完成，比如antlr；对AST进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误</li><li>编译器（Physical Plan）：将AST编译生成逻辑执行计划</li><li>优化器（Query Optimizer）：对逻辑执行计划进行优化。</li><li>执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于Hive来说，就是MR/Spark</li></ul></li></ul><h1 id="Hive和数据库比较"><a href="#Hive和数据库比较" class="headerlink" title="Hive和数据库比较"></a>Hive和数据库比较</h1><p>  由于 Hive 采用了类似SQL 的查询语言 HQL(Hive Query Language)，因此很容易将 Hive 理解为数据库。其实从结构上来看，Hive 和数据库除了拥有类似的查询语言，再无类似之处。本文将从多个方面来阐述 Hive 和数据库的差异。数据库可以用在 Online 的应用中，但是Hive 是为数据仓库而设计的，清楚这一点，有助于从应用角度理解 Hive 的特性。</p><ul><li><p>查询语言</p><p>  由于SQL被广泛的应用在数据仓库中，因此，专门针对Hive的特性设计了类SQL的查询语言HQL。熟悉SQL开发的开发者可以很方便的使用Hive进行开发</p></li><li><p>Hive 是建立在 Hadoop 之上的，所有 Hive 的数据都是存储在 HDFS 中的。而数据库则可以将数据保存在块设备或者本地文件系统中</p></li><li><p>由于Hive是针对数据仓库应用设计的，而数据仓库的内容是读多写少的。因此，Hive中不建议对数据的改写，所有的数据都是在加载的时候确定好的。而数据库中的数据通常是需要经常进行修改的，因此可以使用 INSERT INTO … VALUES 添加数据，使用 UPDATE … SET修改数据</p></li><li><p>Hive在加载数据的过程中不会对数据进行任何处理，甚至不会对数据进行扫描，因此也没有对数据中的某些Key建立索引。Hive要访问数据中满足条件的特定值时，需要暴力扫描整个数据，因此访问延迟较高。由于 MapReduce 的引入， Hive 可以并行访问数据，因此即使没有索引，对于大数据量的访问，Hive 仍然可以体现出优势。数据库中，通常会针对一个或者几个列建立索引，因此对于少量的特定条件的数据的访问，数据库可以有很高的效率，较低的延迟。由于数据的访问延迟较高，决定了 Hive 不适合在线数据查询</p></li><li><p>Hive中大多数查询的执行是通过 Hadoop 提供的 MapReduce 来实现的。而数据库通常有自己的执行引擎</p></li><li><p>Hive 在查询数据的时候，由于没有索引，需要扫描整个表，因此延迟较高。另外一个导致 Hive 执行延迟高的因素是 MapReduce框架。由于MapReduce 本身具有较高的延迟，因此在利用MapReduce 执行Hive查询时，也会有较高的延迟。相对的，数据库的执行延迟较低。当然，这个低是有条件的，即数据规模较小，当数据规模大到超过数据库的处理能力的时候，Hive的并行计算显然能体现出优势</p></li><li><p>由于Hive是建立在Hadoop之上的，因此Hive的可扩展性是和Hadoop的可扩展性是一致的（世界上最大的Hadoop 集群在 Yahoo!，2009年的规模在4000 台节点左右）。而数据库由于 ACID 语义的严格限制，扩展行非常有限。目前最先进的并行数据库 Oracle 在理论上的扩展能力也只有100台左右</p></li><li><p>由于Hive建立在集群上并可以利用MapReduce进行并行计算，因此可以支持很大规模的数据；对应的，数据库可以支持的数据规模较小</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>Hive</category>
      
    </categories>
    
    
    <tags>
      
      <tag>起源</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大数据生态Hadoop(三)：实现官方自带wordcount案例</title>
    <link href="/2017/05/08/hadoop3/"/>
    <url>/2017/05/08/hadoop3/</url>
    
    <content type="html"><![CDATA[<h1 id="Hadoop官方wordcount示例"><a href="#Hadoop官方wordcount示例" class="headerlink" title="Hadoop官方wordcount示例"></a>Hadoop官方wordcount示例</h1><p>前面的安装准备工作准备好之后，当然要实现下大数据之入门wordcount案例<br>提供版本JDK1.8+Hadoop2.7.2</p><h4 id="在hadoop-2-7-2文件下面创建一个input文件夹"><a href="#在hadoop-2-7-2文件下面创建一个input文件夹" class="headerlink" title="在hadoop-2.7.2文件下面创建一个input文件夹"></a>在hadoop-2.7.2文件下面创建一个input文件夹</h4><pre><code class="hljs bash">[root@hadoop101 hadoop-2.7.2]<span class="hljs-variable">$mkdir</span> input</code></pre><h4 id="在wcinput文件下创建一个wc-input文件"><a href="#在wcinput文件下创建一个wc-input文件" class="headerlink" title="在wcinput文件下创建一个wc.input文件"></a>在wcinput文件下创建一个wc.input文件</h4><pre><code class="hljs bash">[root@hadoop101 hadoop-2.7.2]<span class="hljs-built_in">cd</span> input[root@hadoop101 input]touch wc.input</code></pre><h4 id="编辑wc-input文件"><a href="#编辑wc-input文件" class="headerlink" title="编辑wc.input文件"></a>编辑wc.input文件</h4><pre><code class="hljs bash">[root@hadoop01 input]vim wc.input<span class="hljs-comment"># 文件内容</span>hadoopmapreduceyarnyarn</code></pre><h4 id="wc-input文件加载到hdfs"><a href="#wc-input文件加载到hdfs" class="headerlink" title="wc.input文件加载到hdfs"></a>wc.input文件加载到hdfs</h4><pre><code class="hljs bash">[root@hadoop101 hadoop-2.7.2]hadoop fs -put input/ /tmp/</code></pre><h4 id="运行官方jar包"><a href="#运行官方jar包" class="headerlink" title="运行官方jar包"></a>运行官方jar包</h4><pre><code class="hljs bash">[root@hadoop101 hadoop-2.7.2]hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /tmp/input/  /tmp/output/</code></pre><h4 id="查看wordcount统计词频"><a href="#查看wordcount统计词频" class="headerlink" title="查看wordcount统计词频"></a>查看wordcount统计词频</h4><pre><code class="hljs bash">[root@hadoop101 hadoop-2.7.2]hadoop fs -cat /tmp/output//part-r-00000<span class="hljs-comment"># 统计内容</span>hadoop1mapreduce1yarn2</code></pre>]]></content>
    
    
    <categories>
      
      <category>Hadoop</category>
      
    </categories>
    
    
    <tags>
      
      <tag>实践</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大数据生态Hadoop(二)：hadoop安装部署</title>
    <link href="/2017/05/07/hadoop2/"/>
    <url>/2017/05/07/hadoop2/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>Hadoop</category>
      
    </categories>
    
    
    <tags>
      
      <tag>部署</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大数据生态Hadoop(一)：起源</title>
    <link href="/2017/05/05/hadoop1/"/>
    <url>/2017/05/05/hadoop1/</url>
    
    <content type="html"><![CDATA[<h1 id="What-is-Hadoop"><a href="#What-is-Hadoop" class="headerlink" title="What is Hadoop"></a>What is Hadoop</h1><ul><li>官方文档<br>The Apache™ Hadoop® project develops open-source software for reliable, scalable, distributed computing.</li><li>释义<br>Apache™Hadoop®项目开发用于可靠、可伸缩的分布式计算的开源软件。</li><li>广义<br>广义上来说，Hadoop通常是指一个更广泛的概念——Hadoop生态圈。</li></ul><h1 id="Hadoop起源"><a href="#Hadoop起源" class="headerlink" title="Hadoop起源"></a>Hadoop起源</h1><ul><li>Lucene框架是Doug Cutting开创的开源软件，用Java书写代码，实现与Google类似的全文搜索功能，它提供了全文检索引擎的架构，包括完整的查询引擎和索引引擎。</li><li>2001年年底Lucene成为Apache基金会的一个子项目。</li><li>对于海量数据的场景，Lucene面对与Google同样的困难，存储数据困难，检索速度慢。</li><li>可以说Google是Hadoop的思想之源(Google在大数据方面的三篇论文)<pre><code class="hljs applescript"><span class="hljs-comment"># Google三篇论文</span>GFS <span class="hljs-comment">---&gt;HDFS</span>Map-Reduce <span class="hljs-comment">---&gt;MR</span>BigTable <span class="hljs-comment">---&gt;HBase</span></code></pre></li><li>2003-2004年，Google公开了部分GFS和MapReduce思想的细节，以此为基础Doug Cutting等人用了2年业余时间实现了DFS和MapReduce机制，使Nutch性能飙升。</li><li>2005 年Hadoop 作为 Lucene的子项目 Nutch的一部分正式引入Apache基金会。</li><li>2006 年 3 月份，Map-Reduce和Nutch Distributed File System (NDFS) 分别被纳入到 Hadoop 项目中，Hadoop就此正式诞生，标志着大数据时代来临，而名字来源于Doug Cutting儿子的玩具大象。</li></ul><h1 id="Hadoop三大发行版本"><a href="#Hadoop三大发行版本" class="headerlink" title="Hadoop三大发行版本"></a>Hadoop三大发行版本</h1><ul><li><a href="http://hadoop.apache.org/releases.html" target="_blank" rel="noopener">Apache Hadoop</a><br>版本最原始（最基础）的版本，对于入门学习最好。</li><li><a href="https://www.cloudera.com/downloads/cdh/5-10-0.html" target="_blank" rel="noopener">Cloudera Hadoop</a><br>在大型互联网企业中用的较多。</li><li><a href="https://hortonworks.com/products/data-center/hdp/" target="_blank" rel="noopener">Hortonworks Hadoop</a><br>文档完善、已经被CDH收购</li></ul><h1 id="Hadoop优势"><a href="#Hadoop优势" class="headerlink" title="Hadoop优势"></a>Hadoop优势</h1><ul><li>高可靠性<br>Hadoop底层维护多个数据副本，所以即使Hadoop某个计算元素或存储出现故障，也不会导致数据的丢失。</li><li>高扩展性<br>在集群间分配任务数据，可方便的扩展数以千计的节点。</li><li>高效性<br>在MapReduce的思想下，Hadoop是并行工作的，以加快任务处理速度。</li><li>高容错性<br>能够自动将失败的任务重新分配。</li></ul><h1 id="Hadoop组成"><a href="#Hadoop组成" class="headerlink" title="Hadoop组成"></a>Hadoop组成</h1><ul><li>HDFS<br>Hadoop分布式文件系统(HDFS)是指被设计成适合运行在通用硬件(commodity hardware)上的分布式文件系统（Distributed File System）</li><li>MapRedurce<br>MapReduce是一种编程模型，用于大规模数据集（大于1TB）的并行运算。概念”Map（映射）”和”Reduce（归约）”，是它们的主要思想，都是从函数式编程语言里借来的，还有从矢量编程语言里借来的特性。</li><li>Yarn<br>Apache Hadoop YARN （Yet Another Resource Negotiator，另一种资源协调者）是一种新的 Hadoop 资源管理器，它是一个通用资源管理系统，可为上层应用提供统一的资源管理和调度，它的引入为集群在利用率、资源统一管理和数据共享等方面带来了巨大好处。</li></ul><h1 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h1><ul><li>商品广告推荐</li><li>保险</li><li>物流</li><li>旅游</li><li>…</li></ul><h1 id="Hadoop生态架构图"><a href="#Hadoop生态架构图" class="headerlink" title="Hadoop生态架构图"></a>Hadoop生态架构图</h1><p><img src="http://q7jwslv80.bkt.clouddn.com/hadoop_1_1.png" srcset="/img/loading.gif" alt="hadoop生态架构图"></p>]]></content>
    
    
    <categories>
      
      <category>Hadoop</category>
      
    </categories>
    
    
    <tags>
      
      <tag>起源</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>详解JPS命令</title>
    <link href="/2015/05/17/java1/"/>
    <url>/2015/05/17/java1/</url>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>jps(Java Virtual Machine Process Status Tool)是JDK 1.5提供的一个显示当前所有java进程pid的命令，简单实用，非常适合在linux/unix平台上简单察看当前java进程的一些简单情况。</p><h1 id="unix的ps命令"><a href="#unix的ps命令" class="headerlink" title="unix的ps命令"></a>unix的ps命令</h1><ul><li>用过unix系统里的ps命令，这个命令主要是用来显示当前系统的进程情况，有哪些进程，及其 id。 </li><li>jps也是一样，它的作用是显示当前系统的java进程情况，及其id号。我们可以通过它来查看我们到底启动了几个java进程，因为每一个java程序都会独占一个java虚拟机实例，和他们的进程号，并可通过opt来查看这些进程的详细启动参数。</li></ul><h1 id="JPS使用方法"><a href="#JPS使用方法" class="headerlink" title="JPS使用方法"></a>JPS使用方法</h1><h4 id="使用方法：在当前命令行下打-jps-系统要配置需要JAVA-HOME环境"><a href="#使用方法：在当前命令行下打-jps-系统要配置需要JAVA-HOME环境" class="headerlink" title="使用方法：在当前命令行下打 jps,系统要配置需要JAVA_HOME环境."></a>使用方法：在当前命令行下打 jps,系统要配置需要JAVA_HOME环境.</h4><p>命令：<code>jps</code></p><pre><code class="hljs basic"><span class="hljs-symbol">26177 </span>AmbariServer<span class="hljs-symbol">29041 </span>TimelineReaderServer<span class="hljs-symbol">4082 </span>ServiceMaster<span class="hljs-symbol">32275 </span>Jps</code></pre><h1 id="常用的参数"><a href="#常用的参数" class="headerlink" title="常用的参数"></a>常用的参数</h1><h5 id="q-只显示pid，不显示class名称-jar文件名和传递给main-方法的参数"><a href="#q-只显示pid，不显示class名称-jar文件名和传递给main-方法的参数" class="headerlink" title="-q 只显示pid，不显示class名称,jar文件名和传递给main 方法的参数"></a>-q 只显示pid，不显示class名称,jar文件名和传递给main 方法的参数</h5><p>命令：<code>jpq -q</code></p><pre><code class="hljs angelscript"><span class="hljs-number">26177</span><span class="hljs-number">29041</span><span class="hljs-number">4082</span><span class="hljs-number">32741</span></code></pre><h4 id="m-输出传递给main-方法的参数，在嵌入式jvm上可能是null"><a href="#m-输出传递给main-方法的参数，在嵌入式jvm上可能是null" class="headerlink" title="-m 输出传递给main 方法的参数，在嵌入式jvm上可能是null"></a>-m 输出传递给main 方法的参数，在嵌入式jvm上可能是null</h4><p>命令：<code>jps -m</code></p><pre><code class="hljs basic"><span class="hljs-symbol">6177 </span>AmbariServer<span class="hljs-symbol">29041 </span>TimelineReaderServer<span class="hljs-symbol">32741 </span>RunJar /<span class="hljs-keyword">usr</span>/hdp/<span class="hljs-number">3.1.0.0</span>-<span class="hljs-number">78</span>/hive/lib/hive-metastore-<span class="hljs-number">3.1.0.3.1.0.0</span>-<span class="hljs-number">78.</span>jar org.apache.hadoop.hive.metastore.HiveMetaStore</code></pre><h4 id="l-输出应用程序main-class的完整package名-或者-应用程序的jar文件完整路径名"><a href="#l-输出应用程序main-class的完整package名-或者-应用程序的jar文件完整路径名" class="headerlink" title="-l 输出应用程序main class的完整package名 或者 应用程序的jar文件完整路径名"></a>-l 输出应用程序main class的完整package名 或者 应用程序的jar文件完整路径名</h4><p>命令：<code>jps -l</code></p><pre><code class="hljs basic"><span class="hljs-symbol">26177 </span>org.apache.ambari.server.controller.AmbariServer<span class="hljs-symbol">29041 </span>org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderServer<span class="hljs-symbol">4082 </span>org.apache.hadoop.yarn.service.ServiceMaster</code></pre><h4 id="v-输出传递给JVM的参数"><a href="#v-输出传递给JVM的参数" class="headerlink" title="-v 输出传递给JVM的参数"></a>-v 输出传递给JVM的参数</h4><p>命令：<code>jps -v</code></p><pre><code class="hljs routeros">6177 AmbariServer -XX:<span class="hljs-attribute">NewRatio</span>=3 -XX:+UseConcMarkSweepGC -XX:-UseGCOverheadLimit -XX:<span class="hljs-attribute">CMSInitiatingOccupancyFraction</span>=60 -XX:+CMSClassUnloadingEnabled -Dsun.zip.<span class="hljs-attribute">disableMemoryMapping</span>=<span class="hljs-literal">true</span> -Xms512m -Xmx2048m -XX:<span class="hljs-attribute">MaxPermSize</span>=128m -Djava.security.auth.login.<span class="hljs-attribute">config</span>=/etc/ambari-server/conf/krb5JAASLogin.conf -Djava.security.krb5.<span class="hljs-attribute">conf</span>=/etc/krb5.conf -Djavax.security.auth.<span class="hljs-attribute">useSubjectCredsOnly</span>=<span class="hljs-literal">false</span> -Dcom.sun.jndi.ldap.connect.pool.<span class="hljs-attribute">protocol</span>=plain ssl -Dcom.sun.jndi.ldap.connect.pool.<span class="hljs-attribute">maxsize</span>=20 -Dcom.sun.jndi.ldap.connect.pool.<span class="hljs-attribute">timeout</span>=30000029041 TimelineReaderServer -Dproc_timelinereader -Dhdp.<span class="hljs-attribute">version</span>=3.1.0.0-78 -Djava.net.<span class="hljs-attribute">preferIPv4Stack</span>=<span class="hljs-literal">true</span> -Dhdp.<span class="hljs-attribute">version</span>=3.1.0.0-78 -Dyarn.id.str= -Dyarn.policy.<span class="hljs-attribute">file</span>=hadoop-policy.xml -Djava.io.<span class="hljs-attribute">tmpdir</span>=/var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir -Dyarn.log.<span class="hljs-attribute">dir</span>=/var/log/hadoop-yarn/yarn -Dyarn.log.<span class="hljs-attribute">file</span>=hadoop-yarn-timelinereader-vm10-101-179-203.ksc.com.log -Dyarn.home.<span class="hljs-attribute">dir</span>=/usr/hdp/3.1.0.0-78/hadoop-yarn -Dyarn.root.<span class="hljs-attribute">logger</span>=INFO,console -Djava.library.<span class="hljs-attribute">path</span>=:/usr/hdp/3.1.0.0-78/hadoop/lib/native/Linux-amd64-64:/usr/hdp/3.1.0.0-78/hadoop/lib/native/Linux-amd64-64:/var/lib/ambari-agent/tmp/hadoop_java_io_tmpdir:/usr/hdp/3.1.0.0-78/hadoop/lib/native -Xmx1024m -Dhadoop.log.<span class="hljs-attribute">dir</span>=/var/log/hadoop-yarn/yarn -Dhadoop.log.<span class="hljs-attribute">file</span>=hadoop-yarn-timelinereader-vm10-101-179-203.ksc.com.log -Dhadoop.home.<span class="hljs-attribute">dir</span>=/usr/hdp/3.1.0.0-78/hadoop -Dhadoop.id.<span class="hljs-attribute">str</span>=yarn -Dhadoop.root.<span class="hljs-attribute">logger</span>=INFO,RFA -Dhadoop.policy.<span class="hljs-attribute">file</span>=hadoop-policy.xml -Dhadoop.security.<span class="hljs-attribute">logger</span>=INFO,NullAppender</code></pre><h4 id="附详JPS细文档"><a href="#附详JPS细文档" class="headerlink" title="附详JPS细文档"></a><a href="https://docs.oracle.com/javase/1.5.0/docs/tooldocs/share/jps.html" target="_blank" rel="noopener">附详JPS细文档</a></h4>]]></content>
    
    
    <categories>
      
      <category>Java</category>
      
    </categories>
    
    
    <tags>
      
      <tag>工具</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
